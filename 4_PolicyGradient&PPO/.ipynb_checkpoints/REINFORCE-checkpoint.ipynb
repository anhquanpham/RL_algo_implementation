{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a90b036",
   "metadata": {},
   "source": [
    "# REINFORCE for different environments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71361df",
   "metadata": {},
   "source": [
    "## 1. Import the libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36e2728",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt #plotting library\n",
    "from matplotlib import animation #animated visualizations library\n",
    "from collections import namedtuple, deque \n",
    "#nametuple creates tuple subclasses with name fields, access elements by names instead of index\n",
    "#deque (double-ended queue) for adding and removing elements from both ends\n",
    "from tqdm import tqdm\n",
    "#add progress bars to Python code for easy monitoring progress of loops and tasks\n",
    "# %matplotlib inline \n",
    "import gym #environments for agents\n",
    "from datetime import datetime #manipulating dates and times\n",
    "import pandas as pd #work with structured data\n",
    "import torch #Pytorch supports tensor computations and neural networks\n",
    "import torch.nn as nn #Pytorch supports building neural networks\n",
    "import torch.nn.functional as F\n",
    "#common functions in neural network operations \n",
    "    # Activation functions (ReLU, sigmoid, tanh)\n",
    "    # Loss functions (cross_entropy, mse_loss)\n",
    "    #Utility functions for tensor manipulation (softmax, dropout, batch_norm, etc.)\n",
    "import torch.optim as optim #optimization algorithms for training neural networks\n",
    "import random #generate random numbers/selections\n",
    "from collections import namedtuple, deque \n",
    "import itertools \n",
    "# provides various functions for creating iterators and combining them for complex interators\n",
    "# includes cycle, chain, zip, etc.\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch.autograd as autograd\n",
    "\n",
    "from torch.distributions import Normal\n",
    "\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcda5845",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade tqdm\n",
    "!pip install --upgrade torch\n",
    "!pip install --upgrade gym\n",
    "!pip install --upgrade numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7520b77d",
   "metadata": {},
   "source": [
    "# Current Gym version: 0.26.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b610c0",
   "metadata": {},
   "source": [
    "## 2. Algorithm implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555fdd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Monte-Carlo Policy Gradient \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "class reinforce(nn.Module):\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units = 64, fc2_units = 64):\n",
    "        super(reinforce, self).__init__()\n",
    "        # policy network\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        state_size = env.observation_space.shape[0]\n",
    "#         action_size= env.action_space.n\n",
    "        action_size = env.action_space.shape[0]\n",
    "        self.action_size = action_size\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.logstd = nn.Parameter(torch.zeros(1, action_size))\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self.fc1(state)\n",
    "        x = self.tanh(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.tanh(x)\n",
    "        x = self.fc3(x)\n",
    "\n",
    "#         x = self.softmax(x)\n",
    "\n",
    "            # Output both mean and log standard deviation\n",
    "        action_mean = x\n",
    "\n",
    "        action_logstd = self.logstd\n",
    "\n",
    "        action_logstd = action_logstd.squeeze()\n",
    "\n",
    "\n",
    "        if len(action_logstd.shape) == 0:\n",
    "            action_logstd = action_logstd.unsqueeze(0)  # Convert scalar to tensor with shape (1,)\n",
    "            concatenated = torch.cat((action_mean, action_logstd), dim=0)\n",
    "\n",
    "        else:\n",
    "            concatenated = torch.cat((action_mean, action_logstd.squeeze()), dim=0)\n",
    "        return concatenated #\n",
    "#         return torch.cat((action_mean, action_logstd), dim=1)\n",
    "    \n",
    "\n",
    "\n",
    "    def get_action(self, state, action_size):\n",
    "        state = torch.tensor(state, dtype=torch.float32)\n",
    "        probs = self.forward(state)\n",
    "\n",
    "        # Assuming your policy network outputs both mean and log standard deviation\n",
    "        action_mean = probs[:self.action_size]\n",
    "\n",
    "        action_logstd = probs[self.action_size:]\n",
    "\n",
    "\n",
    "        action_std = torch.exp(action_logstd)\n",
    "\n",
    "        # Sample from a normal distribution with the mean and standard deviation\n",
    "        normal = Normal(action_mean, action_std)\n",
    "        action = normal.sample()\n",
    "\n",
    "        # Clip action to ensure it's within the valid range\n",
    "        action = torch.clamp(action, -1.0, 1.0)  # Assuming action range is [-1, 1]\n",
    "\n",
    "\n",
    "        return action.detach().numpy()  # Convert PyTorch tensor to NumPy array\n",
    "    \n",
    "\n",
    "    def pi(self, state, action):\n",
    "\n",
    "        state = torch.tensor(state)  \n",
    "        probs = self.forward(state)\n",
    "\n",
    "        return probs[action] + 0.000000001\n",
    "\n",
    "    def update_weight(self, states, actions, rewards, optimizer):\n",
    "        G = 0\n",
    "        # for each step of the episode t = T - 1, ..., 0\n",
    "        # r_tt represents r_{t+1}\n",
    "        for s_t, a_t, r_tt in zip(states[::-1], actions[::-1], rewards[::-1]):\n",
    "            print(\"State input: \", s_t)\n",
    "            print(\"-------------------------------------------\")\n",
    "            print(\"Action: \", a_t)\n",
    "            print(\"-------------------------------------------\")\n",
    "            print(\"Immediate reward: \", r_tt)\n",
    "            print(\"-------------------------------------------\")\n",
    "            G = r_tt + GAMMA * G  # No need for Variable wrapper\n",
    "            print(\"Return: \", G)\n",
    "            print(\"-------------------------------------------\")\n",
    "            loss = (-1.0) * G * torch.log(self.pi(s_t, a_t))\n",
    "            loss = loss[0] \n",
    "            \n",
    "#             print(\"Loss ((-1.0) * G * torch.log(policy of action given state): \", loss)\n",
    "            # update policy parameter \\theta\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "#         print(\"Return outside loop: \", G)\n",
    "#         print(\"-------------------------------------------\")\n",
    "        return (loss)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fa473f",
   "metadata": {},
   "source": [
    "## 3. Training implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58a269a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env_names = ['MountainCarContinuous-v0'] #,'LunarLanderContinuous-v2'\n",
    "seeds = [1,37,42] #\n",
    "MAX_EPISODES = 2000\n",
    "\n",
    "\n",
    "ALPHA = 0.01\n",
    "GAMMA = 1\n",
    "# action_size = env.action_space.n\n",
    "\n",
    "for i in env_names:\n",
    "    print(\"ENVIRONMENT:-----------\", i)\n",
    "    env = gym.make(i)\n",
    "    MAX_TIMESTEPS = 200 #env.spec.max_episode_steps\n",
    "    res = []\n",
    "    for seed in seeds:\n",
    "        print(\"Seed: ---------------------\", seed)\n",
    "        rewards_nonaver = []\n",
    "        aver_reward = []\n",
    "        aver = deque(maxlen = 100)\n",
    "        state_size = env.observation_space.shape[0]\n",
    "#         print(\"State_size: \", state_size)\n",
    "        action_size = env.action_space.shape[0]\n",
    "        print(\"Action size\", action_size)\n",
    "        agent = reinforce(state_size, action_size, seed)\n",
    "        optimizer = optim.Adam(agent.parameters(), lr=ALPHA)\n",
    "    \n",
    "        for i_episode in tqdm(range(1,MAX_EPISODES+1)):\n",
    "            state = env.reset()\n",
    "            states = []\n",
    "            actions = []\n",
    "            rewards = [0]   # no reward at t = 0\n",
    "            score = 0\n",
    "            timestep = 0\n",
    "            while True:\n",
    "                \n",
    "                state = torch.tensor(state[0])\n",
    "#                 state = torch.tensor(state)\n",
    "\n",
    "                action = agent.get_action(state, action_size)\n",
    "                states.append(state)\n",
    "                actions.append(action)\n",
    "                step_result = env.step(action)\n",
    "                next_state, reward, done, _ = step_result[:4]\n",
    "#                 state = next_state\n",
    "                state = (next_state, {})\n",
    "                rewards.append(reward)\n",
    "\n",
    "                score = score + reward\n",
    "\n",
    "                timestep = timestep + 1\n",
    "                if timestep == MAX_TIMESTEPS:\n",
    "                    break\n",
    "                if done:\n",
    "#                     print(\"Episode {} finished after {} timesteps\".format(i_episode, timesteps+1))\n",
    "                    break\n",
    "            print(f\"After finish episode {i_episode}, we obtain trajectory: \")\n",
    "            print(\"-------------------------------------------\")\n",
    "            print(\"States: \", states)\n",
    "            print(\"---------------------------------------------\")\n",
    "            print(\"Actions: \", actions)\n",
    "            print(\"---------------------------------------------\")\n",
    "            print(\"Rewards \", rewards)\n",
    "            print(\"---------------------------------------------\")\n",
    "            agent.update_weight(states, actions, rewards, optimizer) \n",
    "            if i_episode <10 or i_episode > 1980:\n",
    "                print(f\"For episode {i_episode}\")\n",
    "                print(\"Loss: \", agent.update_weight(states, actions, rewards, optimizer))\n",
    "            \n",
    "            \n",
    "        \n",
    "            aver.append(score)\n",
    "            aver_reward.append(np.mean(aver))\n",
    "            rewards_nonaver.append(score)\n",
    "            \n",
    "#             print(\"Episode end with :\", timesteps)\n",
    "        \n",
    "        \n",
    "        policy_filename = f\"model/Seed{seed}_{i}_REINFORCE_{datetime.now().strftime('%Y%m%d%H%M%S')}.pt\"\n",
    "        torch.save(agent.state_dict(), policy_filename)\n",
    "        res.append(aver_reward)\n",
    "#         res.append(rewards_nonaver) #in case want to retrieve non smooth result\n",
    "        print(\"SAVED MODEL SUCCESSFULLY\")\n",
    "\n",
    "        \n",
    "    fig = plt.figure()\n",
    "    \n",
    "    reward_name = 'plots/' + i + '_REINFORCE_result' + str(datetime.now().strftime(\"%Y%m%d%H%M%S\"))\n",
    "    df = pd.DataFrame({str(seeds[0]): res[0], str(seeds[1]): res[1], str(seeds[2]): res[2]})  # Use seed as column labels\n",
    "    df.to_csv(reward_name + '.csv')\n",
    "    print(\"Saved rewards for plot successfully\")\n",
    "    \n",
    "    env.close()\n",
    "    print(\"------------------------End Environment-------------------\")\n",
    "        \n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "\n",
    "    # Plot rewards for each seed\n",
    "    for seed in seeds:\n",
    "        plt.plot(df[str(seed)], label='Seed ' + str(seed))\n",
    "\n",
    "    plt.title('Learning Curve ' + i)\n",
    "\n",
    "    # Insert the legends in the plot\n",
    "    fig.legend(loc='lower right')\n",
    "    fig.savefig(reward_name + '.png', dpi=100)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273b9ef4",
   "metadata": {},
   "source": [
    "## Simulation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e8a074",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gym\n",
    "\n",
    "# Step 1: Load the Policy Model\n",
    "model = torch.load('model/Seed42_MountainCarContinuous-v0_REINFORCE_20240502160736.pt')\n",
    "\n",
    "# Step 2: Set up Environment\n",
    "seed = 1\n",
    "env_name = 'MountainCarContinuous-v0' #'LunarLanderContinuous-v2'\n",
    "env = gym.make(env_name, render_mode = 'human')\n",
    "# env = gym.make(env_name) #, render_mode = 'human' , render_mode = 'human'\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.shape[0]\n",
    "agent = reinforce(state_size, action_size, seed)\n",
    "\n",
    "# Step 3: Run Simulation\n",
    "num_episodes = 10  # You can adjust the number of episodes you want to run\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    env.render()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    timesteps = 1\n",
    "    while not done:\n",
    "        \n",
    "        \n",
    "        # Convert state to tensor and add batch dimension if needed\n",
    "        state = torch.tensor(state[0])\n",
    "        \n",
    "        # Get action probabilities from the policy\n",
    "        action = agent.get_action(state, action_size)\n",
    "        \n",
    "        step_result = env.step(action)\n",
    "        next_state, reward, done, _ = step_result[:4]\n",
    "        \n",
    "        state = (next_state, {})\n",
    "#         state = next_state\n",
    "        \n",
    "        # Update total reward\n",
    "        total_reward += reward\n",
    "        \n",
    "        timesteps = timesteps + 1\n",
    "        \n",
    "    \n",
    "    print(f\"Episode {episode+1} ends in {timesteps} timesteps, Total Reward: {total_reward}\")\n",
    "\n",
    "env.close()  # Close the environment after running the simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86dcbd9",
   "metadata": {},
   "source": [
    "## Shaded plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f908e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Read Data from CSV\n",
    "df = pd.read_csv('plots/MountainCarContinuous-v0_REINFORCE_result20240428024956.csv')\n",
    "\n",
    "# Step 2: Remove the first row (Episodes, Run 1, Run 2, Run 3) to keep only the rewards data\n",
    "# df = df.drop(0)\n",
    "\n",
    "# Step 3: Convert the remaining DataFrame to numeric values\n",
    "df = df.apply(pd.to_numeric)\n",
    "\n",
    "# Step 4: Select only the three late columns (Run 1, Run 2, Run 3)\n",
    "later_columns = df.iloc[:, 1:4]\n",
    "\n",
    "# Step 5: Calculate Mean and Standard Deviation for the three later columns\n",
    "mean_values = later_columns.mean(axis=1)\n",
    "std_values = later_columns.std(axis=1)\n",
    "print(\"Standard deviation: \", std_values)\n",
    "print(\"Mean: \", mean_values)\n",
    "\n",
    "# Step 6: Plot the Data\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(mean_values, label='Mean Sum of Rewards', color = \"red\")\n",
    "plt.fill_between(mean_values.index, mean_values - std_values, mean_values + std_values, alpha=0.5, label='Standard Deviation', color = \"orange\")\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Sum of Rewards')\n",
    "plt.title('Shaded Plot for REINFORCE Algorithm, MountainCarContinuous-v0')\n",
    "plt.savefig('plots/shaded_plot_REINFORCE_MountainCarContinuous.png')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
