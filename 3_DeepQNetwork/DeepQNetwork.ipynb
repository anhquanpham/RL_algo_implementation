{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34729e1e",
   "metadata": {},
   "source": [
    "# Deep Q Network for different environments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc128c0",
   "metadata": {},
   "source": [
    "## 1. Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c75e7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt #plotting library\n",
    "from matplotlib import animation #animated visualizations library\n",
    "from collections import namedtuple, deque \n",
    "#nametuple creates tuple subclasses with name fields, access elements by names instead of index\n",
    "#deque (double-ended queue) for adding and removing elements from both ends\n",
    "from tqdm import tqdm\n",
    "#add progress bars to Python code for easy monitoring progress of loops and tasks\n",
    "# %matplotlib inline \n",
    "import gym #environments for agents\n",
    "from datetime import datetime #manipulating dates and times\n",
    "import pandas as pd #work with structured data\n",
    "import torch #Pytorch supports tensor computations and neural networks\n",
    "import torch.nn as nn #Pytorch supports building neural networks\n",
    "import torch.nn.functional as Function \n",
    "#common functions in neural network operations \n",
    "    # Activation functions (ReLU, sigmoid, tanh)\n",
    "    # Loss functions (cross_entropy, mse_loss)\n",
    "    #Utility functions for tensor manipulation (softmax, dropout, batch_norm, etc.)\n",
    "import torch.optim as optim #optimization algorithms for training neural networks\n",
    "import random #generate random numbers/selections\n",
    "from collections import namedtuple, deque \n",
    "import itertools \n",
    "# provides various functions for creating iterators and combining them for complex interators\n",
    "# includes cycle, chain, zip, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45645d39",
   "metadata": {},
   "source": [
    "## 2. Neural network model for approximating Q-values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ece1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    #Actor Model\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=10, fc2_units=10):\n",
    "        #Parameters\n",
    "#             state_size (int): Dimensionality of input state space\n",
    "#             action_size (int): Dimensionality of output action space\n",
    "#             seed (int): Random seed for reproducibility\n",
    "#             fc1_units (int): Number of neurons (units) in first fully connected hidden layer\n",
    "#             fc2_units (int): Number of neurons (unit) in second fully connected hidden layer  \n",
    "        \n",
    "        #Initialization\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed) # Set the random seed :\n",
    "        state_size = env.observation_space.shape[0]\n",
    "        #Ensure random numbers generate are reproducible\n",
    "        #Running same code with the same seed will produce the same sequence of random numbers\n",
    "        \n",
    "        # nn.Linear creates fully connected layers (input units, output units)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units) \n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        # Network to map state with action values (Q-values)\n",
    "        #inside method, input state pass through each layer\n",
    "        #ReLU activation functions are applied to the outputs of hidden layers\n",
    "#         print('state:', state.shape)\n",
    "        #x output of 1 layer is input to the next\n",
    "        x = Function.relu(self.fc1(state))\n",
    "        x = Function.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9498e90a",
   "metadata": {},
   "source": [
    "## 3. Implement the replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edd30ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\") #Device Initialization\n",
    "\n",
    "class ReplayBuffer: #Fixed-size buffer to store experience tuples.\n",
    "    # Initialize a ReplayBuffer object.\n",
    "    def __init__(self, state_size, action_size, buffer_size, batch_size, priority=False):\n",
    "            #state_size(int): dimension of each state\n",
    "            #action_size (int): dimension of each action \n",
    "            #buffer_size (int): max size of buffer \n",
    "            #batch_size (int): size of 1 training batch\n",
    "            #seed (int): random seed\n",
    "                \n",
    "        #Pointer to keep track of position within the replay buffer where next experiene will be added\n",
    "        # ptr = 0, new experience added -> increment to ptr = 1\n",
    "        self. ptr = 0\n",
    "        #Check if buffer have been filled\n",
    "        self.n = 0 \n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.state_size = state_size\n",
    "        \n",
    "        state_size = env.observation_space.shape[0]\n",
    "\n",
    "        #create states tensor full of zeros with {buffer_size} rows & {state_size} columns\n",
    "        #store in device CPU, allocate RAM to it \n",
    "        self.states = torch.zeros((buffer_size,)+(state_size,)).to(device)\n",
    "        \n",
    "        #Similar to above \n",
    "        self.next_states = torch.zeros((buffer_size,)+(state_size,)).to(device)\n",
    "        \n",
    "        #Only 1 action taken per experience tuple stored\n",
    "        # => action size = buffer size rows but 1 column only needed to store\n",
    "        self.actions = torch.zeros(buffer_size,1, dtype=torch.long).to(device)\n",
    "        \n",
    "        # Reward same with action\n",
    "        self.rewards = torch.zeros(buffer_size, 1, dtype=torch.float).to(device)\n",
    "        \n",
    "        # Flag to indicate transition/end between an episode ('done' flag)\n",
    "        # True/False floating values (0.0 False, 1.0 True)\n",
    "        self.dones = torch.zeros(buffer_size, 1, dtype=torch.float).to(device) \n",
    "        \n",
    "        # Error in case implement prioritize replay\n",
    "        self.error = np.zeros((buffer_size, 1), dtype=float)\n",
    "        \n",
    "        # Priority\n",
    "        self.priority = priority\n",
    "    \n",
    "  # Add new experience to buffer\n",
    "    def add(self, state, action, reward, next_state, done, state_size):\n",
    "        \n",
    "        state_size = env.observation_space.shape[0]\n",
    "        \n",
    "        \n",
    "        # Function to flatten ragged nested sequences into a flat array of compatible type\n",
    "        def flatten_nested_sequence(elem):\n",
    "            flattened = []\n",
    "            for e in elem:\n",
    "                if isinstance(e, (list, tuple)):\n",
    "                    flattened.extend(flatten_nested_sequence(e))\n",
    "                elif isinstance(e, np.ndarray):\n",
    "                    if e.dtype == np.object_:\n",
    "                        raise ValueError(\"Cannot handle np.ndarray of type numpy.object_\")\n",
    "                    else:\n",
    "                        flattened.append(e)\n",
    "                else:\n",
    "                    flattened.append(e)\n",
    "            return flattened\n",
    "\n",
    "        # Convert state and next_state to appropriate types\n",
    "        if isinstance(state, np.ndarray):\n",
    "            state = flatten_nested_sequence(state)\n",
    "            state = torch.as_tensor(state, dtype=torch.float32).to(device)\n",
    "        elif isinstance(state, torch.Tensor):\n",
    "            state = state.to(device)\n",
    "        else:\n",
    "            state = torch.zeros((1, state_size), dtype=torch.float32).to(device)\n",
    "\n",
    "        if isinstance(next_state, np.ndarray):\n",
    "            next_state = flatten_nested_sequence(next_state)\n",
    "            next_state = torch.as_tensor(next_state, dtype=torch.float32).to(device)\n",
    "        elif isinstance(next_state, torch.Tensor):\n",
    "            next_state = next_state.to(device)\n",
    "        else:\n",
    "            next_state = torch.zeros((1, state_size), dtype=torch.float32).to(device) \n",
    "        # Convert action, reward, and done flag to tensors\n",
    "        action = torch.as_tensor(action, dtype=torch.float32).to(device)\n",
    "        reward = torch.as_tensor(reward, dtype=torch.float32).to(device)\n",
    "        done = torch.as_tensor(done, dtype=torch.bool).to(device)\n",
    "\n",
    "        # Store the data in the replay buffer\n",
    "        self.states[self.ptr] = state\n",
    "        self.next_states[self.ptr] = next_state\n",
    "        self.actions[self.ptr] = action\n",
    "        self.rewards[self.ptr] = reward\n",
    "        self.dones[self.ptr] = done\n",
    "\n",
    "        # Increment the pointer\n",
    "        self.ptr = (self.ptr + 1) % self.buffer_size\n",
    "\n",
    "        # Reset pointer and flag when buffer is filled\n",
    "        if self.ptr == 0:\n",
    "            self.n = self.buffer_size\n",
    "        \n",
    "    #Sample a batch of experience from memory\n",
    "    def sample(self, get_all=False):\n",
    "        n = len(self) # Length of the Buffer\n",
    "        \n",
    "        # Return all experience stored in buffer, no sampling\n",
    "        if get_all:\n",
    "            return self.states[:n], self.actions[:n], self.rewards[:n], self.next_states[:n], self.dones[:n]\n",
    "        \n",
    "        # else, do sampling: \n",
    "        else:\n",
    "            if self.priority:     \n",
    "            #enable prioritized experience replay\n",
    "            #experience are sampled based on priorities probability distribution p = self.error\n",
    "                idx = np.random.choice(n, self.batch_size, replace=False, p=self.error)\n",
    "            else: #uniform sampling\n",
    "                idx = np.random.choice(n, self.batch_size, replace=False)\n",
    "            #Replace = False to only sample once for each element\n",
    "            \n",
    "            #Retrieve sampled experiences \n",
    "            states = self.states[idx]\n",
    "            next_states = self.next_states[idx]\n",
    "            actions = self.actions[idx]\n",
    "            rewards = self.rewards[idx]\n",
    "            dones = self.dones[idx]\n",
    "        \n",
    "            return (states, actions, rewards, next_states, dones), idx\n",
    "    \n",
    "    # Update the error associated with experiences in replay buffer\n",
    "    def update_error(self, error, idx=None): #specify index number, if not specify, all are updated\n",
    "        error = torch.abs(error.detach()) #absolute value of errors, detach to prevent gradient computation to be attached to error tensor\n",
    "        error = error / error.sum() #Normalize, ensure all errors add up to 1\n",
    "        if idx is not None: #index are specified, then only update specified indices\n",
    "            self.error[idx] = error.cpu().numpy()\n",
    "        else: # not specify index, all are updated\n",
    "            self.error[:len(self)] = error.cpu().numpy()\n",
    "    \n",
    "    # \n",
    "    def __len__(self): \n",
    "        if self.n == 0:\n",
    "            return self.ptr\n",
    "        else:\n",
    "            return self.n #when buffer is filled self.n stored that size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829addeb",
   "metadata": {},
   "source": [
    "## 4. Set the parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf8a424",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = 100000 #Replay buffer size\n",
    "batch_size = 4 #Minibatch size\n",
    "gamma = 0.99 #Discount factor\n",
    "tau = 1 #Soft update target parameters, tau = 1 = copy completely\n",
    "alpha = 0.0005 #learning rate\n",
    "update_every = 4 #How often to update the network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301e9c28",
   "metadata": {},
   "source": [
    "## 5. Agent learning implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a68e9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(): #Learning by interacting with the environment\n",
    "    def __init__(self, state_size, action_size, seed=1, ddqn=True, priority=False):\n",
    "        self.state_size = state_size #Dimension of each state\n",
    "        self.action_size = action_size #Dimension of action\n",
    "        self.seed = random.seed(seed) #Choose the random seed\n",
    "        self.ddqn = ddqn #Store whether agent uses DDQN\n",
    "        self.priority = priority #Whether uses experience replay\n",
    "        \n",
    "        #Q-Network\n",
    "        self.qnetwork_local = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.qnetwork_target = QNetwork(state_size, action_size, seed).to(device)\n",
    "        \n",
    "        #Initializes optimizer for updating the weights of local Q-Network\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=alpha)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(state_size, (action_size,), buffer_size, batch_size)\n",
    "        # Initialize timestep (to keep track timesteps for updating target)\n",
    "        self.t_step = 0\n",
    "        \n",
    "    def step(self, state, action, reward, next_state, done, step_size):\n",
    "        \n",
    "        # Save experience in replay memory\n",
    "        self.memory.add(state, action, reward, next_state, done, step_size)\n",
    "        \n",
    "        \n",
    "        self.t_step = self.t_step + 1  \n",
    "        ## Update Q-network after each step in DQN case\n",
    "        \n",
    "        if not self.ddqn: #DQN case\n",
    "            \n",
    "            if len(self.memory) > batch_size:\n",
    "                experiences, idx = self.memory.sample() #sample batch of experience from replay memory\n",
    "                error = self.learn(experiences) #update Q-network based on sampled experiences\n",
    "                                #outputs the error (difference) between predicted Q-value and target Q-value during training\n",
    "                self.memory.update_error(error,idx)\n",
    "                \n",
    "        else: #DDQN case, only update after a period\n",
    "            if len(self.memory) > batch_size:\n",
    "                experiences, idx = self.memory.sample() #sample batch of experience from replay memory\n",
    "                error = self.learn(experiences) #update Q-network based on sampled experiences\n",
    "                                #outputs the error (difference) between predicted Q-value and target Q-value during training\n",
    "            #if there is no remainder we update\n",
    "            if (self.t_step % update_every) == 0 and len(self.memory) > batch_size:\n",
    "                self.memory.update_error(error, idx) #update\n",
    " \n",
    "    def act(self, state, epsilon):\n",
    "        if isinstance(state, np.ndarray):\n",
    "            if state.dtype == np.object_:\n",
    "                # Convert elements of the NumPy array to compatible types\n",
    "                state = np.array([self.convert_to_compatible_type(elem) for elem in state])\n",
    "            # Convert the modified NumPy array to a PyTorch tensor for neural network input\n",
    "            state = torch.as_tensor(state, dtype=torch.float32).to(device)\n",
    "        elif isinstance(state, list) or isinstance(state, tuple):\n",
    "            # Convert the state to a NumPy array and handle ragged nested sequences\n",
    "            state = self.handle_ragged_nested_sequence(state)\n",
    "            # Convert the modified NumPy array to a PyTorch tensor for neural network input\n",
    "            state = torch.as_tensor(state, dtype=torch.float32).to(device)\n",
    "        elif isinstance(state, torch.Tensor):\n",
    "            # Ensure the tensor is in the correct device and data type\n",
    "            state = state.to(device)\n",
    "        else:\n",
    "            raise TypeError(\"Input state should be a NumPy array, list, tuple, or torch.Tensor.\")\n",
    "\n",
    "        # Ensure the state has the correct shape for the neural network input\n",
    "        state = state.unsqueeze(0)  # Add a batch dimension\n",
    "\n",
    "        self.qnetwork_local.eval()  # Evaluation mode\n",
    "        with torch.no_grad():  # Disable gradient calculation when choosing action\n",
    "            action_values = self.qnetwork_local(state)  # Pass preprocessed states into local Q-network\n",
    "        self.qnetwork_local.train()  # Set local Q network back to training mode after inference is complete\n",
    "\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() > epsilon:\n",
    "            # Return action with highest Q-value\n",
    "            action = np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            # Select a random action\n",
    "            action = random.choice(range(self.action_size))\n",
    "        action = min(max(action, 0), self.action_size - 1) # Ensure action is within valid range\n",
    "        \n",
    "        return action\n",
    "\n",
    "    def convert_to_compatible_type(self, elem):\n",
    "        # Convert the element to a compatible type if it is a numpy.object_\n",
    "        if isinstance(elem, str):\n",
    "            return float(elem)\n",
    "        elif isinstance(elem, np.ndarray):\n",
    "            return elem.astype(np.float32)\n",
    "        else:\n",
    "            return float(elem)\n",
    "\n",
    "    def handle_ragged_nested_sequence(self, state):\n",
    "        if not state:\n",
    "            return np.array([])  # Return an empty NumPy array if the state is empty\n",
    "        else:\n",
    "            # Convert dictionaries to lists of values\n",
    "            state = [list(d.values()) if isinstance(d, dict) else d for d in state]\n",
    "            # Determine the maximum length among all nested sequences\n",
    "            max_length = max(len(sublist) for sublist in state)\n",
    "            # Pad shorter sequences with zeros to make them of equal length\n",
    "            state = [sublist + [0] * (max_length - len(sublist)) if len(sublist) < max_length else sublist for sublist in state]\n",
    "            # Convert the nested sequence to a NumPy array\n",
    "            state = np.array(state)\n",
    "            return state\n",
    "    \n",
    "    #Error update for prioritization experience replay\n",
    "    def update_error(self):\n",
    "        #Sample experience from replay memory\n",
    "        states, actions, rewards, next_states, dones = self.memory.sample(get_all=True)\n",
    "        with torch.no_grad(): #no gradient computations within block\n",
    "            if self.ddqn: #DDQN is enabled, use Q Network for both local and target\n",
    "                #Q values of action taken in current state\n",
    "                old_val = self.qnetwork_local(states).gather(-1, actions)\n",
    "                #Determine actions with highest Q values in next states using local Q-network\n",
    "                actions = self.qnetwork_local(next_states).argmax(-1, keepdim=True)\n",
    "                # Compute Q-values of selected actions from target Q-network for next states\n",
    "                maxQ = self.qnetwork_target(next_states).gather(-1, actions)\n",
    "                #Calculate target as the normal formula\n",
    "                #dones is a binary tensor indicating the next state is terminal or not (store done flags)\n",
    "                #Use 1-dones to ensure discounted future rewards are considered only for non-terminal states\n",
    "                #At terminal done = 1, so 1 - dones = 0, cancels out the maxQ => Q terminal = 0\n",
    "                target = rewards+gamma*maxQ*(1-dones)\n",
    "            else: # Normal DQN\n",
    "                #Difference in this DQN is obtained directly from target Q-network using max function\n",
    "                #instead of considering the action from local state\n",
    "                maxQ = self.qnetwork_target(next_states).max(-1, keepdim=True)[0]\n",
    "                target = rewards+gamma*maxQ*(1-dones)\n",
    "                old_val = self.qnetwork_local(states).gather(-1, actions)\n",
    "            error = old_val - target\n",
    "            self.memory.update_error(error)      \n",
    "            \n",
    "    #Update value parameters using batch of experience tuples\n",
    "    def learn(self, experiences):\n",
    "        #experiences (Tuple[torch.Variable]): tuple of (s, a, r, s', done) tuples \n",
    "        #gamma (float): discount factor\n",
    "      \n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        ## compute and minimize the loss\n",
    "        self.optimizer.zero_grad() #Resets all gradients to zero, gradients from previous batches do not accumulate\n",
    "        if i_episode < 3 or i_episode > 1998:\n",
    "            print(\"Current state: \", state)\n",
    "            print(\" \")\n",
    "            print(\"Experiences: \")\n",
    "            print(\"States: \", states)\n",
    "            print(\"Actions: \", actions)\n",
    "            print(\"Rewards: \", rewards)\n",
    "            print(\"Next states: \", next_states)\n",
    "            print(\" \")\n",
    "        #Same as explained above\n",
    "        if self.ddqn: #DDQN\n",
    "            old_val = self.qnetwork_local(states).gather(-1, actions)\n",
    "            if i_episode < 3 or i_episode > 1998:\n",
    "                print(\"Q_current: \", old_val)\n",
    "                print(\" \")\n",
    "            with torch.no_grad():\n",
    "                if i_episode < 3 or i_episode > 1998:\n",
    "                    print(\"All Q values for next_states in order to choose actions, estimate with local network: \")\n",
    "                    print(self.qnetwork_local(next_states))\n",
    "                    print(\" \")\n",
    "                next_actions = self.qnetwork_local(next_states).argmax(-1, keepdim=True)\n",
    "                \n",
    "                if i_episode < 3 or i_episode > 1998:\n",
    "                    print(\"Actions, chose with local network = \", next_actions)\n",
    "                    print(\" \")\n",
    "                    print(\"All Q values for next_states, estimate with target network: \")\n",
    "                    print(self.qnetwork_target(next_states))\n",
    "                    print(\" \")\n",
    "                maxQ = self.qnetwork_target(next_states).gather(-1, next_actions)\n",
    "                \n",
    "                if i_episode < 3 or i_episode > 1998:\n",
    "                    print(\"Chosen maximum Q - value from actions gathered above = \", maxQ)\n",
    "                    print(\" \")\n",
    "                target = rewards+gamma*maxQ*(1-dones)\n",
    "                \n",
    "                if i_episode < 3 or i_episode > 1998:\n",
    "                    print(\"Q_target = \", target)\n",
    "                    print(\" \")\n",
    "                    \n",
    "        else: # Normal DQN\n",
    "            old_val = self.qnetwork_local(states).gather(-1, actions)\n",
    "            \n",
    "            if i_episode < 3 or i_episode > 1998:\n",
    "                print(\"Q_current: \", old_val)\n",
    "                print(\" \")\n",
    "                print(\"Actions = \", actions)\n",
    "                print(\" \")\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                \n",
    "                if i_episode < 3 or i_episode > 1998:\n",
    "                    print(\"All Q values for next_states = \")\n",
    "                    print(self.qnetwork_target(next_states))\n",
    "                    print(\" \")\n",
    "                \n",
    "                maxQ = self.qnetwork_target(next_states).max(-1, keepdim=True)[0]\n",
    "                \n",
    "                if i_episode < 3 or i_episode > 1998:\n",
    "                    print(\"Chosen maximum Q - value= \", maxQ)\n",
    "                    print(\" \")\n",
    "                \n",
    "                target = rewards+gamma*maxQ*(1-dones)\n",
    "                \n",
    "                if i_episode < 3 or i_episode > 1998:\n",
    "                    print(\"Q_target = \", target)\n",
    "                    print(\" \")\n",
    "               \n",
    "        if i_episode < 3 or i_episode > 1998:\n",
    "            print(\"Error(Q_target - Q_current): \", target - old_val)\n",
    "            print(\" \")\n",
    "        loss = Function.mse_loss(old_val, target) #Calculate loss of current and target Q-values\n",
    "        loss.backward() #Gradient of loss with respect to Q-network parameters are computed\n",
    "                        #and used to update the parameters\n",
    "        self.optimizer.step() #update the neural network\n",
    "                                #step method applies optimization to update parameters\n",
    "                                #steps:\n",
    "                                #1. Optimizer uses computed gradients to update parameters\n",
    "                                #2. Adam optimizer is applied to each parameter based on the gradients and learning rate\n",
    "                                #3. Gradients are cleared back to zero as first step for next training iteration\n",
    "\n",
    "        ##UPDATE THE TARGET NETWORK##\n",
    "        if not self.ddqn:  # If DQN, update after each episode\n",
    "            self.soft_update(self.qnetwork_local, self.qnetwork_target, tau) \n",
    "        else:  # If DDQN, update after every update_every episodes\n",
    "            if (self.t_step % update_every) == 0:\n",
    "                self.soft_update(self.qnetwork_local, self.qnetwork_target, tau)\n",
    "        \n",
    "        return old_val - target #temporal difference TD error between old Q and target Q to monitor training progress\n",
    "\n",
    "    \n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        #local_model: online model, actively being trained. Weights will be copied from here\n",
    "        #target_model: use to generate target Q-values during training. Weights will be copied to here\n",
    "        #tau: interpolation parameters determine rate at which parameters of target models are updated\n",
    "        #small tau slower update, big tau faster update, less stable\n",
    "        \n",
    "        #function iterates over parameters of both target model and local model using zip\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            #for each target param - local param pair, update target param by the formula\n",
    "            # target_param = tau*local_param + (1-tau)*target_local \n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "            \n",
    "    # Function to simulate a model in an environment\n",
    "    def simulate_model(env_name, model_path):\n",
    "        # Load the environment\n",
    "        env = gym.make(env_name)\n",
    "\n",
    "        # Get environment parameters\n",
    "        state_size = env.observation_space.shape[0]\n",
    "        action_size = env.action_space.n\n",
    "\n",
    "        # Initialize the agent\n",
    "        agent = Agent(state_size, action_size, seed) \n",
    "\n",
    "        # Load the model weights\n",
    "        agent.qnetwork_local.load_state_dict(torch.load(model_path))\n",
    "        agent.qnetwork_local.eval()\n",
    "\n",
    "        # Simulate the model in the environment\n",
    "        scores = []\n",
    "        n_episodes = 100  # Number of episodes for simulation\n",
    "        max_t = 1000  # Maximum number of timesteps per episode\n",
    "\n",
    "        for i_episode in tqdm(range(1, n_episodes+1)):\n",
    "            state = env.reset()\n",
    "            score = 0\n",
    "\n",
    "            for t in range(max_t):\n",
    "                action = agent.act(state, epsilon=0.)  # Greedy action selection, no exploration\n",
    "                state, reward, done, _ = env.step(action)\n",
    "                score += reward\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            scores.append(score)\n",
    "\n",
    "        # Close the environment\n",
    "        env.close()\n",
    "\n",
    "        # Print average score\n",
    "        print(\"Average score:\", np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c363019a",
   "metadata": {},
   "source": [
    "## 6. Training parameters and environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87c52f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = ['LunarLander-v2'] #list of environments 'MountainCar-v0',\n",
    "#list of implementing algos, each element consists of [DDQN is enabled?, Prioritized experience replay enabled?, type of algo]\n",
    "algos = [[True, False, 'DDQN']] #,, [True, True, 'PriorityDDQN'], [False, False, 'DQN'], \n",
    "seed = 1\n",
    "n_episodes = 2000 #number of training episodes\n",
    "max_t = 1000 #maximum number of timesteps\n",
    "epsilon_start = 0.7 #starting value of epsilon greedy\n",
    "epsilon_end = 0.01 #minimum value of epsilon\n",
    "epsilon_decay = 0.995 #rate at which epsilon decays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964c5660",
   "metadata": {},
   "source": [
    "## 7. Training implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4686fe2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Seed = 1\")\n",
    "for i in envs:\n",
    "    print(\"ENVIRONMENT:-----------\", i)\n",
    "    env = gym.make(i)\n",
    "    res=[]\n",
    "    for j in algos:\n",
    "        print(\"Algorithm:\", j[2])\n",
    "        rewards = []\n",
    "        aver_reward = []\n",
    "        aver = deque(maxlen=100)\n",
    "        state_size = env.observation_space.shape[0]\n",
    "        action_size=env.action_space.n\n",
    "        agent = Agent(state_size, action_size, seed, ddqn=j[0], priority=j[1])\n",
    "        epsilon = epsilon_start                    # initialize epsilon\n",
    "        \n",
    "        for i_episode in tqdm(range(1, n_episodes+1)):\n",
    "            if i_episode < 3 or i_episode > 1998:\n",
    "                print(\"Iteration number: \", i_episode-1)\n",
    "            state = env.reset()\n",
    "            score = 0\n",
    "            for t in range(max_t):\n",
    "                action = agent.act(state, epsilon)\n",
    "                step_result = env.step(action)\n",
    "                next_state, reward, done, _ = step_result[:4]\n",
    "                agent.step(state, action, reward, next_state, done, state_size)\n",
    "                state = next_state\n",
    "                score = score + reward\n",
    "                if done:\n",
    "                    break \n",
    "\n",
    "            aver.append(score)     \n",
    "            aver_reward.append(np.mean(aver))\n",
    "            rewards.append(score)\n",
    "            epsilon = max(epsilon_end, epsilon_decay*epsilon) # decrease epsilon\n",
    "            \n",
    "        reward=\"model/\"+i+\"_\"+j[2]+\"_\"+str(n_episodes)+\"_\"+str(datetime.now().strftime(\"%Y%m%d%H%M%S\"))\n",
    "        torch.save(agent.qnetwork_local.state_dict(),reward+'.pt')\n",
    "        res.append(aver_reward)\n",
    "        print(\"----------------End Algorithm--------------------\")\n",
    "    \n",
    "    fig=plt.figure()   \n",
    "    \n",
    "    reward='plots/'+i+'_result'+str(datetime.now().strftime(\"%Y%m%d%H%M%S\"))\n",
    "    df=pd.DataFrame({'DQN':res[0],'DDQN':res[1],'PriorityDDQN':res[2]})\n",
    "    df.to_csv(reward+'.csv')\n",
    "    print(\"------------------------End Environment-------------------\")\n",
    "    \n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.plot(df['DQN'], 'r', label='DQN')\n",
    "\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.plot(df['DDQN'], 'orange',label='DDQN')\n",
    "\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.plot(df['PriorityDDQN'],'b',label='PER')\n",
    "    \n",
    "    plt.title('Learning Curve '+i)\n",
    "\n",
    "    #Insert the legends in the plot\n",
    "    fig.legend(loc='lower right')\n",
    "    fig.savefig(reward+'.png', dpi=100)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4097372",
   "metadata": {},
   "source": [
    "## 8. Demonstration with random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63229b33",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load the environment\n",
    "env_name = 'MountainCar-v0'  # Change this to the environment you want to simulate\n",
    "env = gym.make(env_name, render_mode = 'human')\n",
    "\n",
    "# Get environment parameters\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "# Initialize the agent\n",
    "agent = Agent(state_size, action_size, seed=65)  \n",
    "\n",
    "# Simulate the model in the environment with random actions\n",
    "scores = []\n",
    "n_episodes = 100  # Number of episodes for simulation\n",
    "max_t = 1000  # Maximum number of timesteps per episode\n",
    "\n",
    "for i_episode in tqdm(range(1, n_episodes+1)):\n",
    "    state = env.reset()\n",
    "    env.render()\n",
    "    score = 0\n",
    "    \n",
    "    for t in range(max_t):\n",
    "        action = agent.act(state, epsilon=0.5) \n",
    "        step_result = env.step(action)\n",
    "        next_state, reward, done, _ = step_result[:4]\n",
    "        state = next_state\n",
    "        score = score + reward\n",
    "        if done:\n",
    "            break\n",
    "    print(score)        \n",
    "    scores.append(score)\n",
    "\n",
    "# Close the environment\n",
    "env.close()\n",
    "\n",
    "# Print average score\n",
    "print(\"Average score with random actions:\", np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c99acbc",
   "metadata": {},
   "source": [
    "## 9. Demonstration with learned policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726af1a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the environment\n",
    "env_name = 'MountainCar-v0'  # Change this to the environment you want to simulate\n",
    "env = gym.make(env_name, render_mode = 'human')\n",
    "\n",
    "# Get environment parameters\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "# Initialize the agent\n",
    "agent = Agent(state_size, action_size, seed=1)  \n",
    "# Load the model weights\n",
    "model_path = 'model/Seed42_MountainCar-v0_PriorityDDQN_4000_20240404234612.pt'  # Path to your trained model\n",
    "agent.qnetwork_local.load_state_dict(torch.load(model_path))\n",
    "agent.qnetwork_local.eval()\n",
    "\n",
    "# Simulate the model in the environment\n",
    "scores = []\n",
    "n_episodes = 100  # Number of episodes for simulation\n",
    "max_t = 1000  # Maximum number of timesteps per episode\n",
    "\n",
    "for i_episode in tqdm(range(1, n_episodes+1)):\n",
    "    state = env.reset()\n",
    "    env.render()\n",
    "    score = 0\n",
    "    \n",
    "    for t in range(max_t):\n",
    "        action = agent.act(state, epsilon=0.)  # Greedy action selection, no exploration\n",
    "        step_result = env.step(action)\n",
    "        next_state, reward, done, _ = step_result[:4]\n",
    "        state = next_state\n",
    "        score = score + reward\n",
    "        if done:\n",
    "            break\n",
    "    print(score)        \n",
    "    scores.append(score)\n",
    "\n",
    "# Close the environment\n",
    "env.close()\n",
    "\n",
    "# Print average score\n",
    "print(\"Average score:\", np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2aff9e",
   "metadata": {},
   "source": [
    "## 10. Draw the shaded plot for each case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec60e9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Read Data from CSV\n",
    "df = pd.read_csv('plots/DQN_LunarLander-v2_reward.csv')\n",
    "\n",
    "# Step 2: Remove the first row (Episodes, Run 1, Run 2, Run 3) to keep only the rewards data\n",
    "# df = df.drop(0)\n",
    "\n",
    "# Step 3: Convert the remaining DataFrame to numeric values\n",
    "df = df.apply(pd.to_numeric)\n",
    "\n",
    "# Step 4: Select only the three late columns (Run 1, Run 2, Run 3)\n",
    "later_columns = df.iloc[:, 1:4]\n",
    "\n",
    "# Step 5: Calculate Mean and Standard Deviation for the three later columns\n",
    "mean_values = later_columns.mean(axis=1)\n",
    "std_values = later_columns.std(axis=1)\n",
    "print(\"Standard deviation: \", std_values)\n",
    "print(\"Mean: \", mean_values)\n",
    "\n",
    "# Step 6: Plot the Data\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(mean_values, label='Mean Sum of Rewards', color = \"red\")\n",
    "plt.fill_between(mean_values.index, mean_values - std_values, mean_values + std_values, alpha=0.5, label='Standard Deviation', color = \"orange\")\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Sum of Rewards')\n",
    "plt.title('Shaded Plot of Sum of Rewards with Mean and Standard Deviation for Deep Q-Learning Algorithm, Lunar Lander')\n",
    "plt.savefig('plots/shaded_plot_DQN_LunarLander.png')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71670df9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
