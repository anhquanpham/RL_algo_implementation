{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2480ed9",
   "metadata": {},
   "source": [
    "# LSTM Deep Q Network for different environments "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc4a8ac",
   "metadata": {},
   "source": [
    "## 1. Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "606f2c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt #plotting library\n",
    "from matplotlib import animation #animated visualizations library\n",
    "from collections import namedtuple, deque \n",
    "#nametuple creates tuple subclasses with name fields, access elements by names instead of index\n",
    "#deque (double-ended queue) for adding and removing elements from both ends\n",
    "from tqdm import tqdm\n",
    "#add progress bars to Python code for easy monitoring progress of loops and tasks\n",
    "# %matplotlib inline \n",
    "import gym #environments for agents\n",
    "from datetime import datetime #manipulating dates and times\n",
    "import pandas as pd #work with structured data\n",
    "import torch #Pytorch supports tensor computations and neural networks\n",
    "import torch.nn as nn #Pytorch supports building neural networks\n",
    "import torch.nn.functional as F\n",
    "#common functions in neural network operations \n",
    "    # Activation functions (ReLU, sigmoid, tanh)\n",
    "    # Loss functions (cross_entropy, mse_loss)\n",
    "    #Utility functions for tensor manipulation (softmax, dropout, batch_norm, etc.)\n",
    "import torch.optim as optim #optimization algorithms for training neural networks\n",
    "import random #generate random numbers/selections\n",
    "from collections import namedtuple, deque \n",
    "import itertools \n",
    "# provides various functions for creating iterators and combining them for complex interators\n",
    "# includes cycle, chain, zip, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3593047",
   "metadata": {},
   "source": [
    "## 2. LSTM model for approximating Q-values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f571f047",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 64\n",
    "num_layers = 3\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_size, hidden_size, num_layers, action_size, seed):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "\n",
    "        self.hidden_dim = hidden_size\n",
    "        self.layer_dim = num_layers\n",
    "        \n",
    "        #LSTM model\n",
    "        self.lstm = nn.LSTM(input_size=state_size, hidden_size = 64, num_layers = 3, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, action_size)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        # Reshape state to have 3 dimensions: batch_size, sequence_length, input_size\n",
    "        # Assuming state has shape (batch_size, input_size)\n",
    "        h0 = torch.zeros(self.layer_dim, state.size(0), self.hidden_dim).requires_grad_()\n",
    "        c0 = torch.zeros(self.layer_dim, state.size(0), self.hidden_dim).requires_grad_()\n",
    "        # Reshape lstm_out to remove sequence length dimension\n",
    "        lstm_out, (hn,cn) = self.lstm(state, (h0.detach(), c0.detach()))\n",
    "        \n",
    "        lstm_out = self.fc(lstm_out[:, -1, :]) \n",
    "        \n",
    "        return lstm_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba2eecb",
   "metadata": {},
   "source": [
    "## 3. Implement the replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d6df66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\") #Device Initialization\n",
    "\n",
    "class ReplayBuffer: #Fixed-size buffer to store experience tuples.\n",
    "    # Initialize a ReplayBuffer object.\n",
    "    def __init__(self, state_size, action_size, buffer_size, batch_size, priority):\n",
    "            #state_size(int): dimension of each state\n",
    "            #action_size (int): dimension of each action \n",
    "            #buffer_size (int): max size of buffer \n",
    "            #batch_size (int): size of 1 training batch\n",
    "            #seed (int): random seed\n",
    "        self.priority = priority        \n",
    "        #Pointer to keep track of position within the replay buffer where next experiene will be added\n",
    "        # ptr = 0, new experience added -> increment to ptr = 1\n",
    "        self. ptr = 0\n",
    "        #Check if buffer have been filled\n",
    "        self.n = 0 \n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.state_size = state_size\n",
    "        \n",
    "        state_size = env.observation_space.shape[0]\n",
    "        action_size = env.action_space.n\n",
    "\n",
    "        #create states tensor full of zeros with {buffer_size} rows & {state_size} columns\n",
    "        #store in device CPU, allocate RAM to it \n",
    "        self.states = torch.zeros((1,)+(buffer_size,)+(state_size,)).to(device)\n",
    "        #Similar to above \n",
    "        self.next_states = torch.zeros((1,)+(buffer_size,)+(state_size,)).to(device)\n",
    "        #Only 1 action taken per experience tuple stored\n",
    "        # => action size = buffer size rows but 1 column only needed to store\n",
    "        self.actions = torch.zeros(1, buffer_size, dtype=torch.long).to(device) \n",
    "        # Reward same with action\n",
    "        self.rewards = torch.zeros(1, buffer_size, dtype=torch.float).to(device) \n",
    "        \n",
    "        # Flag to indicate transition/end between an episode ('done' flag)\n",
    "        # True/False floating values (0.0 False, 1.0 True)\n",
    "        self.dones = torch.zeros(1, buffer_size, dtype=torch.float).to(device)\n",
    "        \n",
    "        # Error in case implement prioritize replay\n",
    "        self.error = np.zeros((1, buffer_size), dtype=float)\n",
    "        \n",
    "        # Priority\n",
    "        self.priority = priority\n",
    "    \n",
    "  # Add new experience to buffer\n",
    "    def add(self, state, action, reward, next_state, done, state_size):\n",
    "        \n",
    "        state_size = env.observation_space.shape[0]\n",
    "\n",
    "        # Convert state and next_state to appropriate types\n",
    "        state = torch.tensor(state[0], dtype=torch.float32).reshape(1, 1,state_size)\n",
    "        state = state.to(device)\n",
    "            \n",
    "\n",
    "        next_state = torch.tensor(next_state[0], dtype=torch.float32).reshape(1, 1, state_size)\n",
    "        next_state = next_state.to(device)\n",
    "            \n",
    "        # Convert action, reward, and done flag to tensors\n",
    "        #action = torch.as_tensor(action, dtype=torch.float32).to(device)\n",
    "        reward = torch.as_tensor(reward, dtype=torch.float32).to(device)\n",
    "        done = torch.as_tensor(done, dtype=torch.bool).to(device)\n",
    "        \n",
    "        # Store the data in the replay buffer\n",
    "        self.states[0][self.ptr] = state\n",
    "        self.next_states[0][self.ptr] = next_state\n",
    "        self.actions[0][self.ptr] = action\n",
    "        self.rewards[0][self.ptr] = reward\n",
    "        self.dones[0][self.ptr] = done\n",
    "        \n",
    "        \n",
    "       \n",
    "        # Increment the pointer\n",
    "        self.ptr = (self.ptr + 1) % self.buffer_size\n",
    "\n",
    "        # Reset pointer and flag when buffer is filled\n",
    "        if self.ptr == 0:\n",
    "            self.n = self.buffer_size\n",
    "        \n",
    "    #Sample a batch of experience from memory\n",
    "    def sample(self, get_all=False):\n",
    "        n = len(self) # Length of the Buffer\n",
    "        state_size = env.observation_space.shape[0]\n",
    "        action_size = env.action_space.n\n",
    "        # Return all experience stored in buffer, no sampling\n",
    "        if get_all:\n",
    "            return self.states[:n], self.actions[:n], self.rewards[:n], self.next_states[:n], self.dones[:n]\n",
    "        \n",
    "        # else, do sampling: \n",
    "        else:\n",
    "            if self.priority:     \n",
    "            #enable prioritized experience replay\n",
    "            #experience are sampled based on priorities probability distribution p = self.error\n",
    "                idx = np.random.choice(n, self.batch_size, replace=False, p=self.error)\n",
    "            else: #uniform sampling\n",
    "                idx = np.random.choice(n, self.batch_size, replace=False)\n",
    "            #Replace = False to only sample once for each element\n",
    "            \n",
    "            states = torch.empty(1, self.batch_size, state_size)\n",
    "            next_states = torch.empty(1, self.batch_size, state_size)\n",
    "            actions = torch.empty(1, self.batch_size)\n",
    "            rewards = torch.empty(1, self.batch_size)\n",
    "            dones = torch.empty(1, self.batch_size)\n",
    "            #Retrieve sampled experiences \n",
    "            states[0] = self.states[0][idx]\n",
    "            next_states[0] = self.next_states[0][idx]\n",
    "            actions[0] = self.actions[0][idx]\n",
    "            rewards[0] = self.rewards[0][idx]\n",
    "            dones[0] = self.dones[0][idx]\n",
    "            \n",
    "            return (states, actions, rewards, next_states, dones), idx\n",
    "    \n",
    "    # Update the error associated with experiences in replay buffer\n",
    "    def update_error(self, error, idx=None): #specify index number, if not specify, all are updated\n",
    "        error = torch.abs(error.detach()) #absolute value of errors, detach to prevent gradient computation to be attached to error tensor\n",
    "        error = error / error.sum() #Normalize, ensure all errors add up to 1\n",
    "        if idx is not None: #index are specified, then only update specified indices\n",
    "            self.error[0][idx] = error.cpu().numpy()\n",
    "        else: # not specify index, all are updated\n",
    "            self.error[0][:len(self)] = error.cpu().numpy()\n",
    "    \n",
    "    # \n",
    "    def __len__(self): \n",
    "        if self.n == 0:\n",
    "            return self.ptr\n",
    "        else:\n",
    "            return self.n #when buffer is filled self.n stored that size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e6742c",
   "metadata": {},
   "source": [
    "## 4. Set the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b7e89a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = 100000 #Replay buffer size\n",
    "# batch_size = 3 #Minibatch size\n",
    "gamma = 0.99 #Discount factor\n",
    "tau = 1 #Soft update target parameters, tau = 1 = copy completely\n",
    "alpha = 0.0005 #learning rate\n",
    "update_every = 4 #How often to update the network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4817f2b6",
   "metadata": {},
   "source": [
    "## 5. Agent learning implementation¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0d1af1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(): #Learning by interacting with the environment\n",
    "    def __init__(self, state_size, action_size, hidden_size, num_layers, batch_size, seed, ddqn, priority):\n",
    "        self.state_size = state_size #Dimension of each state\n",
    "        self.action_size = action_size #Dimension of action\n",
    "        self.seed = random.seed(seed) #Choose the random seed\n",
    "        self.ddqn = ddqn #Store whether agent uses DDQN\n",
    "        self.priority = priority #Whether uses experience replay\n",
    "        self.batch_size = batch_size\n",
    "        #Q-Network\n",
    "        self.qnetwork_local = QNetwork(state_size, hidden_size, num_layers, action_size, seed).to(device)\n",
    "        self.qnetwork_target = QNetwork(state_size, hidden_size, num_layers, action_size, seed).to(device)\n",
    "        \n",
    "        #Initializes optimizer for updating the weights of local Q-Network\n",
    "        self.optimizer = torch.optim.SGD(self.qnetwork_local.parameters(), lr=alpha)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(state_size, (action_size,), buffer_size, batch_size, priority)\n",
    "        # Initialize timestep (to keep track timesteps for updating target)\n",
    "        self.t_step = 0\n",
    "        \n",
    "    def step(self, state, action, reward, next_state, done, step_size):\n",
    "        # Convert states to torch tensors and add batch dimension\n",
    "\n",
    "        # Save experience in replay memory\n",
    "        self.memory.add(state, action, reward, next_state, done, step_size)\n",
    "\n",
    "        self.t_step += 1  # Increment the timestep counter\n",
    "\n",
    "        # Update the Q-network after each step in the DQN case\n",
    "        if not self.ddqn and len(self.memory) > self.batch_size:\n",
    "            experiences, idx = self.memory.sample()\n",
    "            error = self.learn(experiences)\n",
    "            self.memory.update_error(error, idx)\n",
    "\n",
    "        # In the DDQN case, update the Q-network after a certain number of steps\n",
    "        if self.ddqn and (self.t_step % update_every) == 0 and len(self.memory) > self.batch_size:\n",
    "            experiences, idx = self.memory.sample()\n",
    "            error = self.learn(experiences)\n",
    "            self.memory.update_error(error, idx)\n",
    "\n",
    "                \n",
    " \n",
    "    def act(self, state, epsilon):\n",
    "\n",
    "        state = torch.tensor(state[0], dtype=torch.float32)\n",
    "\n",
    "        \n",
    "        self.qnetwork_local.eval()  # Evaluation mode\n",
    "        with torch.no_grad():  # Disable gradient calculation when choosing action\n",
    "            state_input = state.view(1,1,-1) #treat this tensor as a single time step of a sequence for a batch size of 1\n",
    "            action_values = self.qnetwork_local(state_input)  # Pass preprocessed states into local Q-network\n",
    "        self.qnetwork_local.train()  # Set local Q network back to training mode after inference is complete\n",
    "\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() > epsilon:\n",
    "            # Return action with highest Q-value\n",
    "            action = np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            # Select a random action\n",
    "            action = random.choice(range(self.action_size))\n",
    "        action = min(max(action, 0), self.action_size - 1) # Ensure action is within valid range\n",
    "        \n",
    "        return action   \n",
    "            \n",
    "    #Update value parameters using batch of experience tuples\n",
    "    def learn(self, experiences):\n",
    "        #experiences (Tuple[torch.Variable]): tuple of (s, a, r, s', done) tuples \n",
    "        #gamma (float): discount factor\n",
    "        \n",
    "        \n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        ## compute and minimize the loss\n",
    "        self.optimizer.zero_grad() #Resets all gradients to zero, gradients from previous batches do not accumulate\n",
    "        if i_episode < 3 or i_episode > 1998:    \n",
    "            print(\"Current state: \", state)\n",
    "            print(\" \")\n",
    "            print(\"Experiences: \")\n",
    "            print(\"States: \", states)\n",
    "            print(\"Actions: \", actions)\n",
    "            print(\"Rewards: \", rewards)\n",
    "            print(\"Next states: \", next_states)\n",
    "            print(\" \")\n",
    "        #Same as explained above\n",
    "        if self.ddqn: #DDQN\n",
    "            old_val = self.qnetwork_local(states).gather(-1, actions)\n",
    "            if i_episode < 3 or i_episode > 1998:\n",
    "                print(\"Q_current: \", old_val)\n",
    "                print(\" \")\n",
    "            with torch.no_grad():\n",
    "                if i_episode < 3 or i_episode > 1998:\n",
    "                    print(\"All Q values for next_states in order to choose actions, estimate with local network: \")\n",
    "                    print(self.qnetwork_local(next_states))\n",
    "                    print(\" \")\n",
    "                next_actions = self.qnetwork_local(next_states).argmax(-1, keepdim=True)\n",
    "                if i_episode < 3 or i_episode > 1998:\n",
    "                    print(\"Actions, chose with local network = \", next_actions)\n",
    "                    print(\" \")\n",
    "                    print(\"All Q values for next_states, estimate with target network: \")\n",
    "                    print(self.qnetwork_target(next_states))\n",
    "                    print(\" \")\n",
    "                maxQ = self.qnetwork_target(next_states).gather(-1, next_actions)\n",
    "                if i_episode < 3 or i_episode > 1998:\n",
    "                    print(\"Chosen maximum Q - value from actions gathered above = \", maxQ)\n",
    "                    print(\" \")\n",
    "                target = rewards+gamma*maxQ*(1-dones)\n",
    "                if i_episode < 3 or i_episode > 1998:\n",
    "                    print(\"Q_target = \", target)\n",
    "                    print(\" \")\n",
    "        else: # Normal DQN\n",
    "            old_val = self.qnetwork_local(states)[0][action]\n",
    "            if i_episode < 3 or i_episode > 1998:\n",
    "                print(\"All Q_current: \",self.qnetwork_local(states))\n",
    "                print(\" \")\n",
    "\n",
    "                print(\"Q_current: \", old_val)\n",
    "                print(\" \")\n",
    "\n",
    "            with torch.no_grad():\n",
    "                if i_episode < 3 or i_episode > 1998:\n",
    "                    print(\"All Q values for next_states = \")\n",
    "                    print(self.qnetwork_target(next_states))\n",
    "                    print(\" \")\n",
    "                maxQ = self.qnetwork_target(next_states).max(-1, keepdim=True)[0]\n",
    "                if i_episode < 3 or i_episode > 1998:\n",
    "                    print(\"Chosen maximum Q - value= \", maxQ)\n",
    "                    print(\" \")\n",
    "                target = reward+gamma*maxQ*(1-done)\n",
    "                if i_episode < 3 or i_episode > 1998:\n",
    "                    print(\"Q_target = \", target)\n",
    "                    print(\" \")\n",
    "               \n",
    "        if i_episode < 3 or i_episode > 1998:\n",
    "            print(\"Error(Q_target - Q_current): \", target - old_val)\n",
    "            print(\" \")\n",
    "        \n",
    "            # mean-squared error for regression\n",
    "        loss = F.mse_loss(old_val, target) #Calculate loss of current and target Q-values\n",
    "        loss.backward() #Gradient of loss with respect to Q-network parameters are computed\n",
    "                        #and used to update the parameters\n",
    "        self.optimizer.step() #update the neural network\n",
    "                                #step method applies optimization to update parameters\n",
    "                                #steps:\n",
    "                                #1. Optimizer uses computed gradients to update parameters\n",
    "                                #2. Adam optimizer is applied to each parameter based on the gradients and learning rate\n",
    "                                #3. Gradients are cleared back to zero as first step for next training iteration\n",
    "\n",
    "        ##UPDATE THE TARGET NETWORK##\n",
    "        if not self.ddqn:  # If DQN, update after each episode\n",
    "            self.soft_update(self.qnetwork_local, self.qnetwork_target, tau) \n",
    "        else:  # If DDQN, update after every update_every episodes\n",
    "            if (self.t_step % update_every) == 0:\n",
    "                self.soft_update(self.qnetwork_local, self.qnetwork_target, tau)\n",
    "        \n",
    "        return old_val - target #temporal difference TD error between old Q and target Q to monitor training progress\n",
    "\n",
    "    \n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        #local_model: online model, actively being trained. Weights will be copied from here\n",
    "        #target_model: use to generate target Q-values during training. Weights will be copied to here\n",
    "        #tau: interpolation parameters determine rate at which parameters of target models are updated\n",
    "        #small tau slower update, big tau faster update, less stable\n",
    "        \n",
    "        #function iterates over parameters of both target model and local model using zip\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            #for each target param - local param pair, update target param by the formula\n",
    "            # target_param = tau*local_param + (1-tau)*target_local \n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "            \n",
    "    # Function to simulate a model in an environment\n",
    "    def simulate_model(env_name, model_path):\n",
    "        # Load the environment\n",
    "        env = gym.make(env_name)\n",
    "\n",
    "        # Get environment parameters\n",
    "        state_size = env.observation_space.shape[0]\n",
    "        action_size = env.action_space.n\n",
    "\n",
    "        # Initialize the agent\n",
    "        agent = Agent(state_size, action_size, seed) \n",
    "\n",
    "        # Load the model weights\n",
    "        agent.qnetwork_local.load_state_dict(torch.load(model_path))\n",
    "        agent.qnetwork_local.eval()\n",
    "\n",
    "        # Simulate the model in the environment\n",
    "        scores = []\n",
    "        n_episodes = 100  # Number of episodes for simulation\n",
    "        max_t = 1000  # Maximum number of timesteps per episode\n",
    "\n",
    "        for i_episode in tqdm(range(1, n_episodes+1)):\n",
    "            state = env.reset()\n",
    "            score = 0\n",
    "\n",
    "            for t in range(max_t):\n",
    "                action = agent.act(state, epsilon=0.)  # Greedy action selection, no exploration\n",
    "                state, reward, done, _ = env.step(action)\n",
    "                score += reward\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            scores.append(score)\n",
    "\n",
    "        # Close the environment\n",
    "        env.close()\n",
    "\n",
    "        # Print average score\n",
    "        print(\"Average score:\", np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c721e03",
   "metadata": {},
   "source": [
    "## 6. Training parameters and environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bc87b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = ['LunarLander-v2'] #list of environments  ,'\n",
    "#list of implementing algos, each element consists of [DDQN is enabled?, Prioritized experience replay enabled?, type of algo]\n",
    "algos = [[False, False, 'DQN']] #, [True, False, 'DDQN'], [True, True, 'PriorityDDQN']]\n",
    "n_episodes = 2000 #number of training episodes\n",
    "max_t = 1000 #maximum number of timesteps\n",
    "epsilon_start = 0.99 #starting value of epsilon greedy\n",
    "epsilon_end = 0.01 #minimum value of epsilon\n",
    "epsilon_decay = 0.995 #rate at which epsilon decays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71578dd0",
   "metadata": {},
   "source": [
    "## 7. Training implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e088cb1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed = 1\n",
      "ENVIRONMENT:----------- LunarLander-v2\n",
      "Algorithm: DQN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                      | 0/4000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number:  0\n",
      "Current state:  (array([ 0.01484909,  1.378611  ,  0.30349433, -0.3402862 , -0.0235381 ,\n",
      "       -0.1007756 ,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0029,  1.4056,  0.2952, -0.2345, -0.0034, -0.0669,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0088,  1.3933,  0.3039, -0.2871, -0.0118, -0.1026,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0148,  1.3786,  0.3035, -0.3403, -0.0235, -0.1008,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0119,  1.3863,  0.3117, -0.3142, -0.0185, -0.1339,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 3., 0., 1.]])\n",
      "Rewards:  tensor([[-1.3631, -2.4486, -1.7064, -1.1112]])\n",
      "Next states:  tensor([[[ 0.0058,  1.3998,  0.2948, -0.2602, -0.0067, -0.0661,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0119,  1.3863,  0.3117, -0.3142, -0.0185, -0.1339,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0178,  1.3704,  0.3035, -0.3670, -0.0286, -0.1008,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0148,  1.3786,  0.3035, -0.3403, -0.0235, -0.1008,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1041,  0.0154,  0.0809, -0.0981]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1041, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1041,  0.0154,  0.0810, -0.0981]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0810]])\n",
      " \n",
      "Q_target =  tensor([[-1.6262]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-1.5221]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.01783323,  1.3703564 ,  0.30350837, -0.3669563 , -0.0285767 ,\n",
      "       -0.10078132,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0178,  1.3704,  0.3035, -0.3670, -0.0286, -0.1008,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0148,  1.3786,  0.3035, -0.3403, -0.0235, -0.1008,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0119,  1.3863,  0.3117, -0.3142, -0.0185, -0.1339,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0088,  1.3933,  0.3039, -0.2871, -0.0118, -0.1026,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 0., 1., 3.]])\n",
      "Rewards:  tensor([[-2.6538, -1.7064, -1.1112, -2.4486]])\n",
      "Next states:  tensor([[[ 0.0209,  1.3615,  0.3142, -0.3942, -0.0358, -0.1437,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0178,  1.3704,  0.3035, -0.3670, -0.0286, -0.1008,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0148,  1.3786,  0.3035, -0.3403, -0.0235, -0.1008,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0119,  1.3863,  0.3117, -0.3142, -0.0185, -0.1339,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1061,  0.0154,  0.0810, -0.0981]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.0981, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1061,  0.0155,  0.0810, -0.0981]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0810]])\n",
      " \n",
      "Q_target =  tensor([[-2.5736]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-2.4756]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.02090254,  1.3614908 ,  0.31419548, -0.39418668, -0.03575917,\n",
      "       -0.14366265,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0209,  1.3615,  0.3142, -0.3942, -0.0358, -0.1437,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0029,  1.4056,  0.2952, -0.2345, -0.0034, -0.0669,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0119,  1.3863,  0.3117, -0.3142, -0.0185, -0.1339,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0178,  1.3704,  0.3035, -0.3670, -0.0286, -0.1008,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 1., 3.]])\n",
      "Rewards:  tensor([[-1.8901, -1.3631, -1.1112, -2.6538]])\n",
      "Next states:  tensor([[[ 0.0240,  1.3520,  0.3142, -0.4209, -0.0429, -0.1436,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0058,  1.3998,  0.2948, -0.2602, -0.0067, -0.0661,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0148,  1.3786,  0.3035, -0.3403, -0.0235, -0.1008,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0209,  1.3615,  0.3142, -0.3942, -0.0358, -0.1437,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1061,  0.0155,  0.0810, -0.1015]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1061, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1061,  0.0155,  0.0810, -0.1014]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0810]])\n",
      " \n",
      "Q_target =  tensor([[-1.8099]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-1.7038]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.02397213,  1.3520259 ,  0.31421715, -0.42085883, -0.04293955,\n",
      "       -0.14362118,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0119,  1.3863,  0.3117, -0.3142, -0.0185, -0.1339,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0148,  1.3786,  0.3035, -0.3403, -0.0235, -0.1008,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0209,  1.3615,  0.3142, -0.3942, -0.0358, -0.1437,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0029,  1.4056,  0.2952, -0.2345, -0.0034, -0.0669,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[1., 0., 0., 0.]])\n",
      "Rewards:  tensor([[-1.1112, -1.7064, -1.8901, -1.3631]])\n",
      "Next states:  tensor([[[ 0.0148,  1.3786,  0.3035, -0.3403, -0.0235, -0.1008,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0178,  1.3704,  0.3035, -0.3670, -0.0286, -0.1008,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0240,  1.3520,  0.3142, -0.4209, -0.0429, -0.1436,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0058,  1.3998,  0.2948, -0.2602, -0.0067, -0.0661,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1084,  0.0155,  0.0810, -0.1014]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.0810, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1083,  0.0155,  0.0810, -0.1014]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0810]])\n",
      " \n",
      "Q_target =  tensor([[2.0813]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[2.0003]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.02720709,  1.3434591 ,  0.33016178, -0.3809497 , -0.04953805,\n",
      "       -0.13198197,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0240,  1.3520,  0.3142, -0.4209, -0.0429, -0.1436,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0209,  1.3615,  0.3142, -0.3942, -0.0358, -0.1437,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0178,  1.3704,  0.3035, -0.3670, -0.0286, -0.1008,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0148,  1.3786,  0.3035, -0.3403, -0.0235, -0.1008,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[2., 0., 3., 0.]])\n",
      "Rewards:  tensor([[ 2.0011, -1.8901, -2.6538, -1.7064]])\n",
      "Next states:  tensor([[[ 0.0272,  1.3435,  0.3302, -0.3809, -0.0495, -0.1320,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0240,  1.3520,  0.3142, -0.4209, -0.0429, -0.1436,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0209,  1.3615,  0.3142, -0.3942, -0.0358, -0.1437,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0178,  1.3704,  0.3035, -0.3670, -0.0286, -0.1008,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1083,  0.0156,  0.0838, -0.1014]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.0156, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1083,  0.0156,  0.0838, -0.1014]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0838]])\n",
      " \n",
      "Q_target =  tensor([[-0.7350]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.7506]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.03034353,  1.3342881 ,  0.31780744, -0.40774634, -0.05366091,\n",
      "       -0.08246449,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0029,  1.4056,  0.2952, -0.2345, -0.0034, -0.0669,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0148,  1.3786,  0.3035, -0.3403, -0.0235, -0.1008,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0058,  1.3998,  0.2948, -0.2602, -0.0067, -0.0661,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0209,  1.3615,  0.3142, -0.3942, -0.0358, -0.1437,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 3., 0.]])\n",
      "Rewards:  tensor([[-1.3631, -1.7064, -2.3868, -1.8901]])\n",
      "Next states:  tensor([[[ 0.0058,  1.3998,  0.2948, -0.2602, -0.0067, -0.0661,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0178,  1.3704,  0.3035, -0.3670, -0.0286, -0.1008,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0088,  1.3933,  0.3039, -0.2871, -0.0118, -0.1026,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0240,  1.3520,  0.3142, -0.4209, -0.0429, -0.1436,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1084,  0.0145,  0.0836, -0.1014]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1084, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1084,  0.0145,  0.0837, -0.1014]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0837]])\n",
      " \n",
      "Q_target =  tensor([[-1.4889]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-1.3805]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.03348017,  1.3245173 ,  0.31781903, -0.4344103 , -0.057784  ,\n",
      "       -0.08246902,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0303,  1.3343,  0.3178, -0.4077, -0.0537, -0.0825,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0240,  1.3520,  0.3142, -0.4209, -0.0429, -0.1436,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0029,  1.4056,  0.2952, -0.2345, -0.0034, -0.0669,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0088,  1.3933,  0.3039, -0.2871, -0.0118, -0.1026,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 2., 0., 3.]])\n",
      "Rewards:  tensor([[-1.5717,  2.0011, -1.3631, -2.4486]])\n",
      "Next states:  tensor([[[ 0.0335,  1.3245,  0.3178, -0.4344, -0.0578, -0.0825,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0272,  1.3435,  0.3302, -0.3809, -0.0495, -0.1320,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0058,  1.3998,  0.2948, -0.2602, -0.0067, -0.0661,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0119,  1.3863,  0.3117, -0.3142, -0.0185, -0.1339,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1102,  0.0145,  0.0837, -0.1013]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1102, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1102,  0.0146,  0.0837, -0.1013]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0837]])\n",
      " \n",
      "Q_target =  tensor([[-1.4762]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-1.3660]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.0366169 ,  1.3141466 ,  0.3178311 , -0.46107978, -0.06190591,\n",
      "       -0.08244571,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0058,  1.3998,  0.2948, -0.2602, -0.0067, -0.0661,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0303,  1.3343,  0.3178, -0.4077, -0.0537, -0.0825,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0148,  1.3786,  0.3035, -0.3403, -0.0235, -0.1008,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0240,  1.3520,  0.3142, -0.4209, -0.0429, -0.1436,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 0., 0., 2.]])\n",
      "Rewards:  tensor([[-2.3868, -1.5717, -1.7064,  2.0011]])\n",
      "Next states:  tensor([[[ 0.0088,  1.3933,  0.3039, -0.2871, -0.0118, -0.1026,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0335,  1.3245,  0.3178, -0.4344, -0.0578, -0.0825,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0178,  1.3704,  0.3035, -0.3670, -0.0286, -0.1008,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0272,  1.3435,  0.3302, -0.3809, -0.0495, -0.1320,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1120,  0.0145,  0.0837, -0.1013]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1120, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1120,  0.0146,  0.0837, -0.1013]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0837]])\n",
      " \n",
      "Q_target =  tensor([[-1.4580]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-1.3460]], grad_fn=<SubBackward0>)\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\phama\\AppData\\Local\\Temp\\ipykernel_16512\\1174236218.py:138: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = F.mse_loss(old_val, target) #Calculate loss of current and target Q-values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current state:  (array([ 0.03975372,  1.3031764 ,  0.31784317, -0.4877499 , -0.06602719,\n",
      "       -0.08243327,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0119,  1.3863,  0.3117, -0.3142, -0.0185, -0.1339,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0029,  1.4056,  0.2952, -0.2345, -0.0034, -0.0669,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0335,  1.3245,  0.3178, -0.4344, -0.0578, -0.0825,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0398,  1.3032,  0.3178, -0.4877, -0.0660, -0.0824,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[1., 0., 0., 3.]])\n",
      "Rewards:  tensor([[-1.1112, -1.3631, -1.5591, -2.5052]])\n",
      "Next states:  tensor([[[ 0.0148,  1.3786,  0.3035, -0.3403, -0.0235, -0.1008,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0058,  1.3998,  0.2948, -0.2602, -0.0067, -0.0661,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0366,  1.3141,  0.3178, -0.4611, -0.0619, -0.0824,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0430,  1.2916,  0.3298, -0.5154, -0.0726, -0.1306,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1138,  0.0145,  0.0837, -0.1013]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1013, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1138,  0.0146,  0.0837, -0.1013]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0837]])\n",
      " \n",
      "Q_target =  tensor([[-2.4223]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-2.3210]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.04298554,  1.2915866 ,  0.32977155, -0.51540565, -0.07255573,\n",
      "       -0.13058266,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0240,  1.3520,  0.3142, -0.4209, -0.0429, -0.1436,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0303,  1.3343,  0.3178, -0.4077, -0.0537, -0.0825,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0430,  1.2916,  0.3298, -0.5154, -0.0726, -0.1306,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0148,  1.3786,  0.3035, -0.3403, -0.0235, -0.1008,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[2., 0., 3., 0.]])\n",
      "Rewards:  tensor([[ 2.0011, -1.5717, -2.5061, -1.7064]])\n",
      "Next states:  tensor([[[ 0.0272,  1.3435,  0.3302, -0.3809, -0.0495, -0.1320,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0335,  1.3245,  0.3178, -0.4344, -0.0578, -0.0825,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0463,  1.2794,  0.3392, -0.5431, -0.0810, -0.1687,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0178,  1.3704,  0.3035, -0.3670, -0.0286, -0.1008,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1137,  0.0146,  0.0838, -0.1044]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1044, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1137,  0.0146,  0.0838, -0.1044]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0838]])\n",
      " \n",
      "Q_target =  tensor([[-2.4231]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-2.3188]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.04629259,  1.2793778 ,  0.3392111 , -0.5430507 , -0.08099195,\n",
      "       -0.16873994,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0303,  1.3343,  0.3178, -0.4077, -0.0537, -0.0825,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0148,  1.3786,  0.3035, -0.3403, -0.0235, -0.1008,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0029,  1.4056,  0.2952, -0.2345, -0.0034, -0.0669,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0272,  1.3435,  0.3302, -0.3809, -0.0495, -0.1320,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 1.]])\n",
      "Rewards:  tensor([[-1.5717, -1.7064, -1.3631, -0.8179]])\n",
      "Next states:  tensor([[[ 0.0335,  1.3245,  0.3178, -0.4344, -0.0578, -0.0825,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0178,  1.3704,  0.3035, -0.3670, -0.0286, -0.1008,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0058,  1.3998,  0.2948, -0.2602, -0.0067, -0.0661,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0303,  1.3343,  0.3178, -0.4077, -0.0537, -0.0825,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1137,  0.0146,  0.0836, -0.1076]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.0146, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1137,  0.0146,  0.0836, -0.1076]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0836]])\n",
      " \n",
      "Q_target =  tensor([[-0.9581]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.9727]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.04950943,  1.2665782 ,  0.32789645, -0.5692248 , -0.08714644,\n",
      "       -0.12310112,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0272,  1.3435,  0.3302, -0.3809, -0.0495, -0.1320,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0088,  1.3933,  0.3039, -0.2871, -0.0118, -0.1026,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0240,  1.3520,  0.3142, -0.4209, -0.0429, -0.1436,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0366,  1.3141,  0.3178, -0.4611, -0.0619, -0.0824,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[1., 3., 2., 0.]])\n",
      "Rewards:  tensor([[-0.8179, -2.4486,  2.0011, -1.5409]])\n",
      "Next states:  tensor([[[ 0.0303,  1.3343,  0.3178, -0.4077, -0.0537, -0.0825,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0119,  1.3863,  0.3117, -0.3142, -0.0185, -0.1339,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0272,  1.3435,  0.3302, -0.3809, -0.0495, -0.1320,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0398,  1.3032,  0.3178, -0.4877, -0.0660, -0.0824,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1137,  0.0133,  0.0837, -0.1076]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1076, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1137,  0.0133,  0.0837, -0.1076]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0837]])\n",
      " \n",
      "Q_target =  tensor([[-2.1961]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-2.0885]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.05279236,  1.2531643 ,  0.33618242, -0.59664714, -0.0949769 ,\n",
      "       -0.15662372,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0463,  1.2794,  0.3392, -0.5431, -0.0810, -0.1687,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0430,  1.2916,  0.3298, -0.5154, -0.0726, -0.1306,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0303,  1.3343,  0.3178, -0.4077, -0.0537, -0.0825,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0335,  1.3245,  0.3178, -0.4344, -0.0578, -0.0825,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[1., 3., 0., 0.]])\n",
      "Rewards:  tensor([[-1.0409, -2.5061, -1.5717, -1.5591]])\n",
      "Next states:  tensor([[[ 0.0495,  1.2666,  0.3279, -0.5692, -0.0871, -0.1231,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0463,  1.2794,  0.3392, -0.5431, -0.0810, -0.1687,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0335,  1.3245,  0.3178, -0.4344, -0.0578, -0.0825,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0366,  1.3141,  0.3178, -0.4611, -0.0619, -0.0824,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1135,  0.0134,  0.0838, -0.1103]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.0838, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1135,  0.0135,  0.0838, -0.1103]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0838]])\n",
      " \n",
      "Q_target =  tensor([[1.1017]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[1.0179]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.05624599,  1.2401698 ,  0.35274366, -0.57801753, -0.10231151,\n",
      "       -0.14670545,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0463,  1.2794,  0.3392, -0.5431, -0.0810, -0.1687,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0119,  1.3863,  0.3117, -0.3142, -0.0185, -0.1339,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0430,  1.2916,  0.3298, -0.5154, -0.0726, -0.1306,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0562,  1.2402,  0.3527, -0.5780, -0.1023, -0.1467,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[1., 1., 3., 1.]])\n",
      "Rewards:  tensor([[-1.0409, -1.1112, -2.5061, -1.1889]])\n",
      "Next states:  tensor([[[ 0.0495,  1.2666,  0.3279, -0.5692, -0.0871, -0.1231,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0148,  1.3786,  0.3035, -0.3403, -0.0235, -0.1008,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0463,  1.2794,  0.3392, -0.5431, -0.0810, -0.1687,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0596,  1.2266,  0.3452, -0.6047, -0.1081, -0.1167,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1135,  0.0135,  0.0852, -0.1104]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.0135, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1135,  0.0135,  0.0852, -0.1103]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0852]])\n",
      " \n",
      "Q_target =  tensor([[-1.1045]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-1.1180]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.05963964,  1.226573  ,  0.3452427 , -0.6047117 , -0.10814749,\n",
      "       -0.11673017,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0463,  1.2794,  0.3392, -0.5431, -0.0810, -0.1687,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0029,  1.4056,  0.2952, -0.2345, -0.0034, -0.0669,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0209,  1.3615,  0.3142, -0.3942, -0.0358, -0.1437,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0148,  1.3786,  0.3035, -0.3403, -0.0235, -0.1008,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[1., 0., 0., 0.]])\n",
      "Rewards:  tensor([[-1.0409, -1.3631, -1.8901, -1.7064]])\n",
      "Next states:  tensor([[[ 0.0495,  1.2666,  0.3279, -0.5692, -0.0871, -0.1231,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0058,  1.3998,  0.2948, -0.2602, -0.0067, -0.0661,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0240,  1.3520,  0.3142, -0.4209, -0.0429, -0.1436,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0178,  1.3704,  0.3035, -0.3670, -0.0286, -0.1008,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1136,  0.0119,  0.0850, -0.1104]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.0850, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1136,  0.0119,  0.0850, -0.1103]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0850]])\n",
      " \n",
      "Q_target =  tensor([[-0.3556]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.4406]], grad_fn=<SubBackward0>)\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current state:  (array([ 0.06322823,  1.2129712 ,  0.36401555, -0.60491544, -0.11327842,\n",
      "       -0.10262765,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0463,  1.2794,  0.3392, -0.5431, -0.0810, -0.1687,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0335,  1.3245,  0.3178, -0.4344, -0.0578, -0.0825,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0178,  1.3704,  0.3035, -0.3670, -0.0286, -0.1008,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0303,  1.3343,  0.3178, -0.4077, -0.0537, -0.0825,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[1., 0., 3., 0.]])\n",
      "Rewards:  tensor([[-1.0409, -1.5591, -2.6538, -1.5717]])\n",
      "Next states:  tensor([[[ 0.0495,  1.2666,  0.3279, -0.5692, -0.0871, -0.1231,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0366,  1.3141,  0.3178, -0.4611, -0.0619, -0.0824,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0209,  1.3615,  0.3142, -0.3942, -0.0358, -0.1437,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0335,  1.3245,  0.3178, -0.4344, -0.0578, -0.0825,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1136,  0.0119,  0.0845, -0.1103]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1103, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1135,  0.0120,  0.0845, -0.1103]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0845]])\n",
      " \n",
      "Q_target =  tensor([[-2.2349]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-2.1245]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.06690903,  1.1987576 ,  0.37556314, -0.6322921 , -0.12073488,\n",
      "       -0.14914232,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0119,  1.3863,  0.3117, -0.3142, -0.0185, -0.1339,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0366,  1.3141,  0.3178, -0.4611, -0.0619, -0.0824,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0669,  1.1988,  0.3756, -0.6323, -0.1207, -0.1491,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0240,  1.3520,  0.3142, -0.4209, -0.0429, -0.1436,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[1., 0., 0., 2.]])\n",
      "Rewards:  tensor([[-1.1112, -1.5409, -1.5943,  2.0011]])\n",
      "Next states:  tensor([[[ 0.0148,  1.3786,  0.3035, -0.3403, -0.0235, -0.1008,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0398,  1.3032,  0.3178, -0.4877, -0.0660, -0.0824,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0706,  1.1839,  0.3756, -0.6590, -0.1282, -0.1491,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0272,  1.3435,  0.3302, -0.3809, -0.0495, -0.1320,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1135,  0.0120,  0.0845, -0.1133]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1135, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1135,  0.0120,  0.0845, -0.1132]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0845]])\n",
      " \n",
      "Q_target =  tensor([[-1.5106]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-1.3971]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.07059002,  1.1839448 ,  0.37558445, -0.6589642 , -0.12818894,\n",
      "       -0.14909482,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0495,  1.2666,  0.3279, -0.5692, -0.0871, -0.1231,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0596,  1.2266,  0.3452, -0.6047, -0.1081, -0.1167,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0148,  1.3786,  0.3035, -0.3403, -0.0235, -0.1008,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0029,  1.4056,  0.2952, -0.2345, -0.0034, -0.0669,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 2., 0., 0.]])\n",
      "Rewards:  tensor([[-2.2790, -0.4397, -1.7064, -1.3631]])\n",
      "Next states:  tensor([[[ 0.0528,  1.2532,  0.3362, -0.5966, -0.0950, -0.1566,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0632,  1.2130,  0.3640, -0.6049, -0.1133, -0.1026,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0178,  1.3704,  0.3035, -0.3670, -0.0286, -0.1008,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0058,  1.3998,  0.2948, -0.2602, -0.0067, -0.0661,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1154,  0.0120,  0.0845, -0.1132]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.0845, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1153,  0.0120,  0.0845, -0.1132]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0845]])\n",
      " \n",
      "Q_target =  tensor([[1.7503]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[1.6658]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.07417393,  1.1693717 ,  0.36665413, -0.64842373, -0.13642345,\n",
      "       -0.16470522,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0088,  1.3933,  0.3039, -0.2871, -0.0118, -0.1026,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0119,  1.3863,  0.3117, -0.3142, -0.0185, -0.1339,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0596,  1.2266,  0.3452, -0.6047, -0.1081, -0.1167,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0632,  1.2130,  0.3640, -0.6049, -0.1133, -0.1026,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 1., 2., 3.]])\n",
      "Rewards:  tensor([[-2.4486, -1.1112, -0.4397, -2.3185]])\n",
      "Next states:  tensor([[[ 0.0119,  1.3863,  0.3117, -0.3142, -0.0185, -0.1339,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0148,  1.3786,  0.3035, -0.3403, -0.0235, -0.1008,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0632,  1.2130,  0.3640, -0.6049, -0.1133, -0.1026,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0669,  1.1988,  0.3756, -0.6323, -0.1207, -0.1491,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1154,  0.0120,  0.0868, -0.1132]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.0868, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1154,  0.0120,  0.0868, -0.1132]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0868]])\n",
      " \n",
      "Q_target =  tensor([[3.4109]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[3.3241]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.07774363,  1.1556145 ,  0.3658827 , -0.61226565, -0.14531823,\n",
      "       -0.1779113 ,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0303,  1.3343,  0.3178, -0.4077, -0.0537, -0.0825,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0632,  1.2130,  0.3640, -0.6049, -0.1133, -0.1026,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0742,  1.1694,  0.3667, -0.6484, -0.1364, -0.1647,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0706,  1.1839,  0.3756, -0.6590, -0.1282, -0.1491,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 3., 2., 2.]])\n",
      "Rewards:  tensor([[-1.5717, -2.3185,  3.3249,  1.6666]])\n",
      "Next states:  tensor([[[ 0.0335,  1.3245,  0.3178, -0.4344, -0.0578, -0.0825,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0669,  1.1988,  0.3756, -0.6323, -0.1207, -0.1491,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0777,  1.1556,  0.3659, -0.6123, -0.1453, -0.1779,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0742,  1.1694,  0.3667, -0.6484, -0.1364, -0.1647,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1152,  0.0123,  0.0915, -0.1131]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1131, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1152,  0.0123,  0.0916, -0.1131]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0916]])\n",
      " \n",
      "Q_target =  tensor([[-2.4797]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-2.3666]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.08139191,  1.1412458 ,  0.37570134, -0.63970256, -0.15619408,\n",
      "       -0.2175364 ,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0148,  1.3786,  0.3035, -0.3403, -0.0235, -0.1008,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0240,  1.3520,  0.3142, -0.4209, -0.0429, -0.1436,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0178,  1.3704,  0.3035, -0.3670, -0.0286, -0.1008,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0119,  1.3863,  0.3117, -0.3142, -0.0185, -0.1339,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 2., 3., 1.]])\n",
      "Rewards:  tensor([[-1.7064,  2.0011, -2.6538, -1.1112]])\n",
      "Next states:  tensor([[[ 0.0178,  1.3704,  0.3035, -0.3670, -0.0286, -0.1008,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0272,  1.3435,  0.3302, -0.3809, -0.0495, -0.1320,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0209,  1.3615,  0.3142, -0.3942, -0.0358, -0.1437,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0148,  1.3786,  0.3035, -0.3403, -0.0235, -0.1008,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1154,  0.0121,  0.0912, -0.1164]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.0121, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1154,  0.0121,  0.0912, -0.1164]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0912]])\n",
      " \n",
      "Q_target =  tensor([[-1.1294]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-1.1415]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.08495998,  1.126287  ,  0.36564958, -0.66579014, -0.16502921,\n",
      "       -0.17670283,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0240,  1.3520,  0.3142, -0.4209, -0.0429, -0.1436,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0562,  1.2402,  0.3527, -0.5780, -0.1023, -0.1467,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0742,  1.1694,  0.3667, -0.6484, -0.1364, -0.1647,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0596,  1.2266,  0.3452, -0.6047, -0.1081, -0.1167,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[2., 1., 2., 2.]])\n",
      "Rewards:  tensor([[ 2.0011, -1.1889,  3.3249, -0.4397]])\n",
      "Next states:  tensor([[[ 0.0272,  1.3435,  0.3302, -0.3809, -0.0495, -0.1320,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0596,  1.2266,  0.3452, -0.6047, -0.1081, -0.1167,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0777,  1.1556,  0.3659, -0.6123, -0.1453, -0.1779,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0632,  1.2130,  0.3640, -0.6049, -0.1133, -0.1026,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1152,  0.0108,  0.0915, -0.1163]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1163, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1152,  0.0108,  0.0915, -0.1164]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0915]])\n",
      " \n",
      "Q_target =  tensor([[-2.2037]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-2.0874]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.08858728,  1.1107186 ,  0.37306267, -0.6931114 , -0.17536916,\n",
      "       -0.2067995 ,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0148,  1.3786,  0.3035, -0.3403, -0.0235, -0.1008,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0058,  1.3998,  0.2948, -0.2602, -0.0067, -0.0661,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0366,  1.3141,  0.3178, -0.4611, -0.0619, -0.0824,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0596,  1.2266,  0.3452, -0.6047, -0.1081, -0.1167,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 3., 0., 2.]])\n",
      "Rewards:  tensor([[-1.7064, -2.3868, -1.5409, -0.4397]])\n",
      "Next states:  tensor([[[ 0.0178,  1.3704,  0.3035, -0.3670, -0.0286, -0.1008,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0088,  1.3933,  0.3039, -0.2871, -0.0118, -0.1026,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0398,  1.3032,  0.3178, -0.4877, -0.0660, -0.0824,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0632,  1.2130,  0.3640, -0.6049, -0.1133, -0.1026,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1154,  0.0106,  0.0911, -0.1193]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1193, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1153,  0.0106,  0.0912, -0.1192]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0912]])\n",
      " \n",
      "Q_target =  tensor([[-2.5472]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-2.4280]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.0923007 ,  1.0945377 ,  0.38382003, -0.7206756 , -0.18788922,\n",
      "       -0.25040144,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0886,  1.1107,  0.3731, -0.6931, -0.1754, -0.2068,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0562,  1.2402,  0.3527, -0.5780, -0.1023, -0.1467,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0209,  1.3615,  0.3142, -0.3942, -0.0358, -0.1437,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0528,  1.2532,  0.3362, -0.5966, -0.0950, -0.1566,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 1., 0., 2.]])\n",
      "Rewards:  tensor([[-2.6375, -1.1889, -1.8901,  1.0187]])\n",
      "Next states:  tensor([[[ 0.0923,  1.0945,  0.3838, -0.7207, -0.1879, -0.2504,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0596,  1.2266,  0.3452, -0.6047, -0.1081, -0.1167,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0240,  1.3520,  0.3142, -0.4209, -0.0429, -0.1436,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0562,  1.2402,  0.3527, -0.5780, -0.1023, -0.1467,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1151,  0.0108,  0.0915, -0.1225]], grad_fn=<AddmmBackward0>)\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q_current:  tensor(-0.1151, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1150,  0.0108,  0.0915, -0.1224]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0915]])\n",
      " \n",
      "Q_target =  tensor([[-1.8859]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-1.7708]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.0960144 ,  1.0777588 ,  0.38381645, -0.74735516, -0.20040914,\n",
      "       -0.25039858,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0398,  1.3032,  0.3178, -0.4877, -0.0660, -0.0824,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0335,  1.3245,  0.3178, -0.4344, -0.0578, -0.0825,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0209,  1.3615,  0.3142, -0.3942, -0.0358, -0.1437,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0886,  1.1107,  0.3731, -0.6931, -0.1754, -0.2068,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 0., 0., 3.]])\n",
      "Rewards:  tensor([[-2.5052, -1.5591, -1.8901, -2.6375]])\n",
      "Next states:  tensor([[[ 0.0430,  1.2916,  0.3298, -0.5154, -0.0726, -0.1306,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0366,  1.3141,  0.3178, -0.4611, -0.0619, -0.0824,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0240,  1.3520,  0.3142, -0.4209, -0.0429, -0.1436,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0923,  1.0945,  0.3838, -0.7207, -0.1879, -0.2504,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1175,  0.0107,  0.0913, -0.1225]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1225, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1175,  0.0108,  0.0913, -0.1225]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0913]])\n",
      " \n",
      "Q_target =  tensor([[-2.5051]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-2.3827]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.09979315,  1.0603628 ,  0.39194602, -0.77512807, -0.2146074 ,\n",
      "       -0.28396496,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0998,  1.0604,  0.3919, -0.7751, -0.2146, -0.2840,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0463,  1.2794,  0.3392, -0.5431, -0.0810, -0.1687,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0178,  1.3704,  0.3035, -0.3670, -0.0286, -0.1008,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0528,  1.2532,  0.3362, -0.5966, -0.0950, -0.1566,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[1., 1., 3., 2.]])\n",
      "Rewards:  tensor([[-1.4533, -1.0409, -2.6538,  1.0187]])\n",
      "Next states:  tensor([[[ 0.1035,  1.0424,  0.3834, -0.8009, -0.2271, -0.2490,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0495,  1.2666,  0.3279, -0.5692, -0.0871, -0.1231,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0209,  1.3615,  0.3142, -0.3942, -0.0358, -0.1437,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0562,  1.2402,  0.3527, -0.5780, -0.1023, -0.1467,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1174,  0.0109,  0.0915, -0.1257]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.0109, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1174,  0.0109,  0.0915, -0.1256]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0915]])\n",
      " \n",
      "Q_target =  tensor([[-1.3627]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-1.3736]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.10350408,  1.0423844 ,  0.38339418, -0.8008687 , -0.22705746,\n",
      "       -0.24900079,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0303,  1.3343,  0.3178, -0.4077, -0.0537, -0.0825,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0240,  1.3520,  0.3142, -0.4209, -0.0429, -0.1436,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0886,  1.1107,  0.3731, -0.6931, -0.1754, -0.2068,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0596,  1.2266,  0.3452, -0.6047, -0.1081, -0.1167,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 2., 3., 2.]])\n",
      "Rewards:  tensor([[-1.5717,  2.0011, -2.6375, -0.4397]])\n",
      "Next states:  tensor([[[ 0.0335,  1.3245,  0.3178, -0.4344, -0.0578, -0.0825,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0272,  1.3435,  0.3302, -0.3809, -0.0495, -0.1320,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0923,  1.0945,  0.3838, -0.7207, -0.1879, -0.2504,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0632,  1.2130,  0.3640, -0.6049, -0.1133, -0.1026,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1174,  0.0090,  0.0913, -0.1257]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.0913, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1174,  0.0090,  0.0914, -0.1257]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0914]])\n",
      " \n",
      "Q_target =  tensor([[1.1913]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[1.1000]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.10751667,  1.0249695 ,  0.41299215, -0.7758472 , -0.23895435,\n",
      "       -0.23793812,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0960,  1.0778,  0.3838, -0.7474, -0.2004, -0.2504,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0178,  1.3704,  0.3035, -0.3670, -0.0286, -0.1008,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0850,  1.1263,  0.3656, -0.6658, -0.1650, -0.1767,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0742,  1.1694,  0.3667, -0.6484, -0.1364, -0.1647,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 3., 3., 2.]])\n",
      "Rewards:  tensor([[-2.5956, -2.6538, -2.2943,  3.3249]])\n",
      "Next states:  tensor([[[ 0.0998,  1.0604,  0.3919, -0.7751, -0.2146, -0.2840,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0209,  1.3615,  0.3142, -0.3942, -0.0358, -0.1437,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0886,  1.1107,  0.3731, -0.6931, -0.1754, -0.2068,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0777,  1.1556,  0.3659, -0.6123, -0.1453, -0.1779,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1173,  0.0091,  0.0931, -0.1256]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1173, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1173,  0.0091,  0.0931, -0.1257]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0931]])\n",
      " \n",
      "Q_target =  tensor([[-1.7130]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-1.5957]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.11152963,  1.0069563 ,  0.412988  , -0.80252534, -0.25085118,\n",
      "       -0.23793657,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0398,  1.3032,  0.3178, -0.4877, -0.0660, -0.0824,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0148,  1.3786,  0.3035, -0.3403, -0.0235, -0.1008,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0632,  1.2130,  0.3640, -0.6049, -0.1133, -0.1026,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1115,  1.0070,  0.4130, -0.8025, -0.2509, -0.2379,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 0., 3., 3.]])\n",
      "Rewards:  tensor([[-2.5052, -1.7064, -2.3185, -2.6805]])\n",
      "Next states:  tensor([[[ 0.0430,  1.2916,  0.3298, -0.5154, -0.0726, -0.1306,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0178,  1.3704,  0.3035, -0.3670, -0.0286, -0.1008,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0669,  1.1988,  0.3756, -0.6323, -0.1207, -0.1491,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1156,  0.9883,  0.4243, -0.8308, -0.2651, -0.2847,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1196,  0.0090,  0.0928, -0.1257]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1257, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1195,  0.0091,  0.0929, -0.1257]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0929]])\n",
      " \n",
      "Q_target =  tensor([[-2.5885]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-2.4629]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.11563282,  0.9883179 ,  0.42426366, -0.8308205 , -0.26508772,\n",
      "       -0.2847306 ,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0029,  1.4056,  0.2952, -0.2345, -0.0034, -0.0669,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0148,  1.3786,  0.3035, -0.3403, -0.0235, -0.1008,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0742,  1.1694,  0.3667, -0.6484, -0.1364, -0.1647,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0562,  1.2402,  0.3527, -0.5780, -0.1023, -0.1467,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 2., 1.]])\n",
      "Rewards:  tensor([[-1.3631, -1.7064,  3.3249, -1.1889]])\n",
      "Next states:  tensor([[[ 0.0058,  1.3998,  0.2948, -0.2602, -0.0067, -0.0661,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0178,  1.3704,  0.3035, -0.3670, -0.0286, -0.1008,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0777,  1.1556,  0.3659, -0.6123, -0.1453, -0.1779,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0596,  1.2266,  0.3452, -0.6047, -0.1081, -0.1167,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1196,  0.0090,  0.0927, -0.1291]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1196, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1196,  0.0090,  0.0927, -0.1291]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0927]])\n",
      " \n",
      "Q_target =  tensor([[-1.8548]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-1.7352]], grad_fn=<SubBackward0>)\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current state:  (array([ 0.11973648,  0.9690821 ,  0.4242572 , -0.8575036 , -0.27932405,\n",
      "       -0.28472668,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0562,  1.2402,  0.3527, -0.5780, -0.1023, -0.1467,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0923,  1.0945,  0.3838, -0.7207, -0.1879, -0.2504,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0960,  1.0778,  0.3838, -0.7474, -0.2004, -0.2504,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0148,  1.3786,  0.3035, -0.3403, -0.0235, -0.1008,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[1., 0., 3., 0.]])\n",
      "Rewards:  tensor([[-1.1889, -1.9765, -2.5956, -1.7064]])\n",
      "Next states:  tensor([[[ 0.0596,  1.2266,  0.3452, -0.6047, -0.1081, -0.1167,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0960,  1.0778,  0.3838, -0.7474, -0.2004, -0.2504,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0998,  1.0604,  0.3919, -0.7751, -0.2146, -0.2840,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0178,  1.3704,  0.3035, -0.3670, -0.0286, -0.1008,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1217,  0.0092,  0.0931, -0.1289]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.0931, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1217,  0.0092,  0.0931, -0.1289]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0931]])\n",
      " \n",
      "Q_target =  tensor([[-1.2182]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-1.3113]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.12412234,  0.9497738 ,  0.45163482, -0.8606945 , -0.29275393,\n",
      "       -0.2685974 ,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0058,  1.3998,  0.2948, -0.2602, -0.0067, -0.0661,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0366,  1.3141,  0.3178, -0.4611, -0.0619, -0.0824,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1115,  1.0070,  0.4130, -0.8025, -0.2509, -0.2379,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1035,  1.0424,  0.3834, -0.8009, -0.2271, -0.2490,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 0., 3., 2.]])\n",
      "Rewards:  tensor([[-2.3868, -1.5409, -2.6805,  1.1009]])\n",
      "Next states:  tensor([[[ 0.0088,  1.3933,  0.3039, -0.2871, -0.0118, -0.1026,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0398,  1.3032,  0.3178, -0.4877, -0.0660, -0.0824,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1156,  0.9883,  0.4243, -0.8308, -0.2651, -0.2847,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1075,  1.0250,  0.4130, -0.7758, -0.2390, -0.2379,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1218,  0.0090,  0.0912, -0.1290]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1290, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1217,  0.0091,  0.0912, -0.1290]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0912]])\n",
      " \n",
      "Q_target =  tensor([[-2.6049]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-2.4759]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.12858896,  0.9298305 ,  0.46177393, -0.88948804, -0.30835935,\n",
      "       -0.31210828,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.1156,  0.9883,  0.4243, -0.8308, -0.2651, -0.2847,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0088,  1.3933,  0.3039, -0.2871, -0.0118, -0.1026,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1286,  0.9298,  0.4618, -0.8895, -0.3084, -0.3121,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1241,  0.9498,  0.4516, -0.8607, -0.2928, -0.2686,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 3., 1., 3.]])\n",
      "Rewards:  tensor([[-1.9466, -2.4486, -1.2496, -2.6953]])\n",
      "Next states:  tensor([[[ 0.1197,  0.9691,  0.4243, -0.8575, -0.2793, -0.2847,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0119,  1.3863,  0.3117, -0.3142, -0.0185, -0.1339,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1330,  0.9093,  0.4525, -0.9146, -0.3220, -0.2733,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1286,  0.9298,  0.4618, -0.8895, -0.3084, -0.3121,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1215,  0.0092,  0.0916, -0.1324]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.0092, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1215,  0.0093,  0.0916, -0.1324]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0916]])\n",
      " \n",
      "Q_target =  tensor([[-1.1589]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-1.1681]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.13298264,  0.90931606,  0.45252505, -0.9146079 , -0.322022  ,\n",
      "       -0.27325267,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0178,  1.3704,  0.3035, -0.3670, -0.0286, -0.1008,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0430,  1.2916,  0.3298, -0.5154, -0.0726, -0.1306,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0998,  1.0604,  0.3919, -0.7751, -0.2146, -0.2840,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0463,  1.2794,  0.3392, -0.5431, -0.0810, -0.1687,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 3., 1., 1.]])\n",
      "Rewards:  tensor([[-2.6538, -2.5061, -1.4533, -1.0409]])\n",
      "Next states:  tensor([[[ 0.0209,  1.3615,  0.3142, -0.3942, -0.0358, -0.1437,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0463,  1.2794,  0.3392, -0.5431, -0.0810, -0.1687,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1035,  1.0424,  0.3834, -0.8009, -0.2271, -0.2490,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0495,  1.2666,  0.3279, -0.5692, -0.0871, -0.1231,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1218,  0.0075,  0.0910, -0.1323]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1218, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1217,  0.0075,  0.0911, -0.1323]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0911]])\n",
      " \n",
      "Q_target =  tensor([[-1.6515]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-1.5298]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.13737687,  0.88820404,  0.4525178 , -0.94128937, -0.33568445,\n",
      "       -0.2732492 ,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0528,  1.2532,  0.3362, -0.5966, -0.0950, -0.1566,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0495,  1.2666,  0.3279, -0.5692, -0.0871, -0.1231,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0960,  1.0778,  0.3838, -0.7474, -0.2004, -0.2504,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0669,  1.1988,  0.3756, -0.6323, -0.1207, -0.1491,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[2., 3., 3., 0.]])\n",
      "Rewards:  tensor([[ 1.0187, -2.2790, -2.5956, -1.5943]])\n",
      "Next states:  tensor([[[ 0.0562,  1.2402,  0.3527, -0.5780, -0.1023, -0.1467,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0528,  1.2532,  0.3362, -0.5966, -0.0950, -0.1566,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0998,  1.0604,  0.3919, -0.7751, -0.2146, -0.2840,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0706,  1.1839,  0.3756, -0.6590, -0.1282, -0.1491,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1237,  0.0076,  0.0912, -0.1322]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.0076, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1237,  0.0076,  0.0912, -0.1322]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0912]])\n",
      " \n",
      "Q_target =  tensor([[-0.6493]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.6569]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.1416812 ,  0.8665465 ,  0.4410684 , -0.965069  , -0.34682193,\n",
      "       -0.22274971,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.1115,  1.0070,  0.4130, -0.8025, -0.2509, -0.2379,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0777,  1.1556,  0.3659, -0.6123, -0.1453, -0.1779,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0669,  1.1988,  0.3756, -0.6323, -0.1207, -0.1491,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1417,  0.8665,  0.4411, -0.9651, -0.3468, -0.2227,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 3., 0., 2.]])\n",
      "Rewards:  tensor([[-2.6805, -2.5704, -1.5943,  2.6854]])\n",
      "Next states:  tensor([[[ 0.1156,  0.9883,  0.4243, -0.8308, -0.2651, -0.2847,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0814,  1.1412,  0.3757, -0.6397, -0.1562, -0.2175,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0706,  1.1839,  0.3756, -0.6590, -0.1282, -0.1491,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1462,  0.8456,  0.4588, -0.9326, -0.3586, -0.2358,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1236,  0.0068,  0.0915, -0.1322]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.0915, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1235,  0.0069,  0.0915, -0.1322]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0915]])\n",
      " \n",
      "Q_target =  tensor([[2.7760]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[2.6846]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.14615726,  0.8456253 ,  0.45882306, -0.9325697 , -0.35861248,\n",
      "       -0.23581083,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0303,  1.3343,  0.3178, -0.4077, -0.0537, -0.0825,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0777,  1.1556,  0.3659, -0.6123, -0.1453, -0.1779,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0178,  1.3704,  0.3035, -0.3670, -0.0286, -0.1008,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1241,  0.9498,  0.4516, -0.8607, -0.2928, -0.2686,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 3., 3., 3.]])\n",
      "Rewards:  tensor([[-1.5717, -2.5704, -2.6538, -2.6953]])\n",
      "Next states:  tensor([[[ 0.0335,  1.3245,  0.3178, -0.4344, -0.0578, -0.0825,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0814,  1.1412,  0.3757, -0.6397, -0.1562, -0.2175,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0209,  1.3615,  0.3142, -0.3942, -0.0358, -0.1437,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1286,  0.9298,  0.4618, -0.8895, -0.3084, -0.3121,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1238,  0.0067,  0.0947, -0.1323]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.0947, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1238,  0.0068,  0.0948, -0.1323]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0948]])\n",
      " \n",
      "Q_target =  tensor([[1.1918]])\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error(Q_target - Q_current):  tensor([[1.0971]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.15109444,  0.8253958 ,  0.50430244, -0.90177727, -0.3698037 ,\n",
      "       -0.22382364,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0706,  1.1839,  0.3756, -0.6590, -0.1282, -0.1491,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0148,  1.3786,  0.3035, -0.3403, -0.0235, -0.1008,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0742,  1.1694,  0.3667, -0.6484, -0.1364, -0.1647,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0463,  1.2794,  0.3392, -0.5431, -0.0810, -0.1687,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[2., 0., 2., 1.]])\n",
      "Rewards:  tensor([[ 1.6666, -1.7064,  3.3249, -1.0409]])\n",
      "Next states:  tensor([[[ 0.0742,  1.1694,  0.3667, -0.6484, -0.1364, -0.1647,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0178,  1.3704,  0.3035, -0.3670, -0.0286, -0.1008,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0777,  1.1556,  0.3659, -0.6123, -0.1453, -0.1779,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0495,  1.2666,  0.3279, -0.5692, -0.0871, -0.1231,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1238,  0.0068,  0.0961, -0.1322]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.0961, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1238,  0.0068,  0.0961, -0.1322]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0961]])\n",
      " \n",
      "Q_target =  tensor([[-0.8189]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.9150]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.15635118,  0.8051934 ,  0.5355822 , -0.90048015, -0.38031483,\n",
      "       -0.2102221 ,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.1286,  0.9298,  0.4618, -0.8895, -0.3084, -0.3121,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1417,  0.8665,  0.4411, -0.9651, -0.3468, -0.2227,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1156,  0.9883,  0.4243, -0.8308, -0.2651, -0.2847,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0058,  1.3998,  0.2948, -0.2602, -0.0067, -0.0661,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[1., 2., 0., 3.]])\n",
      "Rewards:  tensor([[-1.2496,  2.6854, -1.9466, -2.3868]])\n",
      "Next states:  tensor([[[ 0.1330,  0.9093,  0.4525, -0.9146, -0.3220, -0.2733,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1462,  0.8456,  0.4588, -0.9326, -0.3586, -0.2358,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1197,  0.9691,  0.4243, -0.8575, -0.2793, -0.2847,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0088,  1.3933,  0.3039, -0.2871, -0.0118, -0.1026,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1235,  0.0070,  0.0956, -0.1322]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1235, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1235,  0.0070,  0.0956, -0.1322]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0956]])\n",
      " \n",
      "Q_target =  tensor([[-1.3210]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-1.1975]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.16160822,  0.78439236,  0.535577  , -0.9271554 , -0.39082584,\n",
      "       -0.21022049,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0669,  1.1988,  0.3756, -0.6323, -0.1207, -0.1491,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1330,  0.9093,  0.4525, -0.9146, -0.3220, -0.2733,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1511,  0.8254,  0.5043, -0.9018, -0.3698, -0.2238,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1374,  0.8882,  0.4525, -0.9413, -0.3357, -0.2732,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 2., 1.]])\n",
      "Rewards:  tensor([[-1.5943, -1.7417, -0.9141, -0.7396]])\n",
      "Next states:  tensor([[[ 0.0706,  1.1839,  0.3756, -0.6590, -0.1282, -0.1491,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1374,  0.8882,  0.4525, -0.9413, -0.3357, -0.2732,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1564,  0.8052,  0.5356, -0.9005, -0.3803, -0.2102,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1417,  0.8665,  0.4411, -0.9651, -0.3468, -0.2227,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1249,  0.0071,  0.0957, -0.1322]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.0071, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1249,  0.0071,  0.0958, -0.1322]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0958]])\n",
      " \n",
      "Q_target =  tensor([[-0.2657]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.2727]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.16678134,  0.7630529 ,  0.5248363 , -0.9505165 , -0.39887178,\n",
      "       -0.16091801,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0742,  1.1694,  0.3667, -0.6484, -0.1364, -0.1647,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1241,  0.9498,  0.4516, -0.8607, -0.2928, -0.2686,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0119,  1.3863,  0.3117, -0.3142, -0.0185, -0.1339,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0303,  1.3343,  0.3178, -0.4077, -0.0537, -0.0825,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[2., 3., 1., 0.]])\n",
      "Rewards:  tensor([[ 3.3249, -2.6953, -1.1112, -1.5717]])\n",
      "Next states:  tensor([[[ 0.0777,  1.1556,  0.3659, -0.6123, -0.1453, -0.1779,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1286,  0.9298,  0.4618, -0.8895, -0.3084, -0.3121,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0148,  1.3786,  0.3035, -0.3403, -0.0235, -0.1008,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0335,  1.3245,  0.3178, -0.4344, -0.0578, -0.0825,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1254,  0.0064,  0.0950, -0.1322]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.0064, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1254,  0.0064,  0.0951, -0.1322]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0951]])\n",
      " \n",
      "Q_target =  tensor([[-0.2852]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.2916]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.17188902,  0.7411532 ,  0.5165369 , -0.9749565 , -0.40505922,\n",
      "       -0.12374882,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0178,  1.3704,  0.3035, -0.3670, -0.0286, -0.1008,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0335,  1.3245,  0.3178, -0.4344, -0.0578, -0.0825,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0528,  1.2532,  0.3362, -0.5966, -0.0950, -0.1566,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0923,  1.0945,  0.3838, -0.7207, -0.1879, -0.2504,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 0., 2., 0.]])\n",
      "Rewards:  tensor([[-2.6538, -1.5591,  1.0187, -1.9765]])\n",
      "Next states:  tensor([[[ 0.0209,  1.3615,  0.3142, -0.3942, -0.0358, -0.1437,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0366,  1.3141,  0.3178, -0.4611, -0.0619, -0.0824,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0562,  1.2402,  0.3527, -0.5780, -0.1023, -0.1467,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0960,  1.0778,  0.3838, -0.7474, -0.2004, -0.2504,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1255,  0.0059,  0.0948, -0.1322]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1255, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1255,  0.0059,  0.0948, -0.1322]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0948]])\n",
      " \n",
      "Q_target =  tensor([[-0.8188]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.6933]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.1769969 ,  0.7186539 ,  0.516535  , -1.0016261 , -0.41124666,\n",
      "       -0.12374848,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.1417,  0.8665,  0.4411, -0.9651, -0.3468, -0.2227,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1564,  0.8052,  0.5356, -0.9005, -0.3803, -0.2102,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0850,  1.1263,  0.3656, -0.6658, -0.1650, -0.1767,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0528,  1.2532,  0.3362, -0.5966, -0.0950, -0.1566,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[2., 0., 3., 2.]])\n",
      "Rewards:  tensor([[ 2.6854, -1.4156, -2.2943,  1.0187]])\n",
      "Next states:  tensor([[[ 0.1462,  0.8456,  0.4588, -0.9326, -0.3586, -0.2358,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1616,  0.7844,  0.5356, -0.9272, -0.3908, -0.2102,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0886,  1.1107,  0.3731, -0.6931, -0.1754, -0.2068,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0562,  1.2402,  0.3527, -0.5780, -0.1023, -0.1467,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1260,  0.0062,  0.0955, -0.1321]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.0955, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1260,  0.0063,  0.0956, -0.1321]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0956]])\n",
      " \n",
      "Q_target =  tensor([[1.0902]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.9947]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.18244514,  0.69653165,  0.550338  , -0.9848291 , -0.41719472,\n",
      "       -0.11896197,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0923,  1.0945,  0.3838, -0.7207, -0.1879, -0.2504,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1770,  0.7187,  0.5165, -1.0016, -0.4112, -0.1237,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1374,  0.8882,  0.4525, -0.9413, -0.3357, -0.2732,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0148,  1.3786,  0.3035, -0.3403, -0.0235, -0.1008,  0.0000,\n",
      "           0.0000]]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions:  tensor([[0., 2., 1., 0.]])\n",
      "Rewards:  tensor([[-1.9765,  0.9956, -0.7396, -1.7064]])\n",
      "Next states:  tensor([[[ 0.0960,  1.0778,  0.3838, -0.7474, -0.2004, -0.2504,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1824,  0.6965,  0.5503, -0.9848, -0.4172, -0.1190,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1417,  0.8665,  0.4411, -0.9651, -0.3468, -0.2227,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0178,  1.3704,  0.3035, -0.3670, -0.0286, -0.1008,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1260,  0.0063,  0.0969, -0.1321]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.0063, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1260,  0.0063,  0.0970, -0.1321]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0970]])\n",
      " \n",
      "Q_target =  tensor([[-0.0130]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.0193]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.18782893,  0.6738552 ,  0.54213804, -1.0089566 , -0.4212526 ,\n",
      "       -0.08115761,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0058,  1.3998,  0.2948, -0.2602, -0.0067, -0.0661,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0777,  1.1556,  0.3659, -0.6123, -0.1453, -0.1779,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1462,  0.8456,  0.4588, -0.9326, -0.3586, -0.2358,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1878,  0.6739,  0.5421, -1.0090, -0.4213, -0.0812,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 3., 2., 3.]])\n",
      "Rewards:  tensor([[-2.3868, -2.5704,  1.0980, -1.6179]])\n",
      "Next states:  tensor([[[ 0.0088,  1.3933,  0.3039, -0.2871, -0.0118, -0.1026,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0814,  1.1412,  0.3757, -0.6397, -0.1562, -0.2175,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1511,  0.8254,  0.5043, -0.9018, -0.3698, -0.2238,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1933,  0.6505,  0.5517, -1.0385, -0.4275, -0.1249,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1261,  0.0062,  0.0967, -0.1323]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1323, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1260,  0.0062,  0.0968, -0.1323]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0968]])\n",
      " \n",
      "Q_target =  tensor([[-1.5221]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-1.3898]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.19328842,  0.65052897,  0.5517177 , -1.0384619 , -0.4274997 ,\n",
      "       -0.12494195,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0029,  1.4056,  0.2952, -0.2345, -0.0034, -0.0669,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1417,  0.8665,  0.4411, -0.9651, -0.3468, -0.2227,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0632,  1.2130,  0.3640, -0.6049, -0.1133, -0.1026,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0178,  1.3704,  0.3035, -0.3670, -0.0286, -0.1008,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 2., 3., 3.]])\n",
      "Rewards:  tensor([[-1.3631,  2.6854, -2.3185, -2.6538]])\n",
      "Next states:  tensor([[[ 0.0058,  1.3998,  0.2948, -0.2602, -0.0067, -0.0661,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1462,  0.8456,  0.4588, -0.9326, -0.3586, -0.2358,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0669,  1.1988,  0.3756, -0.6323, -0.1207, -0.1491,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0209,  1.3615,  0.3142, -0.3942, -0.0358, -0.1437,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1263,  0.0060,  0.0963, -0.1341]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.0963, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1263,  0.0060,  0.0964, -0.1341]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0964]])\n",
      " \n",
      "Q_target =  tensor([[1.8689]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[1.7726]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.19914827,  0.62788624,  0.5917014 , -1.0080895 , -0.4337019 ,\n",
      "       -0.12404378,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.1241,  0.9498,  0.4516, -0.8607, -0.2928, -0.2686,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0029,  1.4056,  0.2952, -0.2345, -0.0034, -0.0669,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1933,  0.6505,  0.5517, -1.0385, -0.4275, -0.1249,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0088,  1.3933,  0.3039, -0.2871, -0.0118, -0.1026,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 0., 2., 3.]])\n",
      "Rewards:  tensor([[-2.6953, -1.3631,  1.7735, -2.4486]])\n",
      "Next states:  tensor([[[ 0.1286,  0.9298,  0.4618, -0.8895, -0.3084, -0.3121,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0058,  1.3998,  0.2948, -0.2602, -0.0067, -0.0661,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1991,  0.6279,  0.5917, -1.0081, -0.4337, -0.1240,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0119,  1.3863,  0.3117, -0.3142, -0.0185, -0.1339,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1261,  0.0062,  0.0990, -0.1341]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1261, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1261,  0.0062,  0.0991, -0.1341]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0991]])\n",
      " \n",
      "Q_target =  tensor([[-0.8039]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.6777]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.20500831,  0.60464394,  0.5916994 , -1.034759  , -0.43990406,\n",
      "       -0.12404344,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.1933,  0.6505,  0.5517, -1.0385, -0.4275, -0.1249,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0814,  1.1412,  0.3757, -0.6397, -0.1562, -0.2175,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0777,  1.1556,  0.3659, -0.6123, -0.1453, -0.1779,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1824,  0.6965,  0.5503, -0.9848, -0.4172, -0.1190,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[2., 1., 3., 1.]])\n",
      "Rewards:  tensor([[ 1.7735, -1.2197, -2.5704, -0.1090]])\n",
      "Next states:  tensor([[[ 0.1991,  0.6279,  0.5917, -1.0081, -0.4337, -0.1240,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0850,  1.1263,  0.3656, -0.6658, -0.1650, -0.1767,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0814,  1.1412,  0.3757, -0.6397, -0.1562, -0.2175,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1878,  0.6739,  0.5421, -1.0090, -0.4213, -0.0812,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1268,  0.0064,  0.0994, -0.1340]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.0994, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1268,  0.0064,  0.0994, -0.1340]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0994]])\n",
      " \n",
      "Q_target =  tensor([[0.0650]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.0344]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.2111741 ,  0.58150536,  0.6219319 , -1.0300735 , -0.4457503 ,\n",
      "       -0.11692496,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.1564,  0.8052,  0.5356, -0.9005, -0.3803, -0.2102,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0886,  1.1107,  0.3731, -0.6931, -0.1754, -0.2068,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0742,  1.1694,  0.3667, -0.6484, -0.1364, -0.1647,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0148,  1.3786,  0.3035, -0.3403, -0.0235, -0.1008,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 3., 2., 0.]])\n",
      "Rewards:  tensor([[-1.4156, -2.6375,  3.3249, -1.7064]])\n",
      "Next states:  tensor([[[ 0.1616,  0.7844,  0.5356, -0.9272, -0.3908, -0.2102,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0923,  1.0945,  0.3838, -0.7207, -0.1879, -0.2504,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0777,  1.1556,  0.3659, -0.6123, -0.1453, -0.1779,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0178,  1.3704,  0.3035, -0.3670, -0.0286, -0.1008,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1270,  0.0062,  0.0990, -0.1340]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1270, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1270,  0.0062,  0.0990, -0.1340]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0990]])\n",
      " \n",
      "Q_target =  tensor([[-0.7727]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.6456]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.21734008,  0.55776715,  0.62193   , -1.0567428 , -0.45159653,\n",
      "       -0.11692466,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0886,  1.1107,  0.3731, -0.6931, -0.1754, -0.2068,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0706,  1.1839,  0.3756, -0.6590, -0.1282, -0.1491,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0923,  1.0945,  0.3838, -0.7207, -0.1879, -0.2504,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0303,  1.3343,  0.3178, -0.4077, -0.0537, -0.0825,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 2., 0., 0.]])\n",
      "Rewards:  tensor([[-2.6375,  1.6666, -1.9765, -1.5717]])\n",
      "Next states:  tensor([[[ 0.0923,  1.0945,  0.3838, -0.7207, -0.1879, -0.2504,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0742,  1.1694,  0.3667, -0.6484, -0.1364, -0.1647,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0960,  1.0778,  0.3838, -0.7474, -0.2004, -0.2504,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0335,  1.3245,  0.3178, -0.4344, -0.0578, -0.0825,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1280,  0.0062,  0.0988, -0.1340]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.0062, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1279,  0.0062,  0.0989, -0.1340]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0989]])\n",
      " \n",
      "Q_target =  tensor([[-0.0939]])\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error(Q_target - Q_current):  tensor([[-0.1000]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.22344723,  0.53346264,  0.6145279 , -1.0814421 , -0.45578483,\n",
      "       -0.08376565,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0706,  1.1839,  0.3756, -0.6590, -0.1282, -0.1491,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0923,  1.0945,  0.3838, -0.7207, -0.1879, -0.2504,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1197,  0.9691,  0.4243, -0.8575, -0.2793, -0.2847,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1075,  1.0250,  0.4130, -0.7758, -0.2390, -0.2379,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[2., 0., 2., 0.]])\n",
      "Rewards:  tensor([[ 1.6666, -1.9765, -1.3104, -1.8052]])\n",
      "Next states:  tensor([[[ 0.0742,  1.1694,  0.3667, -0.6484, -0.1364, -0.1647,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0960,  1.0778,  0.3838, -0.7474, -0.2004, -0.2504,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1241,  0.9498,  0.4516, -0.8607, -0.2928, -0.2686,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1115,  1.0070,  0.4130, -0.8025, -0.2509, -0.2379,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1278,  0.0062,  0.0991, -0.1340]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.0062, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1278,  0.0062,  0.0992, -0.1340]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0992]])\n",
      " \n",
      "Q_target =  tensor([[0.4544]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.4483]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.22947188,  0.50862646,  0.60397756, -1.1043264 , -0.4574467 ,\n",
      "       -0.03323717,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0632,  1.2130,  0.3640, -0.6049, -0.1133, -0.1026,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0088,  1.3933,  0.3039, -0.2871, -0.0118, -0.1026,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1991,  0.6279,  0.5917, -1.0081, -0.4337, -0.1240,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1075,  1.0250,  0.4130, -0.7758, -0.2390, -0.2379,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 3., 0., 0.]])\n",
      "Rewards:  tensor([[-2.3185, -2.4486, -0.9020, -1.8052]])\n",
      "Next states:  tensor([[[ 0.0669,  1.1988,  0.3756, -0.6323, -0.1207, -0.1491,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0119,  1.3863,  0.3117, -0.3142, -0.0185, -0.1339,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2050,  0.6046,  0.5917, -1.0348, -0.4399, -0.1240,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1115,  1.0070,  0.4130, -0.8025, -0.2509, -0.2379,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1278,  0.0067,  0.0990, -0.1341]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.0990, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1278,  0.0067,  0.0991, -0.1341]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0991]])\n",
      " \n",
      "Q_target =  tensor([[2.2421]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[2.1431]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.23583241,  0.48441026,  0.6378006 , -1.076851  , -0.45937604,\n",
      "       -0.03858761,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0632,  1.2130,  0.3640, -0.6049, -0.1133, -0.1026,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0303,  1.3343,  0.3178, -0.4077, -0.0537, -0.0825,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1035,  1.0424,  0.3834, -0.8009, -0.2271, -0.2490,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0742,  1.1694,  0.3667, -0.6484, -0.1364, -0.1647,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 0., 2., 2.]])\n",
      "Rewards:  tensor([[-2.3185, -1.5717,  1.1009,  3.3249]])\n",
      "Next states:  tensor([[[ 0.0669,  1.1988,  0.3756, -0.6323, -0.1207, -0.1491,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0335,  1.3245,  0.3178, -0.4344, -0.0578, -0.0825,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1075,  1.0250,  0.4130, -0.7758, -0.2390, -0.2379,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0777,  1.1556,  0.3659, -0.6123, -0.1453, -0.1779,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1280,  0.0066,  0.1016, -0.1340]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1280, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1280,  0.0067,  0.1017, -0.1340]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1017]])\n",
      " \n",
      "Q_target =  tensor([[-0.4676]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.3396]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.24219283,  0.4595941 ,  0.6378004 , -1.1035179 , -0.46130544,\n",
      "       -0.03858755,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.2173,  0.5578,  0.6219, -1.0567, -0.4516, -0.1169,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1075,  1.0250,  0.4130, -0.7758, -0.2390, -0.2379,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0335,  1.3245,  0.3178, -0.4344, -0.0578, -0.0825,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1564,  0.8052,  0.5356, -0.9005, -0.3803, -0.2102,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[1., 0., 0., 0.]])\n",
      "Rewards:  tensor([[-0.1918, -1.8052, -1.5591, -1.4156]])\n",
      "Next states:  tensor([[[ 0.2234,  0.5335,  0.6145, -1.0814, -0.4558, -0.0838,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1115,  1.0070,  0.4130, -0.8025, -0.2509, -0.2379,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0366,  1.3141,  0.3178, -0.4611, -0.0619, -0.0824,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1616,  0.7844,  0.5356, -0.9272, -0.3908, -0.2102,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1281,  0.0069,  0.1022, -0.1340]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1281, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1281,  0.0069,  0.1023, -0.1340]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1023]])\n",
      " \n",
      "Q_target =  tensor([[-0.4859]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.3578]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.24855328,  0.43417805,  0.63780016, -1.1301848 , -0.46323475,\n",
      "       -0.038587  ,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0495,  1.2666,  0.3279, -0.5692, -0.0871, -0.1231,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0272,  1.3435,  0.3302, -0.3809, -0.0495, -0.1320,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1330,  0.9093,  0.4525, -0.9146, -0.3220, -0.2733,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1286,  0.9298,  0.4618, -0.8895, -0.3084, -0.3121,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 1., 0., 1.]])\n",
      "Rewards:  tensor([[-2.2790, -0.8179, -1.7417, -1.2496]])\n",
      "Next states:  tensor([[[ 0.0528,  1.2532,  0.3362, -0.5966, -0.0950, -0.1566,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0303,  1.3343,  0.3178, -0.4077, -0.0537, -0.0825,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1374,  0.8882,  0.4525, -0.9413, -0.3357, -0.2732,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1330,  0.9093,  0.4525, -0.9146, -0.3220, -0.2733,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1288,  0.0067,  0.1019, -0.1341]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.1019, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1288,  0.0067,  0.1019, -0.1340]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1019]])\n",
      " \n",
      "Q_target =  tensor([[1.9338]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[1.8319]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.25532132,  0.40945116,  0.678628  , -1.0995843 , -0.46525377,\n",
      "       -0.04038067,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0058,  1.3998,  0.2948, -0.2602, -0.0067, -0.0661,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1564,  0.8052,  0.5356, -0.9005, -0.3803, -0.2102,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1115,  1.0070,  0.4130, -0.8025, -0.2509, -0.2379,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1241,  0.9498,  0.4516, -0.8607, -0.2928, -0.2686,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 0., 3., 3.]])\n",
      "Rewards:  tensor([[-2.3868, -1.4156, -2.6805, -2.6953]])\n",
      "Next states:  tensor([[[ 0.0088,  1.3933,  0.3039, -0.2871, -0.0118, -0.1026,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1616,  0.7844,  0.5356, -0.9272, -0.3908, -0.2102,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1156,  0.9883,  0.4243, -0.8308, -0.2651, -0.2847,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1286,  0.9298,  0.4618, -0.8895, -0.3084, -0.3121,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1287,  0.0069,  0.1046, -0.1341]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1287, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1287,  0.0069,  0.1047, -0.1341]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1047]])\n",
      " \n",
      "Q_target =  tensor([[-0.6235]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.4948]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.26208925,  0.3841243 ,  0.6786278 , -1.1262513 , -0.46727276,\n",
      "       -0.04038028,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.1719,  0.7412,  0.5165, -0.9750, -0.4051, -0.1237,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1991,  0.6279,  0.5917, -1.0081, -0.4337, -0.1240,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0960,  1.0778,  0.3838, -0.7474, -0.2004, -0.2504,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2234,  0.5335,  0.6145, -1.0814, -0.4558, -0.0838,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 3., 1.]])\n",
      "Rewards:  tensor([[-0.9127, -0.9020, -2.5956,  0.3562]])\n",
      "Next states:  tensor([[[ 0.1770,  0.7187,  0.5165, -1.0016, -0.4112, -0.1237,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2050,  0.6046,  0.5917, -1.0348, -0.4399, -0.1240,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0998,  1.0604,  0.3919, -0.7751, -0.2146, -0.2840,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2295,  0.5086,  0.6040, -1.1043, -0.4574, -0.0332,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1290,  0.0071,  0.1052, -0.1339]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.0071, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1289,  0.0072,  0.1052, -0.1339]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1052]])\n",
      " \n",
      "Q_target =  tensor([[0.2205]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.2134]], grad_fn=<SubBackward0>)\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current state:  (array([ 0.26878768,  0.35825223,  0.66975176, -1.1498425 , -0.46718132,\n",
      "        0.00182859,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0430,  1.2916,  0.3298, -0.5154, -0.0726, -0.1306,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1878,  0.6739,  0.5421, -1.0090, -0.4213, -0.0812,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0850,  1.1263,  0.3656, -0.6658, -0.1650, -0.1767,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2688,  0.3583,  0.6698, -1.1498, -0.4672,  0.0018,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 3., 3., 0.]])\n",
      "Rewards:  tensor([[-2.5061, -1.6179, -2.2943, -0.6386]])\n",
      "Next states:  tensor([[[ 0.0463,  1.2794,  0.3392, -0.5431, -0.0810, -0.1687,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1933,  0.6505,  0.5517, -1.0385, -0.4275, -0.1249,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0886,  1.1107,  0.3731, -0.6931, -0.1754, -0.2068,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2755,  0.3318,  0.6698, -1.1765, -0.4671,  0.0018,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1292,  0.0073,  0.1048, -0.1340]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1292, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1291,  0.0073,  0.1049, -0.1340]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1049]])\n",
      " \n",
      "Q_target =  tensor([[-0.5348]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.4056]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.275486  ,  0.3317801 ,  0.66975176, -1.1765091 , -0.4670899 ,\n",
      "        0.00182874,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.2112,  0.5815,  0.6219, -1.0301, -0.4458, -0.1169,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0240,  1.3520,  0.3142, -0.4209, -0.0429, -0.1436,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2234,  0.5335,  0.6145, -1.0814, -0.4558, -0.0838,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0814,  1.1412,  0.3757, -0.6397, -0.1562, -0.2175,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 2., 1., 1.]])\n",
      "Rewards:  tensor([[-0.8707,  2.0011,  0.3562, -1.2197]])\n",
      "Next states:  tensor([[[ 0.2173,  0.5578,  0.6219, -1.0567, -0.4516, -0.1169,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0272,  1.3435,  0.3302, -0.3809, -0.0495, -0.1320,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2295,  0.5086,  0.6040, -1.1043, -0.4574, -0.0332,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0850,  1.1263,  0.3656, -0.6658, -0.1650, -0.1767,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1297,  0.0073,  0.1048, -0.1340]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.1048, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1297,  0.0073,  0.1048, -0.1340]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1048]])\n",
      " \n",
      "Q_target =  tensor([[0.7199]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.6151]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 2.8242883e-01,  3.0546993e-01,  6.9423389e-01, -1.1693289e+00,\n",
      "       -4.6704674e-01,  8.6305692e-04,  0.0000000e+00,  0.0000000e+00],\n",
      "      dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.1564,  0.8052,  0.5356, -0.9005, -0.3803, -0.2102,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0272,  1.3435,  0.3302, -0.3809, -0.0495, -0.1320,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1668,  0.7631,  0.5248, -0.9505, -0.3989, -0.1609,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0209,  1.3615,  0.3142, -0.3942, -0.0358, -0.1437,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 1., 1., 0.]])\n",
      "Rewards:  tensor([[-1.4156, -0.8179, -0.3793, -1.8901]])\n",
      "Next states:  tensor([[[ 0.1616,  0.7844,  0.5356, -0.9272, -0.3908, -0.2102,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0303,  1.3343,  0.3178, -0.4077, -0.0537, -0.0825,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1719,  0.7412,  0.5165, -0.9750, -0.4051, -0.1237,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0240,  1.3520,  0.3142, -0.4209, -0.0429, -0.1436,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1300,  0.0071,  0.1053, -0.1340]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.0071, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1300,  0.0071,  0.1053, -0.1340]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1053]])\n",
      " \n",
      "Q_target =  tensor([[0.1732]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.1660]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.2892972 ,  0.27861336,  0.684806  , -1.1929444 , -0.46479884,\n",
      "        0.04495809,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.2173,  0.5578,  0.6219, -1.0567, -0.4516, -0.1169,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0088,  1.3933,  0.3039, -0.2871, -0.0118, -0.1026,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1878,  0.6739,  0.5421, -1.0090, -0.4213, -0.0812,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0240,  1.3520,  0.3142, -0.4209, -0.0429, -0.1436,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[1., 3., 3., 2.]])\n",
      "Rewards:  tensor([[-0.1918, -2.4486, -1.6179,  2.0011]])\n",
      "Next states:  tensor([[[ 0.2234,  0.5335,  0.6145, -1.0814, -0.4558, -0.0838,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0119,  1.3863,  0.3117, -0.3142, -0.0185, -0.1339,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1933,  0.6505,  0.5517, -1.0385, -0.4275, -0.1249,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0272,  1.3435,  0.3302, -0.3809, -0.0495, -0.1320,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1299,  0.0074,  0.1054, -0.1340]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.1054, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1298,  0.0074,  0.1055, -0.1340]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1055]])\n",
      " \n",
      "Q_target =  tensor([[0.7071]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.6017]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.2966444 ,  0.25223652,  0.73223644, -1.1714762 , -0.46206346,\n",
      "        0.05470782,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.1417,  0.8665,  0.4411, -0.9651, -0.3468, -0.2227,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0706,  1.1839,  0.3756, -0.6590, -0.1282, -0.1491,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0960,  1.0778,  0.3838, -0.7474, -0.2004, -0.2504,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2553,  0.4095,  0.6786, -1.0996, -0.4653, -0.0404,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[2., 2., 3., 0.]])\n",
      "Rewards:  tensor([[ 2.6854,  1.6666, -2.5956, -0.7271]])\n",
      "Next states:  tensor([[[ 0.1462,  0.8456,  0.4588, -0.9326, -0.3586, -0.2358,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0742,  1.1694,  0.3667, -0.6484, -0.1364, -0.1647,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0998,  1.0604,  0.3919, -0.7751, -0.2146, -0.2840,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2621,  0.3841,  0.6786, -1.1263, -0.4673, -0.0404,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1297,  0.0076,  0.1065, -0.1339]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.1065, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1297,  0.0076,  0.1066, -0.1339]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1066]])\n",
      " \n",
      "Q_target =  tensor([[2.1497]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[2.0432]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.30431145,  0.22664216,  0.7647537 , -1.1368796 , -0.45990297,\n",
      "        0.0432092 ,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0058,  1.3998,  0.2948, -0.2602, -0.0067, -0.0661,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1878,  0.6739,  0.5421, -1.0090, -0.4213, -0.0812,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2893,  0.2786,  0.6848, -1.1929, -0.4648,  0.0450,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0923,  1.0945,  0.3838, -0.7207, -0.1879, -0.2504,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 3., 2., 0.]])\n",
      "Rewards:  tensor([[-2.3868, -1.6179,  0.6027, -1.9765]])\n",
      "Next states:  tensor([[[ 0.0088,  1.3933,  0.3039, -0.2871, -0.0118, -0.1026,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1933,  0.6505,  0.5517, -1.0385, -0.4275, -0.1249,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2966,  0.2522,  0.7322, -1.1715, -0.4621,  0.0547,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0960,  1.0778,  0.3838, -0.7474, -0.2004, -0.2504,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1296,  0.0077,  0.1094, -0.1340]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1296, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1295,  0.0077,  0.1094, -0.1340]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1094]])\n",
      " \n",
      "Q_target =  tensor([[-1.0349]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.9053]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.31197864,  0.20044778,  0.76475334, -1.1635467 , -0.45774257,\n",
      "        0.04320877,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0706,  1.1839,  0.3756, -0.6590, -0.1282, -0.1491,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1616,  0.7844,  0.5356, -0.9272, -0.3908, -0.2102,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0148,  1.3786,  0.3035, -0.3403, -0.0235, -0.1008,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0923,  1.0945,  0.3838, -0.7207, -0.1879, -0.2504,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[2., 1., 0., 0.]])\n",
      "Rewards:  tensor([[ 1.6666, -0.3605, -1.7064, -1.9765]])\n",
      "Next states:  tensor([[[ 0.0742,  1.1694,  0.3667, -0.6484, -0.1364, -0.1647,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1668,  0.7631,  0.5248, -0.9505, -0.3989, -0.1609,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0178,  1.3704,  0.3035, -0.3670, -0.0286, -0.1008,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0960,  1.0778,  0.3838, -0.7474, -0.2004, -0.2504,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1312,  0.0074,  0.1088, -0.1340]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1340, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1312,  0.0074,  0.1089, -0.1339]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1089]])\n",
      " \n",
      "Q_target =  tensor([[-2.3483]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-2.2143]], grad_fn=<SubBackward0>)\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current state:  (array([ 0.31972733,  0.17359021,  0.77513933, -1.1937596 , -0.45804006,\n",
      "       -0.00594979,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.1878,  0.6739,  0.5421, -1.0090, -0.4213, -0.0812,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1241,  0.9498,  0.4516, -0.8607, -0.2928, -0.2686,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1991,  0.6279,  0.5917, -1.0081, -0.4337, -0.1240,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0178,  1.3704,  0.3035, -0.3670, -0.0286, -0.1008,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 3., 0., 3.]])\n",
      "Rewards:  tensor([[-1.6179, -2.6953, -0.9020, -2.6538]])\n",
      "Next states:  tensor([[[ 0.1933,  0.6505,  0.5517, -1.0385, -0.4275, -0.1249,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1286,  0.9298,  0.4618, -0.8895, -0.3084, -0.3121,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2050,  0.6046,  0.5917, -1.0348, -0.4399, -0.1240,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0209,  1.3615,  0.3142, -0.3942, -0.0358, -0.1437,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1308,  0.0077,  0.1093, -0.1369]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.0077, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1308,  0.0077,  0.1094, -0.1369]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1094]])\n",
      " \n",
      "Q_target =  tensor([[-0.7554]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.7631]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.3274044 ,  0.14617854,  0.7661034 , -1.2177683 , -0.4562707 ,\n",
      "        0.03538732,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.2173,  0.5578,  0.6219, -1.0567, -0.4516, -0.1169,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1511,  0.8254,  0.5043, -0.9018, -0.3698, -0.2238,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0998,  1.0604,  0.3919, -0.7751, -0.2146, -0.2840,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0303,  1.3343,  0.3178, -0.4077, -0.0537, -0.0825,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[1., 2., 1., 0.]])\n",
      "Rewards:  tensor([[-0.1918, -0.9141, -1.4533, -1.5717]])\n",
      "Next states:  tensor([[[ 0.2234,  0.5335,  0.6145, -1.0814, -0.4558, -0.0838,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1564,  0.8052,  0.5356, -0.9005, -0.3803, -0.2102,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1035,  1.0424,  0.3834, -0.8009, -0.2271, -0.2490,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0335,  1.3245,  0.3178, -0.4344, -0.0578, -0.0825,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1309,  0.0066,  0.1093, -0.1369]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.1093, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1309,  0.0066,  0.1093, -0.1369]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1093]])\n",
      " \n",
      "Q_target =  tensor([[-0.7577]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.8669]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.3352128 ,  0.1186849 ,  0.7793678 , -1.221464  , -0.45466003,\n",
      "        0.0322138 ,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.2553,  0.4095,  0.6786, -1.0996, -0.4653, -0.0404,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0814,  1.1412,  0.3757, -0.6397, -0.1562, -0.2175,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0495,  1.2666,  0.3279, -0.5692, -0.0871, -0.1231,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1197,  0.9691,  0.4243, -0.8575, -0.2793, -0.2847,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 1., 3., 2.]])\n",
      "Rewards:  tensor([[-0.7271, -1.2197, -2.2790, -1.3104]])\n",
      "Next states:  tensor([[[ 0.2621,  0.3841,  0.6786, -1.1263, -0.4673, -0.0404,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0850,  1.1263,  0.3656, -0.6658, -0.1650, -0.1767,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0528,  1.2532,  0.3362, -0.5966, -0.0950, -0.1566,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1241,  0.9498,  0.4516, -0.8607, -0.2928, -0.2686,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1309,  0.0066,  0.1079, -0.1369]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.0066, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1309,  0.0066,  0.1079, -0.1369]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1079]])\n",
      " \n",
      "Q_target =  tensor([[-0.8936]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.9001]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.34294462,  0.09065209,  0.76959884, -1.2447407 , -0.4507291 ,\n",
      "        0.07861825,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0596,  1.2266,  0.3452, -0.6047, -0.1081, -0.1167,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0119,  1.3863,  0.3117, -0.3142, -0.0185, -0.1339,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0632,  1.2130,  0.3640, -0.6049, -0.1133, -0.1026,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1511,  0.8254,  0.5043, -0.9018, -0.3698, -0.2238,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[2., 1., 3., 2.]])\n",
      "Rewards:  tensor([[-0.4397, -1.1112, -2.3185, -0.9141]])\n",
      "Next states:  tensor([[[ 0.0632,  1.2130,  0.3640, -0.6049, -0.1133, -0.1026,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0148,  1.3786,  0.3035, -0.3403, -0.0235, -0.1008,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0669,  1.1988,  0.3756, -0.6323, -0.1207, -0.1491,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1564,  0.8052,  0.5356, -0.9005, -0.3803, -0.2102,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1313,  0.0051,  0.1073, -0.1370]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1370, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1313,  0.0051,  0.1073, -0.1370]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1073]])\n",
      " \n",
      "Q_target =  tensor([[-2.9608]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-2.8238]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.35075778,  0.06197034,  0.7798444 , -1.2742678 , -0.44911557,\n",
      "        0.03227132,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.2234,  0.5335,  0.6145, -1.0814, -0.4558, -0.0838,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2553,  0.4095,  0.6786, -1.0996, -0.4653, -0.0404,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1197,  0.9691,  0.4243, -0.8575, -0.2793, -0.2847,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0463,  1.2794,  0.3392, -0.5431, -0.0810, -0.1687,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[1., 0., 2., 1.]])\n",
      "Rewards:  tensor([[ 0.3562, -0.7271, -1.3104, -1.0409]])\n",
      "Next states:  tensor([[[ 0.2295,  0.5086,  0.6040, -1.1043, -0.4574, -0.0332,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2621,  0.3841,  0.6786, -1.1263, -0.4673, -0.0404,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1241,  0.9498,  0.4516, -0.8607, -0.2928, -0.2686,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0495,  1.2666,  0.3279, -0.5692, -0.0871, -0.1231,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1306,  0.0056,  0.1083, -0.1407]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1306, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1305,  0.0056,  0.1083, -0.1406]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1083]])\n",
      " \n",
      "Q_target =  tensor([[-2.3991]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-2.2686]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.35857096,  0.0326887 ,  0.7798442 , -1.3009347 , -0.44750205,\n",
      "        0.03227133,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0923,  1.0945,  0.3838, -0.7207, -0.1879, -0.2504,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1417,  0.8665,  0.4411, -0.9651, -0.3468, -0.2227,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3352,  0.1187,  0.7794, -1.2215, -0.4547,  0.0322,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1197,  0.9691,  0.4243, -0.8575, -0.2793, -0.2847,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 2., 1., 2.]])\n",
      "Rewards:  tensor([[-1.9765,  2.6854, -1.0004, -1.3104]])\n",
      "Next states:  tensor([[[ 0.0960,  1.0778,  0.3838, -0.7474, -0.2004, -0.2504,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1462,  0.8456,  0.4588, -0.9326, -0.3586, -0.2358,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3429,  0.0907,  0.7696, -1.2447, -0.4507,  0.0786,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1241,  0.9498,  0.4516, -0.8607, -0.2928, -0.2686,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1335,  0.0056,  0.1084, -0.1407]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1407, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1335,  0.0056,  0.1085, -0.1407]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1085]])\n",
      " \n",
      "Q_target =  tensor([[6.2211]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[6.3618]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.36647242,  0.00275442,  0.79097265, -1.3306746 , -0.4483997 ,\n",
      "       -0.0179528 ,  1.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.2173,  0.5578,  0.6219, -1.0567, -0.4516, -0.1169,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3120,  0.2004,  0.7648, -1.1635, -0.4577,  0.0432,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0209,  1.3615,  0.3142, -0.3942, -0.0358, -0.1437,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1770,  0.7187,  0.5165, -1.0016, -0.4112, -0.1237,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[1., 3., 0., 2.]])\n",
      "Rewards:  tensor([[-0.1918, -2.4560, -1.8901,  0.9956]])\n",
      "Next states:  tensor([[[ 0.2234,  0.5335,  0.6145, -1.0814, -0.4558, -0.0838,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3197,  0.1736,  0.7751, -1.1938, -0.4580, -0.0059,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0240,  1.3520,  0.3142, -0.4209, -0.0429, -0.1436,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1824,  0.6965,  0.5503, -0.9848, -0.4172, -0.1190,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1337,  0.0055,  0.1083, -0.1320]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.0055, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1336,  0.0056,  0.1084, -0.1320]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1084]])\n",
      " \n",
      "Q_target =  tensor([[6.6484]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[6.6429]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.37433225, -0.0262174 ,  0.7647312 , -1.2805299 , -0.43043184,\n",
      "        0.3524415 ,  1.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.2173,  0.5578,  0.6219, -1.0567, -0.4516, -0.1169,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0240,  1.3520,  0.3142, -0.4209, -0.0429, -0.1436,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3043,  0.2266,  0.7648, -1.1369, -0.4599,  0.0432,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2295,  0.5086,  0.6040, -1.1043, -0.4574, -0.0332,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[1., 2., 0., 2.]])\n",
      "Rewards:  tensor([[-0.1918,  2.0011, -1.1432,  2.1440]])\n",
      "Next states:  tensor([[[ 0.2234,  0.5335,  0.6145, -1.0814, -0.4558, -0.0838,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0272,  1.3435,  0.3302, -0.3809, -0.0495, -0.1320,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3120,  0.2004,  0.7648, -1.1635, -0.4577,  0.0432,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2358,  0.4844,  0.6378, -1.0769, -0.4594, -0.0386,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1336,  0.0144,  0.1087, -0.1322]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1322, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1336,  0.0144,  0.1087, -0.1322]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1087]])\n",
      " \n",
      "Q_target =  tensor([[-100.]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-99.8678]], grad_fn=<SubBackward0>)\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 1/4000 [00:02<2:19:47,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  -147.86833631948318\n",
      "Iteration number:  1\n",
      "Current state:  (array([-0.00449104,  1.42082   , -0.45492014,  0.43998894,  0.0052109 ,\n",
      "        0.10304622,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0335,  1.3245,  0.3178, -0.4344, -0.0578, -0.0825,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0742,  1.1694,  0.3667, -0.6484, -0.1364, -0.1647,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1075,  1.0250,  0.4130, -0.7758, -0.2390, -0.2379,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3352,  0.1187,  0.7794, -1.2215, -0.4547,  0.0322,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 2., 0., 1.]])\n",
      "Rewards:  tensor([[-1.5591,  3.3249, -1.8052, -1.0004]])\n",
      "Next states:  tensor([[[ 0.0366,  1.3141,  0.3178, -0.4611, -0.0619, -0.0824,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0777,  1.1556,  0.3659, -0.6123, -0.1453, -0.1779,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1115,  1.0070,  0.4130, -0.8025, -0.2509, -0.2379,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3429,  0.0907,  0.7696, -1.2447, -0.4507,  0.0786,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1319,  0.0152,  0.1074, -0.2682]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.0152, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1319,  0.0152,  0.1074, -0.2682]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1074]])\n",
      " \n",
      "Q_target =  tensor([[-0.3095]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.3247]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.00904789,  1.4301343 , -0.46249375,  0.4139244 ,  0.01194612,\n",
      "        0.13471702,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.1197,  0.9691,  0.4243, -0.8575, -0.2793, -0.2847,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1668,  0.7631,  0.5248, -0.9505, -0.3989, -0.1609,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1115,  1.0070,  0.4130, -0.8025, -0.2509, -0.2379,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1156,  0.9883,  0.4243, -0.8308, -0.2651, -0.2847,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[2., 1., 3., 0.]])\n",
      "Rewards:  tensor([[-1.3104, -0.3793, -2.6805, -1.9466]])\n",
      "Next states:  tensor([[[ 0.1241,  0.9498,  0.4516, -0.8607, -0.2928, -0.2686,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1719,  0.7412,  0.5165, -0.9750, -0.4051, -0.1237,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1156,  0.9883,  0.4243, -0.8308, -0.2651, -0.2847,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1197,  0.9691,  0.4243, -0.8575, -0.2793, -0.2847,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1318,  0.0149,  0.1077, -0.2681]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.2681, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1318,  0.0149,  0.1077, -0.2680]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1077]])\n",
      " \n",
      "Q_target =  tensor([[1.3582]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[1.6262]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.01351328,  1.4388562 , -0.45100886,  0.38759804,  0.01636868,\n",
      "        0.08845931,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.3120,  0.2004,  0.7648, -1.1635, -0.4577,  0.0432,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2112,  0.5815,  0.6219, -1.0301, -0.4458, -0.1169,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1330,  0.9093,  0.4525, -0.9146, -0.3220, -0.2733,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0562,  1.2402,  0.3527, -0.5780, -0.1023, -0.1467,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 0., 0., 1.]])\n",
      "Rewards:  tensor([[-2.4560, -0.8707, -1.7417, -1.1889]])\n",
      "Next states:  tensor([[[ 0.3197,  0.1736,  0.7751, -1.1938, -0.4580, -0.0059,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2173,  0.5578,  0.6219, -1.0567, -0.4516, -0.1169,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1374,  0.8882,  0.4525, -0.9413, -0.3357, -0.2732,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0596,  1.2266,  0.3452, -0.6047, -0.1081, -0.1167,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1315,  0.0151,  0.1080, -0.2658]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1315, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1314,  0.0151,  0.1080, -0.2658]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1080]])\n",
      " \n",
      "Q_target =  tensor([[0.5497]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.6811]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.01797876,  1.4469783 , -0.45102063,  0.36092538,  0.02079207,\n",
      "        0.08847617,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0596,  1.2266,  0.3452, -0.6047, -0.1081, -0.1167,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0135,  1.4389, -0.4510,  0.3876,  0.0164,  0.0885,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0148,  1.3786,  0.3035, -0.3403, -0.0235, -0.1008,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0430,  1.2916,  0.3298, -0.5154, -0.0726, -0.1306,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[2., 0., 0., 3.]])\n",
      "Rewards:  tensor([[-0.4397,  0.4427, -1.7064, -2.5061]])\n",
      "Next states:  tensor([[[ 0.0632,  1.2130,  0.3640, -0.6049, -0.1133, -0.1026,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0180,  1.4470, -0.4510,  0.3609,  0.0208,  0.0885,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0178,  1.3704,  0.3035, -0.3670, -0.0286, -0.1008,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0463,  1.2794,  0.3392, -0.5431, -0.0810, -0.1687,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1321,  0.0141,  0.1065, -0.2658]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.1065, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1321,  0.0142,  0.1065, -0.2658]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1065]])\n",
      " \n",
      "Q_target =  tensor([[-3.8248]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-3.9312]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.02238541,  1.4560242 , -0.44551665,  0.401958  ,  0.02559757,\n",
      "        0.09611885,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0335,  1.3245,  0.3178, -0.4344, -0.0578, -0.0825,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0178,  1.3704,  0.3035, -0.3670, -0.0286, -0.1008,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2755,  0.3318,  0.6698, -1.1765, -0.4671,  0.0018,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0303,  1.3343,  0.3178, -0.4077, -0.0537, -0.0825,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 3., 2., 0.]])\n",
      "Rewards:  tensor([[-1.5591, -2.6538,  0.6161, -1.5717]])\n",
      "Next states:  tensor([[[ 3.6617e-02,  1.3141e+00,  3.1783e-01, -4.6108e-01, -6.1906e-02,\n",
      "          -8.2446e-02,  0.0000e+00,  0.0000e+00],\n",
      "         [ 2.0903e-02,  1.3615e+00,  3.1420e-01, -3.9419e-01, -3.5759e-02,\n",
      "          -1.4366e-01,  0.0000e+00,  0.0000e+00],\n",
      "         [ 2.8243e-01,  3.0547e-01,  6.9423e-01, -1.1693e+00, -4.6705e-01,\n",
      "           8.6306e-04,  0.0000e+00,  0.0000e+00],\n",
      "         [ 3.3480e-02,  1.3245e+00,  3.1782e-01, -4.3441e-01, -5.7784e-02,\n",
      "          -8.2469e-02,  0.0000e+00,  0.0000e+00]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1312,  0.0145,  0.1017, -0.2660]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.2660, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1312,  0.0145,  0.1017, -0.2660]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1017]])\n",
      " \n",
      "Q_target =  tensor([[1.3106]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[1.5766]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.02672472,  1.4644673 , -0.43707317,  0.37519178,  0.02870767,\n",
      "        0.06220768,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0135,  1.4389, -0.4510,  0.3876,  0.0164,  0.0885,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0777,  1.1556,  0.3659, -0.6123, -0.1453, -0.1779,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3508,  0.0620,  0.7798, -1.2743, -0.4491,  0.0323,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0669,  1.1988,  0.3756, -0.6323, -0.1207, -0.1491,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 3., 0., 0.]])\n",
      "Rewards:  tensor([[ 0.4427, -2.5704, -2.5064, -1.5943]])\n",
      "Next states:  tensor([[[-0.0180,  1.4470, -0.4510,  0.3609,  0.0208,  0.0885,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0814,  1.1412,  0.3757, -0.6397, -0.1562, -0.2175,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3586,  0.0327,  0.7798, -1.3009, -0.4475,  0.0323,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0706,  1.1839,  0.3756, -0.6590, -0.1282, -0.1491,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1313,  0.0144,  0.1020, -0.2638]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.1020, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1313,  0.0144,  0.1021, -0.2637]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1021]])\n",
      " \n",
      "Q_target =  tensor([[-2.5770]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-2.6791]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.03116388,  1.4730866 , -0.4466498 ,  0.38302776,  0.03141275,\n",
      "        0.05410652,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 2.8243e-01,  3.0547e-01,  6.9423e-01, -1.1693e+00, -4.6705e-01,\n",
      "           8.6306e-04,  0.0000e+00,  0.0000e+00],\n",
      "         [ 7.0590e-02,  1.1839e+00,  3.7558e-01, -6.5896e-01, -1.2819e-01,\n",
      "          -1.4909e-01,  0.0000e+00,  0.0000e+00],\n",
      "         [ 1.0350e-01,  1.0424e+00,  3.8339e-01, -8.0087e-01, -2.2706e-01,\n",
      "          -2.4900e-01,  0.0000e+00,  0.0000e+00],\n",
      "         [ 1.9329e-01,  6.5053e-01,  5.5172e-01, -1.0385e+00, -4.2750e-01,\n",
      "          -1.2494e-01,  0.0000e+00,  0.0000e+00]]])\n",
      "Actions:  tensor([[1., 2., 2., 2.]])\n",
      "Rewards:  tensor([[0.0689, 1.6666, 1.1009, 1.7735]])\n",
      "Next states:  tensor([[[ 0.2893,  0.2786,  0.6848, -1.1929, -0.4648,  0.0450,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0742,  1.1694,  0.3667, -0.6484, -0.1364, -0.1647,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1075,  1.0250,  0.4130, -0.7758, -0.2390, -0.2379,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1991,  0.6279,  0.5917, -1.0081, -0.4337, -0.1240,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1308,  0.0147,  0.0987, -0.2637]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.0147, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1308,  0.0147,  0.0987, -0.2637]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0987]])\n",
      " \n",
      "Q_target =  tensor([[-0.1121]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.1268]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.03567095,  1.4810923 , -0.45516497,  0.35570514,  0.03582995,\n",
      "        0.08835183,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0777,  1.1556,  0.3659, -0.6123, -0.1453, -0.1779,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3665,  0.0028,  0.7910, -1.3307, -0.4484, -0.0180,  1.0000,\n",
      "           0.0000],\n",
      "         [ 0.0850,  1.1263,  0.3656, -0.6658, -0.1650, -0.1767,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2486,  0.4342,  0.6378, -1.1302, -0.4632, -0.0386,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 1., 3., 2.]])\n",
      "Rewards:  tensor([[-2.5704,  6.5411, -2.2943,  1.8329]])\n",
      "Next states:  tensor([[[ 0.0814,  1.1412,  0.3757, -0.6397, -0.1562, -0.2175,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3743, -0.0262,  0.7647, -1.2805, -0.4304,  0.3524,  1.0000,\n",
      "           0.0000],\n",
      "         [ 0.0886,  1.1107,  0.3731, -0.6931, -0.1754, -0.2068,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2553,  0.4095,  0.6786, -1.0996, -0.4653, -0.0404,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1304,  0.0156,  0.0997, -0.2635]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.0997, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1304,  0.0156,  0.0996, -0.2635]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0996]])\n",
      " \n",
      "Q_target =  tensor([[-1.0733]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-1.1730]], grad_fn=<SubBackward0>)\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current state:  (array([-0.04009323,  1.4891675 , -0.44717997,  0.35876414,  0.04072457,\n",
      "        0.09790136,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.2358,  0.4844,  0.6378, -1.0769, -0.4594, -0.0386,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0312,  1.4731, -0.4466,  0.3830,  0.0314,  0.0541,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0742,  1.1694,  0.3667, -0.6484, -0.1364, -0.1647,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3743, -0.0262,  0.7647, -1.2805, -0.4304,  0.3524,  1.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 1., 2., 3.]])\n",
      "Rewards:  tensor([[  -0.5683,   -0.2099,    3.3249, -100.0000]])\n",
      "Next states:  tensor([[[ 0.2422,  0.4596,  0.6378, -1.1035, -0.4613, -0.0386,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0357,  1.4811, -0.4552,  0.3557,  0.0358,  0.0884,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0777,  1.1556,  0.3659, -0.6123, -0.1453, -0.1779,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3832, -0.0538,  0.8811, -0.8416, -0.3617,  4.2807,  1.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1314,  0.0147,  0.0972, -0.2635]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1314, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1319,  0.0144,  0.0961, -0.2638]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0961]])\n",
      " \n",
      "Q_target =  tensor([[0.4753]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.6068]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.0445158 ,  1.4966426 , -0.44719395,  0.3320882 ,  0.04561843,\n",
      "        0.09788598,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.3429,  0.0907,  0.7696, -1.2447, -0.4507,  0.0786,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0045,  1.4208, -0.4549,  0.4400,  0.0052,  0.1030,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3665,  0.0028,  0.7910, -1.3307, -0.4484, -0.0180,  1.0000,\n",
      "           0.0000],\n",
      "         [ 0.3743, -0.0262,  0.7647, -1.2805, -0.4304,  0.3524,  1.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 1., 1., 3.]])\n",
      "Rewards:  tensor([[  -3.0671,   -0.4159,    6.5411, -100.0000]])\n",
      "Next states:  tensor([[[ 0.3508,  0.0620,  0.7798, -1.2743, -0.4491,  0.0323,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0090,  1.4301, -0.4625,  0.4139,  0.0119,  0.1347,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3743, -0.0262,  0.7647, -1.2805, -0.4304,  0.3524,  1.0000,\n",
      "           0.0000],\n",
      "         [ 0.3832, -0.0538,  0.8811, -0.8416, -0.3617,  4.2807,  1.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1298,  0.0160,  0.0987, -0.2633]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.0987, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1303,  0.0156,  0.0975, -0.2635]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0975]])\n",
      " \n",
      "Q_target =  tensor([[-1.2922]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-1.3909]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.04891529,  1.5041139 , -0.44508877,  0.33189207,  0.05070709,\n",
      "        0.10178254,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.1417,  0.8665,  0.4411, -0.9651, -0.3468, -0.2227,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1933,  0.6505,  0.5517, -1.0385, -0.4275, -0.1249,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0178,  1.3704,  0.3035, -0.3670, -0.0286, -0.1008,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1374,  0.8882,  0.4525, -0.9413, -0.3357, -0.2732,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[2., 2., 3., 1.]])\n",
      "Rewards:  tensor([[ 2.6854,  1.7735, -2.6538, -0.7396]])\n",
      "Next states:  tensor([[[ 0.1462,  0.8456,  0.4588, -0.9326, -0.3586, -0.2358,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1991,  0.6279,  0.5917, -1.0081, -0.4337, -0.1240,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0209,  1.3615,  0.3142, -0.3942, -0.0358, -0.1437,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1417,  0.8665,  0.4411, -0.9651, -0.3468, -0.2227,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1303,  0.0143,  0.0950, -0.2637]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.2637, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1303,  0.0143,  0.0950, -0.2638]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0950]])\n",
      " \n",
      "Q_target =  tensor([[1.5674]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[1.8311]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.0532217 ,  1.5109961 , -0.43340635,  0.3057776 ,  0.05344252,\n",
      "        0.05471335,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0135,  1.4389, -0.4510,  0.3876,  0.0164,  0.0885,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0240,  1.3520,  0.3142, -0.4209, -0.0429, -0.1436,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0445,  1.4966, -0.4472,  0.3321,  0.0456,  0.0979,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1462,  0.8456,  0.4588, -0.9326, -0.3586, -0.2358,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 2., 2., 2.]])\n",
      "Rewards:  tensor([[ 0.4427,  2.0011, -1.3887,  1.0980]])\n",
      "Next states:  tensor([[[-0.0180,  1.4470, -0.4510,  0.3609,  0.0208,  0.0885,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0272,  1.3435,  0.3302, -0.3809, -0.0495, -0.1320,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0489,  1.5041, -0.4451,  0.3319,  0.0507,  0.1018,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1511,  0.8254,  0.5043, -0.9018, -0.3698, -0.2238,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1317,  0.0135,  0.0940, -0.2612]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.0135, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1317,  0.0135,  0.0939, -0.2612]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0939]])\n",
      " \n",
      "Q_target =  tensor([[-0.5593]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.5728]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.05762072,  1.5172737 , -0.44501027,  0.27881798,  0.05850561,\n",
      "        0.10127108,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0303,  1.3343,  0.3178, -0.4077, -0.0537, -0.0825,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1511,  0.8254,  0.5043, -0.9018, -0.3698, -0.2238,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2050,  0.6046,  0.5917, -1.0348, -0.4399, -0.1240,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0742,  1.1694,  0.3667, -0.6484, -0.1364, -0.1647,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 2., 2., 2.]])\n",
      "Rewards:  tensor([[-1.5717, -0.9141, -0.0334,  3.3249]])\n",
      "Next states:  tensor([[[ 0.0335,  1.3245,  0.3178, -0.4344, -0.0578, -0.0825,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1564,  0.8052,  0.5356, -0.9005, -0.3803, -0.2102,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2112,  0.5815,  0.6219, -1.0301, -0.4458, -0.1169,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0777,  1.1556,  0.3659, -0.6123, -0.1453, -0.1779,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1303,  0.0135,  0.0949, -0.2614]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1303, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1303,  0.0135,  0.0950, -0.2614]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0950]])\n",
      " \n",
      "Q_target =  tensor([[0.3680]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.4983]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.06201982,  1.5229517 , -0.44502607,  0.25214687,  0.06356645,\n",
      "        0.1012259 ,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0850,  1.1263,  0.3656, -0.6658, -0.1650, -0.1767,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2422,  0.4596,  0.6378, -1.1035, -0.4613, -0.0386,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0742,  1.1694,  0.3667, -0.6484, -0.1364, -0.1647,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3120,  0.2004,  0.7648, -1.1635, -0.4577,  0.0432,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 0., 2., 3.]])\n",
      "Rewards:  tensor([[-2.2943, -0.5871,  3.3249, -2.4560]])\n",
      "Next states:  tensor([[[ 0.0886,  1.1107,  0.3731, -0.6931, -0.1754, -0.2068,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2486,  0.4342,  0.6378, -1.1302, -0.4632, -0.0386,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0777,  1.1556,  0.3659, -0.6123, -0.1453, -0.1779,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3197,  0.1736,  0.7751, -1.1938, -0.4580, -0.0059,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1293,  0.0137,  0.0953, -0.2613]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.0137, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1293,  0.0137,  0.0953, -0.2613]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0953]])\n",
      " \n",
      "Q_target =  tensor([[-1.0426]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-1.0562]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.06651659,  1.5280309 , -0.45725766,  0.22539943,  0.07107251,\n",
      "        0.15013507,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0240,  1.3520,  0.3142, -0.4209, -0.0429, -0.1436,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2755,  0.3318,  0.6698, -1.1765, -0.4671,  0.0018,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3274,  0.1462,  0.7661, -1.2178, -0.4563,  0.0354,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0665,  1.5280, -0.4573,  0.2254,  0.0711,  0.1501,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[2., 2., 2., 3.]])\n",
      "Rewards:  tensor([[ 2.0011,  0.6161, -0.8658,  0.7751]])\n",
      "Next states:  tensor([[[ 2.7207e-02,  1.3435e+00,  3.3016e-01, -3.8095e-01, -4.9538e-02,\n",
      "          -1.3198e-01,  0.0000e+00,  0.0000e+00],\n",
      "         [ 2.8243e-01,  3.0547e-01,  6.9423e-01, -1.1693e+00, -4.6705e-01,\n",
      "           8.6306e-04,  0.0000e+00,  0.0000e+00],\n",
      "         [ 3.3521e-01,  1.1868e-01,  7.7937e-01, -1.2215e+00, -4.5466e-01,\n",
      "           3.2214e-02,  0.0000e+00,  0.0000e+00],\n",
      "         [-7.0947e-02,  1.5325e+00, -4.4891e-01,  1.9949e-01,  7.6883e-02,\n",
      "           1.1623e-01,  0.0000e+00,  0.0000e+00]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1295,  0.0122,  0.0951, -0.2612]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.2612, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1295,  0.0122,  0.0951, -0.2612]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0951]])\n",
      " \n",
      "Q_target =  tensor([[0.8693]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[1.1305]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.07094717,  1.532526  , -0.44891316,  0.19948836,  0.0768834 ,\n",
      "        0.1162286 ,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0272,  1.3435,  0.3302, -0.3809, -0.0495, -0.1320,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1824,  0.6965,  0.5503, -0.9848, -0.4172, -0.1190,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0463,  1.2794,  0.3392, -0.5431, -0.0810, -0.1687,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3352,  0.1187,  0.7794, -1.2215, -0.4547,  0.0322,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[1., 1., 1., 1.]])\n",
      "Rewards:  tensor([[-0.8179, -0.1090, -1.0409, -1.0004]])\n",
      "Next states:  tensor([[[ 0.0303,  1.3343,  0.3178, -0.4077, -0.0537, -0.0825,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1878,  0.6739,  0.5421, -1.0090, -0.4213, -0.0812,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0495,  1.2666,  0.3279, -0.5692, -0.0871, -0.1231,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3429,  0.0907,  0.7696, -1.2447, -0.4507,  0.0786,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1296,  0.0121,  0.0949, -0.2599]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1296, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1296,  0.0121,  0.0949, -0.2598]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0949]])\n",
      " \n",
      "Q_target =  tensor([[0.1226]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.2521]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.07537784,  1.5364214 , -0.4489282 ,  0.1728163 ,  0.08269465,\n",
      "        0.11623538,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.2050,  0.6046,  0.5917, -1.0348, -0.4399, -0.1240,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0706,  1.1839,  0.3756, -0.6590, -0.1282, -0.1491,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1511,  0.8254,  0.5043, -0.9018, -0.3698, -0.2238,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1770,  0.7187,  0.5165, -1.0016, -0.4112, -0.1237,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[2., 2., 2., 2.]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards:  tensor([[-0.0334,  1.6666, -0.9141,  0.9956]])\n",
      "Next states:  tensor([[[ 0.2112,  0.5815,  0.6219, -1.0301, -0.4458, -0.1169,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0742,  1.1694,  0.3667, -0.6484, -0.1364, -0.1647,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1564,  0.8052,  0.5356, -0.9005, -0.3803, -0.2102,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1824,  0.6965,  0.5503, -0.9848, -0.4172, -0.1190,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1292,  0.0122,  0.0952, -0.2598]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.0952, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1291,  0.0122,  0.0952, -0.2598]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0952]])\n",
      " \n",
      "Q_target =  tensor([[-2.4689]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-2.5641]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.07978668,  1.5410744 , -0.44719177,  0.20643924,  0.08895771,\n",
      "        0.12527278,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.3508,  0.0620,  0.7798, -1.2743, -0.4491,  0.0323,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0998,  1.0604,  0.3919, -0.7751, -0.2146, -0.2840,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0029,  1.4056,  0.2952, -0.2345, -0.0034, -0.0669,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1075,  1.0250,  0.4130, -0.7758, -0.2390, -0.2379,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 1., 0., 0.]])\n",
      "Rewards:  tensor([[-2.5064, -1.4533, -1.3631, -1.8052]])\n",
      "Next states:  tensor([[[ 0.3586,  0.0327,  0.7798, -1.3009, -0.4475,  0.0323,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1035,  1.0424,  0.3834, -0.8009, -0.2271, -0.2490,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0058,  1.3998,  0.2948, -0.2602, -0.0067, -0.0661,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1115,  1.0070,  0.4130, -0.8025, -0.2509, -0.2379,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1292,  0.0121,  0.0914, -0.2597]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.2597, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1291,  0.0121,  0.0914, -0.2597]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0914]])\n",
      " \n",
      "Q_target =  tensor([[1.0054]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[1.2652]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.08412876,  1.5451255 , -0.43883318,  0.17976905,  0.09354773,\n",
      "        0.09180842,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.2112,  0.5815,  0.6219, -1.0301, -0.4458, -0.1169,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2050,  0.6046,  0.5917, -1.0348, -0.4399, -0.1240,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0267,  1.4645, -0.4371,  0.3752,  0.0287,  0.0622,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0398,  1.3032,  0.3178, -0.4877, -0.0660, -0.0824,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 2., 2., 3.]])\n",
      "Rewards:  tensor([[-0.8707, -0.0334, -2.6781, -2.5052]])\n",
      "Next states:  tensor([[[ 0.2173,  0.5578,  0.6219, -1.0567, -0.4516, -0.1169,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2112,  0.5815,  0.6219, -1.0301, -0.4458, -0.1169,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0312,  1.4731, -0.4466,  0.3830,  0.0314,  0.0541,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0430,  1.2916,  0.3298, -0.5154, -0.0726, -0.1306,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1299,  0.0118,  0.0912, -0.2579]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.2579, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1298,  0.0118,  0.0913, -0.2579]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0913]])\n",
      " \n",
      "Q_target =  tensor([[1.5402]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[1.7981]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.08837376,  1.5485951 , -0.42663965,  0.15407182,  0.09566854,\n",
      "        0.04241986,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0706,  1.1839,  0.3756, -0.6590, -0.1282, -0.1491,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0998,  1.0604,  0.3919, -0.7751, -0.2146, -0.2840,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1462,  0.8456,  0.4588, -0.9326, -0.3586, -0.2358,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0632,  1.2130,  0.3640, -0.6049, -0.1133, -0.1026,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[2., 1., 2., 3.]])\n",
      "Rewards:  tensor([[ 1.6666, -1.4533,  1.0980, -2.3185]])\n",
      "Next states:  tensor([[[ 0.0742,  1.1694,  0.3667, -0.6484, -0.1364, -0.1647,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1035,  1.0424,  0.3834, -0.8009, -0.2271, -0.2490,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1511,  0.8254,  0.5043, -0.9018, -0.3698, -0.2238,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0669,  1.1988,  0.3756, -0.6323, -0.1207, -0.1491,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1295,  0.0119,  0.0913, -0.2556]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.2556, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1295,  0.0119,  0.0913, -0.2556]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0913]])\n",
      " \n",
      "Q_target =  tensor([[1.7250]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[1.9806]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.09252338,  1.5514891 , -0.4146491 ,  0.1286435 ,  0.09535243,\n",
      "       -0.00632273,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.2966,  0.2522,  0.7322, -1.1715, -0.4621,  0.0547,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2553,  0.4095,  0.6786, -1.0996, -0.4653, -0.0404,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1933,  0.6505,  0.5517, -1.0385, -0.4275, -0.1249,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0777,  1.1556,  0.3659, -0.6123, -0.1453, -0.1779,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[2., 0., 2., 3.]])\n",
      "Rewards:  tensor([[ 2.0442, -0.7271,  1.7735, -2.5704]])\n",
      "Next states:  tensor([[[ 0.3043,  0.2266,  0.7648, -1.1369, -0.4599,  0.0432,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2621,  0.3841,  0.6786, -1.1263, -0.4673, -0.0404,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1991,  0.6279,  0.5917, -1.0081, -0.4337, -0.1240,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0814,  1.1412,  0.3757, -0.6397, -0.1562, -0.2175,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1288,  0.0123,  0.0921, -0.2528]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.2528, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1287,  0.0123,  0.0922, -0.2529]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0922]])\n",
      " \n",
      "Q_target =  tensor([[1.6971]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[1.9499]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.09659366,  1.5538056 , -0.4046503 ,  0.10310096,  0.09300269,\n",
      "       -0.04699904,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.2621,  0.3841,  0.6786, -1.1263, -0.4673, -0.0404,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0841,  1.5451, -0.4388,  0.1798,  0.0935,  0.0918,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0463,  1.2794,  0.3392, -0.5431, -0.0810, -0.1687,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1991,  0.6279,  0.5917, -1.0081, -0.4337, -0.1240,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[1., 3., 1., 0.]])\n",
      "Rewards:  tensor([[ 0.1163,  1.4499, -1.0409, -0.9020]])\n",
      "Next states:  tensor([[[ 0.2688,  0.3583,  0.6698, -1.1498, -0.4672,  0.0018,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0884,  1.5486, -0.4266,  0.1541,  0.0957,  0.0424,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0495,  1.2666,  0.3279, -0.5692, -0.0871, -0.1231,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2050,  0.6046,  0.5917, -1.0348, -0.4399, -0.1240,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1299,  0.0117,  0.0912, -0.2501]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.2501, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1299,  0.0117,  0.0912, -0.2500]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0912]])\n",
      " \n",
      "Q_target =  tensor([[1.7751]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[2.0252]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.1005889 ,  1.5555347 , -0.3952461 ,  0.07711565,  0.08875538,\n",
      "       -0.08495423,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0923,  1.0945,  0.3838, -0.7207, -0.1879, -0.2504,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0178,  1.3704,  0.3035, -0.3670, -0.0286, -0.1008,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0960,  1.0778,  0.3838, -0.7474, -0.2004, -0.2504,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2966,  0.2522,  0.7322, -1.1715, -0.4621,  0.0547,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 3., 3., 2.]])\n",
      "Rewards:  tensor([[-1.9765, -2.6538, -2.5956,  2.0442]])\n",
      "Next states:  tensor([[[ 0.0960,  1.0778,  0.3838, -0.7474, -0.2004, -0.2504,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0209,  1.3615,  0.3142, -0.3942, -0.0358, -0.1437,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0998,  1.0604,  0.3919, -0.7751, -0.2146, -0.2840,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3043,  0.2266,  0.7648, -1.1369, -0.4599,  0.0432,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1296,  0.0118,  0.0913, -0.2475]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1296, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1295,  0.0119,  0.0914, -0.2476]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0914]])\n",
      " \n",
      "Q_target =  tensor([[0.8021]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.9317]], grad_fn=<SubBackward0>)\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current state:  (array([-0.10458393,  1.5566641 , -0.39523312,  0.05044609,  0.08450904,\n",
      "       -0.08493441,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.1330,  0.9093,  0.4525, -0.9146, -0.3220, -0.2733,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0366,  1.3141,  0.3178, -0.4611, -0.0619, -0.0824,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0088,  1.3933,  0.3039, -0.2871, -0.0118, -0.1026,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0058,  1.3998,  0.2948, -0.2602, -0.0067, -0.0661,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 3., 3.]])\n",
      "Rewards:  tensor([[-1.7417, -1.5409, -2.4486, -2.3868]])\n",
      "Next states:  tensor([[[ 0.1374,  0.8882,  0.4525, -0.9413, -0.3357, -0.2732,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0398,  1.3032,  0.3178, -0.4877, -0.0660, -0.0824,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0119,  1.3863,  0.3117, -0.3142, -0.0185, -0.1339,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0088,  1.3933,  0.3039, -0.2871, -0.0118, -0.1026,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1288,  0.0116,  0.0907, -0.2475]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.0907, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1287,  0.0116,  0.0908, -0.2475]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0908]])\n",
      " \n",
      "Q_target =  tensor([[-0.1781]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.2688]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.10851459,  1.5586544 , -0.38947058,  0.08865829,  0.08094049,\n",
      "       -0.07137737,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0495,  1.2666,  0.3279, -0.5692, -0.0871, -0.1231,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0445,  1.4966, -0.4472,  0.3321,  0.0456,  0.0979,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2621,  0.3841,  0.6786, -1.1263, -0.4673, -0.0404,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1933,  0.6505,  0.5517, -1.0385, -0.4275, -0.1249,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 2., 1., 2.]])\n",
      "Rewards:  tensor([[-2.2790, -1.3887,  0.1163,  1.7735]])\n",
      "Next states:  tensor([[[ 0.0528,  1.2532,  0.3362, -0.5966, -0.0950, -0.1566,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0489,  1.5041, -0.4451,  0.3319,  0.0507,  0.1018,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2688,  0.3583,  0.6698, -1.1498, -0.4672,  0.0018,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1991,  0.6279,  0.5917, -1.0081, -0.4337, -0.1240,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1287,  0.0117,  0.0909, -0.2474]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.0117, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1287,  0.0117,  0.0909, -0.2474]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0909]])\n",
      " \n",
      "Q_target =  tensor([[-0.5960]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.6076]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.11253738,  1.560012  , -0.40104556,  0.06040091,  0.07972745,\n",
      "       -0.02426054,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.1878,  0.6739,  0.5421, -1.0090, -0.4213, -0.0812,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1417,  0.8665,  0.4411, -0.9651, -0.3468, -0.2227,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0798,  1.5411, -0.4472,  0.2064,  0.0890,  0.1253,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1462,  0.8456,  0.4588, -0.9326, -0.3586, -0.2358,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 2., 3., 2.]])\n",
      "Rewards:  tensor([[-1.6179,  2.6854,  0.9149,  1.0980]])\n",
      "Next states:  tensor([[[ 0.1933,  0.6505,  0.5517, -1.0385, -0.4275, -0.1249,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1462,  0.8456,  0.4588, -0.9326, -0.3586, -0.2358,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0841,  1.5451, -0.4388,  0.1798,  0.0935,  0.0918,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1511,  0.8254,  0.5043, -0.9018, -0.3698, -0.2238,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1288,  0.0109,  0.0909, -0.2472]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.0109, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1287,  0.0109,  0.0909, -0.2472]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0909]])\n",
      " \n",
      "Q_target =  tensor([[-0.4966]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.5075]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.11661911,  1.5607532 , -0.40847692,  0.0329264 ,  0.08002232,\n",
      "        0.00589728,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0398,  1.3032,  0.3178, -0.4877, -0.0660, -0.0824,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0224,  1.4560, -0.4455,  0.4020,  0.0256,  0.0961,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0489,  1.5041, -0.4451,  0.3319,  0.0507,  0.1018,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1156,  0.9883,  0.4243, -0.8308, -0.2651, -0.2847,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 3., 3., 0.]])\n",
      "Rewards:  tensor([[-2.5052,  1.2099,  1.4733, -1.9466]])\n",
      "Next states:  tensor([[[ 0.0430,  1.2916,  0.3298, -0.5154, -0.0726, -0.1306,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0267,  1.4645, -0.4371,  0.3752,  0.0287,  0.0622,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0532,  1.5110, -0.4334,  0.3058,  0.0534,  0.0547,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1197,  0.9691,  0.4243, -0.8575, -0.2793, -0.2847,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1298,  0.0096,  0.0900, -0.2472]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1298, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1298,  0.0096,  0.0900, -0.2472]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0900]])\n",
      " \n",
      "Q_target =  tensor([[0.1423]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.2721]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.12070093,  1.5608945 , -0.408477  ,  0.00625972,  0.08031718,\n",
      "        0.00589729,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0209,  1.3615,  0.3142, -0.3942, -0.0358, -0.1437,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0632,  1.2130,  0.3640, -0.6049, -0.1133, -0.1026,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0148,  1.3786,  0.3035, -0.3403, -0.0235, -0.1008,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0180,  1.4470, -0.4510,  0.3609,  0.0208,  0.0885,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 3., 0., 2.]])\n",
      "Rewards:  tensor([[-1.8901, -2.3185, -1.7064, -3.9302]])\n",
      "Next states:  tensor([[[ 0.0240,  1.3520,  0.3142, -0.4209, -0.0429, -0.1436,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0669,  1.1988,  0.3756, -0.6323, -0.1207, -0.1491,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0178,  1.3704,  0.3035, -0.3670, -0.0286, -0.1008,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0224,  1.4560, -0.4455,  0.4020,  0.0256,  0.0961,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1288,  0.0099,  0.0900, -0.2474]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.0900, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1288,  0.0099,  0.0900, -0.2474]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0900]])\n",
      " \n",
      "Q_target =  tensor([[0.1955]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.1055]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.12469836,  1.561703  , -0.400742  ,  0.03587509,  0.08131444,\n",
      "        0.01994514,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.3743, -0.0262,  0.7647, -1.2805, -0.4304,  0.3524,  1.0000,\n",
      "           0.0000],\n",
      "         [-0.0357,  1.4811, -0.4552,  0.3557,  0.0358,  0.0884,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1417,  0.8665,  0.4411, -0.9651, -0.3468, -0.2227,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0090,  1.4301, -0.4625,  0.4139,  0.0119,  0.1347,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 2., 2., 3.]])\n",
      "Rewards:  tensor([[-100.0000,   -1.1719,    2.6854,    1.2515]])\n",
      "Next states:  tensor([[[ 0.3832, -0.0538,  0.8811, -0.8416, -0.3617,  4.2807,  1.0000,\n",
      "           0.0000],\n",
      "         [-0.0401,  1.4892, -0.4472,  0.3588,  0.0407,  0.0979,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1462,  0.8456,  0.4588, -0.9326, -0.3586, -0.2358,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0135,  1.4389, -0.4510,  0.3876,  0.0164,  0.0885,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1284,  0.0109,  0.0914, -0.2468]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.2468, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1287,  0.0100,  0.0893, -0.2465]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0893]])\n",
      " \n",
      "Q_target =  tensor([[1.1116]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[1.3585]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.12862548,  1.5619217 , -0.39193374,  0.00976512,  0.08053643,\n",
      "       -0.01556019,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.1166,  1.5608, -0.4085,  0.0329,  0.0800,  0.0059,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0742,  1.1694,  0.3667, -0.6484, -0.1364, -0.1647,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0178,  1.3704,  0.3035, -0.3670, -0.0286, -0.1008,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0090,  1.4301, -0.4625,  0.4139,  0.0119,  0.1347,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 2., 3., 3.]])\n",
      "Rewards:  tensor([[ 0.0532,  3.3249, -2.6538,  1.2515]])\n",
      "Next states:  tensor([[[-0.1207,  1.5609, -0.4085,  0.0063,  0.0803,  0.0059,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0777,  1.1556,  0.3659, -0.6123, -0.1453, -0.1779,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0209,  1.3615,  0.3142, -0.3942, -0.0358, -0.1437,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0135,  1.4389, -0.4510,  0.3876,  0.0164,  0.0885,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1291,  0.0098,  0.0900, -0.2454]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.0098, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1291,  0.0098,  0.0901, -0.2454]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0901]])\n",
      " \n",
      "Q_target =  tensor([[-1.0537]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-1.0635]], grad_fn=<SubBackward0>)\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current state:  (array([-0.13263015,  1.5615237 , -0.40166444, -0.01775121,  0.08172671,\n",
      "        0.02380573,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0884,  1.5486, -0.4266,  0.1541,  0.0957,  0.0424,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0401,  1.4892, -0.4472,  0.3588,  0.0407,  0.0979,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3429,  0.0907,  0.7696, -1.2447, -0.4507,  0.0786,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1326,  1.5615, -0.4017, -0.0178,  0.0817,  0.0238,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 0., 3., 0.]])\n",
      "Rewards:  tensor([[ 1.6346,  0.3802, -3.0671, -0.2597]])\n",
      "Next states:  tensor([[[-0.0925,  1.5515, -0.4146,  0.1286,  0.0954, -0.0063,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0445,  1.4966, -0.4472,  0.3321,  0.0456,  0.0979,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3508,  0.0620,  0.7798, -1.2743, -0.4491,  0.0323,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1366,  1.5605, -0.4017, -0.0444,  0.0829,  0.0238,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1291,  0.0084,  0.0904, -0.2453]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1291, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1291,  0.0084,  0.0905, -0.2453]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0905]])\n",
      " \n",
      "Q_target =  tensor([[-0.1701]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.0410]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.13663483,  1.5605259 , -0.40166444, -0.044418  ,  0.08291701,\n",
      "        0.02380572,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.3429,  0.0907,  0.7696, -1.2447, -0.4507,  0.0786,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0135,  1.4389, -0.4510,  0.3876,  0.0164,  0.0885,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1719,  0.7412,  0.5165, -0.9750, -0.4051, -0.1237,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1156,  0.9883,  0.4243, -0.8308, -0.2651, -0.2847,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 0., 0., 0.]])\n",
      "Rewards:  tensor([[-3.0671,  0.4427, -0.9127, -1.9466]])\n",
      "Next states:  tensor([[[ 0.3508,  0.0620,  0.7798, -1.2743, -0.4491,  0.0323,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0180,  1.4470, -0.4510,  0.3609,  0.0208,  0.0885,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1770,  0.7187,  0.5165, -1.0016, -0.4112, -0.1237,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1197,  0.9691,  0.4243, -0.8575, -0.2793, -0.2847,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1282,  0.0089,  0.0913, -0.2454]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.0089, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1281,  0.0089,  0.0913, -0.2454]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0913]])\n",
      " \n",
      "Q_target =  tensor([[-1.2939]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-1.3028]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.14070511,  1.558925  , -0.40989462, -0.07130706,  0.08575679,\n",
      "        0.05679541,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0224,  1.4560, -0.4455,  0.4020,  0.0256,  0.0961,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2486,  0.4342,  0.6378, -1.1302, -0.4632, -0.0386,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0576,  1.5173, -0.4450,  0.2788,  0.0585,  0.1013,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2173,  0.5578,  0.6219, -1.0567, -0.4516, -0.1169,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 2., 0., 1.]])\n",
      "Rewards:  tensor([[ 1.2099,  1.8329,  0.2740, -0.1918]])\n",
      "Next states:  tensor([[[-0.0267,  1.4645, -0.4371,  0.3752,  0.0287,  0.0622,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2553,  0.4095,  0.6786, -1.0996, -0.4653, -0.0404,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0620,  1.5230, -0.4450,  0.2521,  0.0636,  0.1012,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2234,  0.5335,  0.6145, -1.0814, -0.4558, -0.0838,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1289,  0.0068,  0.0908, -0.2454]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.0068, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1288,  0.0068,  0.0908, -0.2454]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0908]])\n",
      " \n",
      "Q_target =  tensor([[-1.9842]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-1.9910]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.14487047,  1.5567226 , -0.42180175, -0.09819932,  0.0909796 ,\n",
      "        0.1044561 ,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0562,  1.2402,  0.3527, -0.5780, -0.1023, -0.1467,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0884,  1.5486, -0.4266,  0.1541,  0.0957,  0.0424,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1166,  1.5608, -0.4085,  0.0329,  0.0800,  0.0059,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1462,  0.8456,  0.4588, -0.9326, -0.3586, -0.2358,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[1., 3., 0., 2.]])\n",
      "Rewards:  tensor([[-1.1889,  1.6346,  0.0532,  1.0980]])\n",
      "Next states:  tensor([[[ 0.0596,  1.2266,  0.3452, -0.6047, -0.1081, -0.1167,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0925,  1.5515, -0.4146,  0.1286,  0.0954, -0.0063,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1207,  1.5609, -0.4085,  0.0063,  0.0803,  0.0059,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1511,  0.8254,  0.5043, -0.9018, -0.3698, -0.2238,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1293,  0.0040,  0.0900, -0.2452]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.0900, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1293,  0.0040,  0.0901, -0.2452]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0901]])\n",
      " \n",
      "Q_target =  tensor([[-2.3363]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-2.4264]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.1492817 ,  1.5551564 , -0.44559765, -0.06988969,  0.09541519,\n",
      "        0.08871183,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0463,  1.2794,  0.3392, -0.5431, -0.0810, -0.1687,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1115,  1.0070,  0.4130, -0.8025, -0.2509, -0.2379,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0998,  1.0604,  0.3919, -0.7751, -0.2146, -0.2840,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1564,  0.8052,  0.5356, -0.9005, -0.3803, -0.2102,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[1., 3., 1., 0.]])\n",
      "Rewards:  tensor([[-1.0409, -2.6805, -1.4533, -1.4156]])\n",
      "Next states:  tensor([[[ 0.0495,  1.2666,  0.3279, -0.5692, -0.0871, -0.1231,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1156,  0.9883,  0.4243, -0.8308, -0.2651, -0.2847,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1035,  1.0424,  0.3834, -0.8009, -0.2271, -0.2490,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1616,  0.7844,  0.5356, -0.9272, -0.3908, -0.2102,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1281,  0.0045,  0.0876, -0.2456]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.0876, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1281,  0.0045,  0.0877, -0.2456]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0877]])\n",
      " \n",
      "Q_target =  tensor([[-2.3651]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-2.4527]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.15391502,  1.5541189 , -0.46710777, -0.04636179,  0.09915904,\n",
      "        0.07487721,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0528,  1.2532,  0.3362, -0.5966, -0.0950, -0.1566,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0088,  1.3933,  0.3039, -0.2871, -0.0118, -0.1026,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0576,  1.5173, -0.4450,  0.2788,  0.0585,  0.1013,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0272,  1.3435,  0.3302, -0.3809, -0.0495, -0.1320,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[2., 3., 0., 1.]])\n",
      "Rewards:  tensor([[ 1.0187, -2.4486,  0.2740, -0.8179]])\n",
      "Next states:  tensor([[[ 0.0562,  1.2402,  0.3527, -0.5780, -0.1023, -0.1467,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0119,  1.3863,  0.3117, -0.3142, -0.0185, -0.1339,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0620,  1.5230, -0.4450,  0.2521,  0.0636,  0.1012,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0303,  1.3343,  0.3178, -0.4077, -0.0537, -0.0825,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1291,  0.0038,  0.0832, -0.2455]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.0832, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1291,  0.0038,  0.0832, -0.2455]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0832]])\n",
      " \n",
      "Q_target =  tensor([[-0.0872]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.1704]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.1585041 ,  1.5539562 , -0.46334386, -0.00752968,  0.10355607,\n",
      "        0.08794049,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0925,  1.5515, -0.4146,  0.1286,  0.0954, -0.0063,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3429,  0.0907,  0.7696, -1.2447, -0.4507,  0.0786,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0398,  1.3032,  0.3178, -0.4877, -0.0660, -0.0824,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0045,  1.4208, -0.4549,  0.4400,  0.0052,  0.1030,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 3., 3., 1.]])\n",
      "Rewards:  tensor([[ 1.6058, -3.0671, -2.5052, -0.4159]])\n",
      "Next states:  tensor([[[-0.0966,  1.5538, -0.4047,  0.1031,  0.0930, -0.0470,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3508,  0.0620,  0.7798, -1.2743, -0.4491,  0.0323,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0430,  1.2916,  0.3298, -0.5154, -0.0726, -0.1306,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0090,  1.4301, -0.4625,  0.4139,  0.0119,  0.1347,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1286,  0.0041,  0.0836, -0.2454]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.0041, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1286,  0.0041,  0.0837, -0.2454]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0837]])\n",
      " \n",
      "Q_target =  tensor([[-1.8662]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-1.8703]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.16318512,  1.5531783 , -0.47486869, -0.03505757,  0.11028239,\n",
      "        0.13452624,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0884,  1.5486, -0.4266,  0.1541,  0.0957,  0.0424,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1207,  1.5609, -0.4085,  0.0063,  0.0803,  0.0059,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2688,  0.3583,  0.6698, -1.1498, -0.4672,  0.0018,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0814,  1.1412,  0.3757, -0.6397, -0.1562, -0.2175,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 2., 0., 1.]])\n",
      "Rewards:  tensor([[ 1.6346,  0.1064, -0.6386, -1.2197]])\n",
      "Next states:  tensor([[[-0.0925,  1.5515, -0.4146,  0.1286,  0.0954, -0.0063,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1247,  1.5617, -0.4007,  0.0359,  0.0813,  0.0199,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2755,  0.3318,  0.6698, -1.1765, -0.4671,  0.0018,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0850,  1.1263,  0.3656, -0.6658, -0.1650, -0.1767,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1289,  0.0015,  0.0835, -0.2453]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.2453, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1289,  0.0015,  0.0835, -0.2453]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0835]])\n",
      " \n",
      "Q_target =  tensor([[0.5112]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.7566]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.16777869,  1.5518109 , -0.4639184 , -0.06111654,  0.11480376,\n",
      "        0.09042736,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.2050,  0.6046,  0.5917, -1.0348, -0.4399, -0.1240,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0754,  1.5364, -0.4489,  0.1728,  0.0827,  0.1162,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3197,  0.1736,  0.7751, -1.1938, -0.4580, -0.0059,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3352,  0.1187,  0.7794, -1.2215, -0.4547,  0.0322,  0.0000,\n",
      "           0.0000]]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions:  tensor([[2., 2., 1., 1.]])\n",
      "Rewards:  tensor([[-0.0334, -2.5632, -0.8637, -1.0004]])\n",
      "Next states:  tensor([[[ 0.2112,  0.5815,  0.6219, -1.0301, -0.4458, -0.1169,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0798,  1.5411, -0.4472,  0.2064,  0.0890,  0.1253,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3274,  0.1462,  0.7661, -1.2178, -0.4563,  0.0354,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3429,  0.0907,  0.7696, -1.2447, -0.4507,  0.0786,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1279,  0.0021,  0.0845, -0.2444]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1279, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1279,  0.0021,  0.0845, -0.2444]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0845]])\n",
      " \n",
      "Q_target =  tensor([[-0.6454]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.5175]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.17237234,  1.5498438 , -0.46391812, -0.08778489,  0.11932512,\n",
      "        0.09042723,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0178,  1.3704,  0.3035, -0.3670, -0.0286, -0.1008,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1046,  1.5567, -0.3952,  0.0504,  0.0845, -0.0849,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2050,  0.6046,  0.5917, -1.0348, -0.4399, -0.1240,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1462,  0.8456,  0.4588, -0.9326, -0.3586, -0.2358,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 2., 2., 2.]])\n",
      "Rewards:  tensor([[-2.6538, -0.2680, -0.0334,  1.0980]])\n",
      "Next states:  tensor([[[ 0.0209,  1.3615,  0.3142, -0.3942, -0.0358, -0.1437,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1085,  1.5587, -0.3895,  0.0887,  0.0809, -0.0714,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2112,  0.5815,  0.6219, -1.0301, -0.4458, -0.1169,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1511,  0.8254,  0.5043, -0.9018, -0.3698, -0.2238,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1293,  0.0016,  0.0837, -0.2445]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.2445, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1293,  0.0017,  0.0837, -0.2446]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0837]])\n",
      " \n",
      "Q_target =  tensor([[0.3542]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.5987]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.17689209,  1.5472968 , -0.45460454, -0.11341994,  0.12194805,\n",
      "        0.05245871,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.1678,  1.5518, -0.4639, -0.0611,  0.1148,  0.0904,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1374,  0.8882,  0.4525, -0.9413, -0.3357, -0.2732,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2050,  0.6046,  0.5917, -1.0348, -0.4399, -0.1240,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1824,  0.6965,  0.5503, -0.9848, -0.4172, -0.1190,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 1., 2., 1.]])\n",
      "Rewards:  tensor([[-0.7291, -0.7396, -0.0334, -0.1090]])\n",
      "Next states:  tensor([[[-0.1724,  1.5498, -0.4639, -0.0878,  0.1193,  0.0904,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1417,  0.8665,  0.4411, -0.9651, -0.3468, -0.2227,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2112,  0.5815,  0.6219, -1.0301, -0.4458, -0.1169,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1878,  0.6739,  0.5421, -1.0090, -0.4213, -0.0812,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1289,  0.0019,  0.0841, -0.2436]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.0019, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1289,  0.0019,  0.0841, -0.2436]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0841]])\n",
      " \n",
      "Q_target =  tensor([[-1.7353]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-1.7371]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.1814826 ,  1.544126  , -0.4635361 , -0.14129263,  0.12640193,\n",
      "        0.08907788,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0119,  1.3863,  0.3117, -0.3142, -0.0185, -0.1339,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3665,  0.0028,  0.7910, -1.3307, -0.4484, -0.0180,  1.0000,\n",
      "           0.0000],\n",
      "         [ 0.0029,  1.4056,  0.2952, -0.2345, -0.0034, -0.0669,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0489,  1.5041, -0.4451,  0.3319,  0.0507,  0.1018,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[1., 1., 0., 3.]])\n",
      "Rewards:  tensor([[-1.1112,  6.5411, -1.3631,  1.4733]])\n",
      "Next states:  tensor([[[ 0.0148,  1.3786,  0.3035, -0.3403, -0.0235, -0.1008,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3743, -0.0262,  0.7647, -1.2805, -0.4304,  0.3524,  1.0000,\n",
      "           0.0000],\n",
      "         [ 0.0058,  1.3998,  0.2948, -0.2602, -0.0067, -0.0661,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0532,  1.5110, -0.4334,  0.3058,  0.0534,  0.0547,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1290,  0.0004,  0.0843, -0.2434]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.0843, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1290,  0.0003,  0.0841, -0.2433]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0841]])\n",
      " \n",
      "Q_target =  tensor([[-1.6113]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-1.6955]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.18621024,  1.5409789 , -0.47684494, -0.1402255 ,  0.13046923,\n",
      "        0.08134589,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.1330,  0.9093,  0.4525, -0.9146, -0.3220, -0.2733,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0090,  1.4301, -0.4625,  0.4139,  0.0119,  0.1347,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2295,  0.5086,  0.6040, -1.1043, -0.4574, -0.0332,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1770,  0.7187,  0.5165, -1.0016, -0.4112, -0.1237,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 3., 2., 2.]])\n",
      "Rewards:  tensor([[-1.7417,  1.2515,  2.1440,  0.9956]])\n",
      "Next states:  tensor([[[ 0.1374,  0.8882,  0.4525, -0.9413, -0.3357, -0.2732,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0135,  1.4389, -0.4510,  0.3876,  0.0164,  0.0885,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2358,  0.4844,  0.6378, -1.0769, -0.4594, -0.0386,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1824,  0.6965,  0.5503, -0.9848, -0.4172, -0.1190,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1292, -0.0007,  0.0817, -0.2436]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.2436, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1292, -0.0006,  0.0818, -0.2436]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0818]])\n",
      " \n",
      "Q_target =  tensor([[0.5633]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.8069]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.19084053,  1.5372448 , -0.46465355, -0.16609678,  0.1320767 ,\n",
      "        0.03214969,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0401,  1.4892, -0.4472,  0.3588,  0.0407,  0.0979,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0272,  1.3435,  0.3302, -0.3809, -0.0495, -0.1320,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1462,  0.8456,  0.4588, -0.9326, -0.3586, -0.2358,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0088,  1.3933,  0.3039, -0.2871, -0.0118, -0.1026,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 1., 2., 3.]])\n",
      "Rewards:  tensor([[ 0.3802, -0.8179,  1.0980, -2.4486]])\n",
      "Next states:  tensor([[[-0.0445,  1.4966, -0.4472,  0.3321,  0.0456,  0.0979,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0303,  1.3343,  0.3178, -0.4077, -0.0537, -0.0825,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1511,  0.8254,  0.5043, -0.9018, -0.3698, -0.2238,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0119,  1.3863,  0.3117, -0.3142, -0.0185, -0.1339,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1296, -0.0010,  0.0810, -0.2426]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.0010, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1296, -0.0010,  0.0809, -0.2426]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0809]])\n",
      " \n",
      "Q_target =  tensor([[-1.9562]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-1.9552]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.1955554 ,  1.532882  , -0.47529688, -0.19424482,  0.13587096,\n",
      "        0.07588564,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0495,  1.2666,  0.3279, -0.5692, -0.0871, -0.1231,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1511,  0.8254,  0.5043, -0.9018, -0.3698, -0.2238,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0272,  1.3435,  0.3302, -0.3809, -0.0495, -0.1320,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1678,  1.5518, -0.4639, -0.0611,  0.1148,  0.0904,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 2., 1., 0.]])\n",
      "Rewards:  tensor([[-2.2790, -0.9141, -0.8179, -0.7291]])\n",
      "Next states:  tensor([[[ 0.0528,  1.2532,  0.3362, -0.5966, -0.0950, -0.1566,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1564,  0.8052,  0.5356, -0.9005, -0.3803, -0.2102,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0303,  1.3343,  0.3178, -0.4077, -0.0537, -0.0825,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1724,  1.5498, -0.4639, -0.0878,  0.1193,  0.0904,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1294, -0.0034,  0.0809, -0.2424]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.0809, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1294, -0.0033,  0.0810, -0.2424]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0810]])\n",
      " \n",
      "Q_target =  tensor([[-0.9027]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.9837]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.20033097,  1.5284405 , -0.481304  , -0.19774975,  0.13960451,\n",
      "        0.07467096,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.1085,  1.5587, -0.3895,  0.0887,  0.0809, -0.0714,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1241,  0.9498,  0.4516, -0.8607, -0.2928, -0.2686,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0495,  1.2666,  0.3279, -0.5692, -0.0871, -0.1231,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0850,  1.1263,  0.3656, -0.6658, -0.1650, -0.1767,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[1., 3., 3., 3.]])\n",
      "Rewards:  tensor([[-0.6860, -2.6953, -2.2790, -2.2943]])\n",
      "Next states:  tensor([[[-0.1125,  1.5600, -0.4010,  0.0604,  0.0797, -0.0243,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1286,  0.9298,  0.4618, -0.8895, -0.3084, -0.3121,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0528,  1.2532,  0.3362, -0.5966, -0.0950, -0.1566,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0886,  1.1107,  0.3731, -0.6931, -0.1754, -0.2068,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1294, -0.0035,  0.0797, -0.2425]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.0797, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1294, -0.0034,  0.0798, -0.2425]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0798]])\n",
      " \n",
      "Q_target =  tensor([[0.1151]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.0354]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.20505142,  1.523948  , -0.4762772 , -0.20007275,  0.14382604,\n",
      "        0.08443068,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.1862,  1.5410, -0.4768, -0.1402,  0.1305,  0.0813,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1462,  0.8456,  0.4588, -0.9326, -0.3586, -0.2358,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0850,  1.1263,  0.3656, -0.6658, -0.1650, -0.1767,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0209,  1.3615,  0.3142, -0.3942, -0.0358, -0.1437,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 2., 3., 0.]])\n",
      "Rewards:  tensor([[ 0.4823,  1.0980, -2.2943, -1.8901]])\n",
      "Next states:  tensor([[[-0.1908,  1.5372, -0.4647, -0.1661,  0.1321,  0.0321,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1511,  0.8254,  0.5043, -0.9018, -0.3698, -0.2238,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0886,  1.1107,  0.3731, -0.6931, -0.1754, -0.2068,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0240,  1.3520,  0.3142, -0.4209, -0.0429, -0.1436,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1294, -0.0034,  0.0797, -0.2424]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1294, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1293, -0.0034,  0.0798, -0.2424]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0798]])\n",
      " \n",
      "Q_target =  tensor([[-0.9925]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.8632]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.20977187,  1.5188556 , -0.47627687, -0.22674087,  0.14804757,\n",
      "        0.08443056,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.1326,  1.5615, -0.4017, -0.0178,  0.0817,  0.0238,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2003,  1.5284, -0.4813, -0.1977,  0.1396,  0.0747,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0090,  1.4301, -0.4625,  0.4139,  0.0119,  0.1347,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1585,  1.5540, -0.4633, -0.0075,  0.1036,  0.0879,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 2., 3., 1.]])\n",
      "Rewards:  tensor([[-0.2597,  0.0361,  1.2515, -1.9491]])\n",
      "Next states:  tensor([[[-0.1366,  1.5605, -0.4017, -0.0444,  0.0829,  0.0238,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2051,  1.5239, -0.4763, -0.2001,  0.1438,  0.0844,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0135,  1.4389, -0.4510,  0.3876,  0.0164,  0.0885,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1632,  1.5532, -0.4749, -0.0351,  0.1103,  0.1345,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1321, -0.0040,  0.0786, -0.2418]], grad_fn=<AddmmBackward0>)\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q_current:  tensor(0.0786, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1321, -0.0040,  0.0786, -0.2417]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0786]])\n",
      " \n",
      "Q_target =  tensor([[-1.7514]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-1.8300]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.21473908,  1.5140941 , -0.5002417 , -0.21197112,  0.15156315,\n",
      "        0.07031144,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0814,  1.1412,  0.3757, -0.6397, -0.1562, -0.2175,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1632,  1.5532, -0.4749, -0.0351,  0.1103,  0.1345,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1462,  0.8456,  0.4588, -0.9326, -0.3586, -0.2358,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3274,  0.1462,  0.7661, -1.2178, -0.4563,  0.0354,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[1., 3., 2., 2.]])\n",
      "Rewards:  tensor([[-1.2197,  0.4286,  1.0980, -0.8658]])\n",
      "Next states:  tensor([[[ 0.0850,  1.1263,  0.3656, -0.6658, -0.1650, -0.1767,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1678,  1.5518, -0.4639, -0.0611,  0.1148,  0.0904,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1511,  0.8254,  0.5043, -0.9018, -0.3698, -0.2238,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3352,  0.1187,  0.7794, -1.2215, -0.4547,  0.0322,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1303, -0.0033,  0.0777, -0.2424]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1303, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1303, -0.0032,  0.0777, -0.2424]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0777]])\n",
      " \n",
      "Q_target =  tensor([[-0.9095]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.7792]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.21970625,  1.508733  , -0.50024146, -0.23863883,  0.15507872,\n",
      "        0.07031139,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.2755,  0.3318,  0.6698, -1.1765, -0.4671,  0.0018,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1719,  0.7412,  0.5165, -0.9750, -0.4051, -0.1237,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1207,  1.5609, -0.4085,  0.0063,  0.0803,  0.0059,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1862,  1.5410, -0.4768, -0.1402,  0.1305,  0.0813,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[2., 0., 2., 3.]])\n",
      "Rewards:  tensor([[ 0.6161, -0.9127,  0.1064,  0.4823]])\n",
      "Next states:  tensor([[[ 2.8243e-01,  3.0547e-01,  6.9423e-01, -1.1693e+00, -4.6705e-01,\n",
      "           8.6306e-04,  0.0000e+00,  0.0000e+00],\n",
      "         [ 1.7700e-01,  7.1865e-01,  5.1653e-01, -1.0016e+00, -4.1125e-01,\n",
      "          -1.2375e-01,  0.0000e+00,  0.0000e+00],\n",
      "         [-1.2470e-01,  1.5617e+00, -4.0074e-01,  3.5875e-02,  8.1314e-02,\n",
      "           1.9945e-02,  0.0000e+00,  0.0000e+00],\n",
      "         [-1.9084e-01,  1.5372e+00, -4.6465e-01, -1.6610e-01,  1.3208e-01,\n",
      "           3.2150e-02,  0.0000e+00,  0.0000e+00]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1317, -0.0033,  0.0774, -0.2420]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1317, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1316, -0.0033,  0.0775, -0.2420]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0775]])\n",
      " \n",
      "Q_target =  tensor([[-0.9570]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.8253]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.22467351,  1.5027719 , -0.5002412 , -0.26530653,  0.1585943 ,\n",
      "        0.07031134,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.1247,  1.5617, -0.4007,  0.0359,  0.0813,  0.0199,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0709,  1.5325, -0.4489,  0.1995,  0.0769,  0.1162,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1824,  0.6965,  0.5503, -0.9848, -0.4172, -0.1190,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3043,  0.2266,  0.7648, -1.1369, -0.4599,  0.0432,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 0., 1., 0.]])\n",
      "Rewards:  tensor([[ 1.0232,  0.0286, -0.1090, -1.1432]])\n",
      "Next states:  tensor([[[-0.1286,  1.5619, -0.3919,  0.0098,  0.0805, -0.0156,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0754,  1.5364, -0.4489,  0.1728,  0.0827,  0.1162,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1878,  0.6739,  0.5421, -1.0090, -0.4213, -0.0812,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3120,  0.2004,  0.7648, -1.1635, -0.4577,  0.0432,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1329, -0.0036,  0.0774, -0.2424]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1329, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1329, -0.0036,  0.0774, -0.2424]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0774]])\n",
      " \n",
      "Q_target =  tensor([[-0.9980]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.8651]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.22964072,  1.4962108 , -0.5002409 , -0.2919742 ,  0.16210987,\n",
      "        0.07031129,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0148,  1.3786,  0.3035, -0.3403, -0.0235, -0.1008,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0596,  1.2266,  0.3452, -0.6047, -0.1081, -0.1167,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0463,  1.2794,  0.3392, -0.5431, -0.0810, -0.1687,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1719,  0.7412,  0.5165, -0.9750, -0.4051, -0.1237,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 2., 1., 0.]])\n",
      "Rewards:  tensor([[-1.7064, -0.4397, -1.0409, -0.9127]])\n",
      "Next states:  tensor([[[ 0.0178,  1.3704,  0.3035, -0.3670, -0.0286, -0.1008,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0632,  1.2130,  0.3640, -0.6049, -0.1133, -0.1026,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0495,  1.2666,  0.3279, -0.5692, -0.0871, -0.1231,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1770,  0.7187,  0.5165, -1.0016, -0.4112, -0.1237,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1336, -0.0034,  0.0772, -0.2426]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1336, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1336, -0.0034,  0.0772, -0.2426]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0772]])\n",
      " \n",
      "Q_target =  tensor([[-1.0328]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.8992]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.23460798,  1.4890501 , -0.5002407 , -0.31864187,  0.16562542,\n",
      "        0.07031123,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.1862,  1.5410, -0.4768, -0.1402,  0.1305,  0.0813,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2247,  1.5028, -0.5002, -0.2653,  0.1586,  0.0703,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1125,  1.5600, -0.4010,  0.0604,  0.0797, -0.0243,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2295,  0.5086,  0.6040, -1.1043, -0.4574, -0.0332,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 0., 1., 2.]])\n",
      "Rewards:  tensor([[ 0.4823, -1.0747, -0.5866,  2.1440]])\n",
      "Next states:  tensor([[[-0.1908,  1.5372, -0.4647, -0.1661,  0.1321,  0.0321,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2296,  1.4962, -0.5002, -0.2920,  0.1621,  0.0703,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1166,  1.5608, -0.4085,  0.0329,  0.0800,  0.0059,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2358,  0.4844,  0.6378, -1.0769, -0.4594, -0.0386,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1359, -0.0037,  0.0766, -0.2418]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1359, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1359, -0.0037,  0.0766, -0.2418]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0766]])\n",
      " \n",
      "Q_target =  tensor([[-1.0620]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.9260]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.2395753 ,  1.4812896 , -0.50024045, -0.3453096 ,  0.16914098,\n",
      "        0.07031117,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.1908,  1.5372, -0.4647, -0.1661,  0.1321,  0.0321,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0489,  1.5041, -0.4451,  0.3319,  0.0507,  0.1018,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2098,  1.5189, -0.4763, -0.2267,  0.1480,  0.0844,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3043,  0.2266,  0.7648, -1.1369, -0.4599,  0.0432,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[1., 3., 2., 0.]])\n",
      "Rewards:  tensor([[-2.0363,  1.4733, -1.8292, -1.1432]])\n",
      "Next states:  tensor([[[-0.1956,  1.5329, -0.4753, -0.1942,  0.1359,  0.0759,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0532,  1.5110, -0.4334,  0.3058,  0.0534,  0.0547,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2147,  1.5141, -0.5002, -0.2120,  0.1516,  0.0703,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3120,  0.2004,  0.7648, -1.1635, -0.4577,  0.0432,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1372, -0.0038,  0.0767, -0.2419]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.2419, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1372, -0.0038,  0.0767, -0.2419]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0767]])\n",
      " \n",
      "Q_target =  tensor([[-0.1335]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.1084]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.24446702,  1.472937  , -0.49078554, -0.37141144,  0.1707543 ,\n",
      "        0.0322663 ,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0058,  1.3998,  0.2948, -0.2602, -0.0067, -0.0661,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0463,  1.2794,  0.3392, -0.5431, -0.0810, -0.1687,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0267,  1.4645, -0.4371,  0.3752,  0.0287,  0.0622,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0430,  1.2916,  0.3298, -0.5154, -0.0726, -0.1306,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 1., 2., 3.]])\n",
      "Rewards:  tensor([[-2.3868, -1.0409, -2.6781, -2.5061]])\n",
      "Next states:  tensor([[[ 0.0088,  1.3933,  0.3039, -0.2871, -0.0118, -0.1026,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0495,  1.2666,  0.3279, -0.5692, -0.0871, -0.1231,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0312,  1.4731, -0.4466,  0.3830,  0.0314,  0.0541,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0463,  1.2794,  0.3392, -0.5431, -0.0810, -0.1687,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1369, -0.0038,  0.0767, -0.2423]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.2423, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1368, -0.0038,  0.0767, -0.2423]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0767]])\n",
      " \n",
      "Q_target =  tensor([[0.1114]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.3536]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.24927835,  1.4640075 , -0.48066917, -0.39681238,  0.1702898 ,\n",
      "       -0.00928999,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.2396,  1.4813, -0.5002, -0.3453,  0.1691,  0.0703,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0058,  1.3998,  0.2948, -0.2602, -0.0067, -0.0661,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0398,  1.3032,  0.3178, -0.4877, -0.0660, -0.0824,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1417,  0.8665,  0.4411, -0.9651, -0.3468, -0.2227,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 3., 3., 2.]])\n",
      "Rewards:  tensor([[-0.2095, -2.3868, -2.5052,  2.6854]])\n",
      "Next states:  tensor([[[-0.2445,  1.4729, -0.4908, -0.3714,  0.1708,  0.0323,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0088,  1.3933,  0.3039, -0.2871, -0.0118, -0.1026,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0430,  1.2916,  0.3298, -0.5154, -0.0726, -0.1306,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1462,  0.8456,  0.4588, -0.9326, -0.3586, -0.2358,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1365, -0.0035,  0.0768, -0.2416]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.0035, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1364, -0.0035,  0.0769, -0.2417]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0769]])\n",
      " \n",
      "Q_target =  tensor([[-1.8637]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-1.8602]], grad_fn=<SubBackward0>)\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current state:  (array([-0.2541729 ,  1.4544559 , -0.49112764, -0.42471725,  0.17196877,\n",
      "        0.03357949,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0966,  1.5538, -0.4047,  0.1031,  0.0930, -0.0470,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3274,  0.1462,  0.7661, -1.2178, -0.4563,  0.0354,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0495,  1.2666,  0.3279, -0.5692, -0.0871, -0.1231,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0178,  1.3704,  0.3035, -0.3670, -0.0286, -0.1008,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 2., 3., 3.]])\n",
      "Rewards:  tensor([[ 1.6848, -0.8658, -2.2790, -2.6538]])\n",
      "Next states:  tensor([[[-0.1006,  1.5555, -0.3952,  0.0771,  0.0888, -0.0850,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3352,  0.1187,  0.7794, -1.2215, -0.4547,  0.0322,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0528,  1.2532,  0.3362, -0.5966, -0.0950, -0.1566,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0209,  1.3615,  0.3142, -0.3942, -0.0358, -0.1437,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1360, -0.0059,  0.0774, -0.2417]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1360, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1360, -0.0058,  0.0775, -0.2418]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0775]])\n",
      " \n",
      "Q_target =  tensor([[-0.9517]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.8157]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.25906745,  1.4443039 , -0.49112758, -0.45138413,  0.17364776,\n",
      "        0.03357953,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.1286,  0.9298,  0.4618, -0.8895, -0.3084, -0.3121,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0180,  1.4470, -0.4510,  0.3609,  0.0208,  0.0885,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2234,  0.5335,  0.6145, -1.0814, -0.4558, -0.0838,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0532,  1.5110, -0.4334,  0.3058,  0.0534,  0.0547,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[1., 2., 1., 1.]])\n",
      "Rewards:  tensor([[-1.2496, -3.9302,  0.3562, -0.6523]])\n",
      "Next states:  tensor([[[ 0.1330,  0.9093,  0.4525, -0.9146, -0.3220, -0.2733,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0224,  1.4560, -0.4455,  0.4020,  0.0256,  0.0961,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2295,  0.5086,  0.6040, -1.1043, -0.4574, -0.0332,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0576,  1.5173, -0.4450,  0.2788,  0.0585,  0.1013,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1378, -0.0061,  0.0774, -0.2416]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.2416, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1378, -0.0061,  0.0773, -0.2416]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0773]])\n",
      " \n",
      "Q_target =  tensor([[0.0847]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.3263]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.2638784 ,  1.4335793 , -0.48060924, -0.47659197,  0.1731566 ,\n",
      "       -0.00982298,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.2591,  1.4443, -0.4911, -0.4514,  0.1736,  0.0336,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0398,  1.3032,  0.3178, -0.4877, -0.0660, -0.0824,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0178,  1.3704,  0.3035, -0.3670, -0.0286, -0.1008,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0445,  1.4966, -0.4472,  0.3321,  0.0456,  0.0979,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 3., 3., 2.]])\n",
      "Rewards:  tensor([[ 0.0082, -2.5052, -2.6538, -1.3887]])\n",
      "Next states:  tensor([[[-0.2639,  1.4336, -0.4806, -0.4766,  0.1732, -0.0098,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0430,  1.2916,  0.3298, -0.5154, -0.0726, -0.1306,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0209,  1.3615,  0.3142, -0.3942, -0.0358, -0.1437,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0489,  1.5041, -0.4451,  0.3319,  0.0507,  0.1018,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1380, -0.0061,  0.0764, -0.2410]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.0764, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1379, -0.0061,  0.0765, -0.2410]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0765]])\n",
      " \n",
      "Q_target =  tensor([[2.6290]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[2.5526]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.26867285,  1.423474  , -0.47967306, -0.44916275,  0.17338821,\n",
      "        0.00463216,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0178,  1.3704,  0.3035, -0.3670, -0.0286, -0.1008,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0463,  1.2794,  0.3392, -0.5431, -0.0810, -0.1687,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0709,  1.5325, -0.4489,  0.1995,  0.0769,  0.1162,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0148,  1.3786,  0.3035, -0.3403, -0.0235, -0.1008,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 1., 0., 0.]])\n",
      "Rewards:  tensor([[-2.6538, -1.0409,  0.0286, -1.7064]])\n",
      "Next states:  tensor([[[ 0.0209,  1.3615,  0.3142, -0.3942, -0.0358, -0.1437,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0495,  1.2666,  0.3279, -0.5692, -0.0871, -0.1231,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0754,  1.5364, -0.4489,  0.1728,  0.0827,  0.1162,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0178,  1.3704,  0.3035, -0.3670, -0.0286, -0.1008,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1379, -0.0062,  0.0801, -0.2412]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.2412, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1379, -0.0061,  0.0801, -0.2412]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0801]])\n",
      " \n",
      "Q_target =  tensor([[0.2855]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.5267]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.27337593,  1.4127947 , -0.46818653, -0.47438487,  0.17125975,\n",
      "       -0.0425694 ,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0925,  1.5515, -0.4146,  0.1286,  0.0954, -0.0063,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2147,  1.5141, -0.5002, -0.2120,  0.1516,  0.0703,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1815,  1.5441, -0.4635, -0.1413,  0.1264,  0.0891,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1678,  1.5518, -0.4639, -0.0611,  0.1148,  0.0904,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 0., 2., 0.]])\n",
      "Rewards:  tensor([[ 1.6058, -0.9864, -1.6945, -0.7291]])\n",
      "Next states:  tensor([[[-0.0966,  1.5538, -0.4047,  0.1031,  0.0930, -0.0470,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2197,  1.5087, -0.5002, -0.2386,  0.1551,  0.0703,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1862,  1.5410, -0.4768, -0.1402,  0.1305,  0.0813,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1724,  1.5498, -0.4639, -0.0878,  0.1193,  0.0904,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1389, -0.0064,  0.0795, -0.2397]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.0064, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1389, -0.0064,  0.0796, -0.2397]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0796]])\n",
      " \n",
      "Q_target =  tensor([[-1.5854]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-1.5790]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.2781577 ,  1.4015013 , -0.4780503 , -0.50191367,  0.17113318,\n",
      "       -0.00253154,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.2621,  0.3841,  0.6786, -1.1263, -0.4673, -0.0404,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2688,  0.3583,  0.6698, -1.1498, -0.4672,  0.0018,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0706,  1.1839,  0.3756, -0.6590, -0.1282, -0.1491,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0576,  1.5173, -0.4450,  0.2788,  0.0585,  0.1013,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[1., 0., 2., 0.]])\n",
      "Rewards:  tensor([[ 0.1163, -0.6386,  1.6666,  0.2740]])\n",
      "Next states:  tensor([[[ 0.2688,  0.3583,  0.6698, -1.1498, -0.4672,  0.0018,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2755,  0.3318,  0.6698, -1.1765, -0.4671,  0.0018,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0742,  1.1694,  0.3667, -0.6484, -0.1364, -0.1647,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0620,  1.5230, -0.4450,  0.2521,  0.0636,  0.1012,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1369, -0.0076,  0.0813, -0.2404]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1369, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1368, -0.0076,  0.0813, -0.2404]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0813]])\n",
      " \n",
      "Q_target =  tensor([[-0.7898]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.6529]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.28293943,  1.389608  , -0.4780503 , -0.52858037,  0.17100658,\n",
      "       -0.00253159,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0445,  1.4966, -0.4472,  0.3321,  0.0456,  0.0979,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2051,  1.5239, -0.4763, -0.2001,  0.1438,  0.0844,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2966,  0.2522,  0.7322, -1.1715, -0.4621,  0.0547,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0998,  1.0604,  0.3919, -0.7751, -0.2146, -0.2840,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[2., 0., 2., 1.]])\n",
      "Rewards:  tensor([[-1.3887, -1.0715,  2.0442, -1.4533]])\n",
      "Next states:  tensor([[[-0.0489,  1.5041, -0.4451,  0.3319,  0.0507,  0.1018,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2098,  1.5189, -0.4763, -0.2267,  0.1480,  0.0844,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3043,  0.2266,  0.7648, -1.1369, -0.4599,  0.0432,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1035,  1.0424,  0.3834, -0.8009, -0.2271, -0.2490,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1385, -0.0081,  0.0807, -0.2403]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.0807, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1385, -0.0080,  0.0807, -0.2403]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0807]])\n",
      " \n",
      "Q_target =  tensor([[0.9517]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.8710]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-2.8777999e-01,  1.3778758e+00, -4.8406106e-01, -5.2143121e-01,\n",
      "        1.7101111e-01,  9.0600952e-05,  0.0000000e+00,  0.0000000e+00],\n",
      "      dtype=float32), {})\n",
      " \n",
      "Experiences: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States:  tensor([[[-0.0445,  1.4966, -0.4472,  0.3321,  0.0456,  0.0979,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2621,  0.3841,  0.6786, -1.1263, -0.4673, -0.0404,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0798,  1.5411, -0.4472,  0.2064,  0.0890,  0.1253,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1815,  1.5441, -0.4635, -0.1413,  0.1264,  0.0891,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[2., 1., 3., 2.]])\n",
      "Rewards:  tensor([[-1.3887,  0.1163,  0.9149, -1.6945]])\n",
      "Next states:  tensor([[[-0.0489,  1.5041, -0.4451,  0.3319,  0.0507,  0.1018,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2688,  0.3583,  0.6698, -1.1498, -0.4672,  0.0018,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0841,  1.5451, -0.4388,  0.1798,  0.0935,  0.0918,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1862,  1.5410, -0.4768, -0.1402,  0.1305,  0.0813,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1391, -0.0083,  0.0815, -0.2401]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.0815, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1391, -0.0083,  0.0815, -0.2401]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0815]])\n",
      " \n",
      "Q_target =  tensor([[0.2170]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.1355]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.2928616 ,  1.3664517 , -0.5075407 , -0.5076679 ,  0.17039308,\n",
      "       -0.01236054,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.1286,  0.9298,  0.4618, -0.8895, -0.3084, -0.3121,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0665,  1.5280, -0.4573,  0.2254,  0.0711,  0.1501,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1908,  1.5372, -0.4647, -0.1661,  0.1321,  0.0321,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2591,  1.4443, -0.4911, -0.4514,  0.1736,  0.0336,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[1., 3., 1., 3.]])\n",
      "Rewards:  tensor([[-1.2496,  0.7751, -2.0363,  0.0082]])\n",
      "Next states:  tensor([[[ 0.1330,  0.9093,  0.4525, -0.9146, -0.3220, -0.2733,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0709,  1.5325, -0.4489,  0.1995,  0.0769,  0.1162,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1956,  1.5329, -0.4753, -0.1942,  0.1359,  0.0759,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2639,  1.4336, -0.4806, -0.4766,  0.1732, -0.0098,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1393, -0.0082,  0.0814, -0.2398]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.0814, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1393, -0.0082,  0.0814, -0.2398]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0814]])\n",
      " \n",
      "Q_target =  tensor([[0.7043]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.6229]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.29819328,  1.3555558 , -0.53199875, -0.48413068,  0.16922434,\n",
      "       -0.02337493,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0058,  1.3998,  0.2948, -0.2602, -0.0067, -0.0661,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0814,  1.1412,  0.3757, -0.6397, -0.1562, -0.2175,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3743, -0.0262,  0.7647, -1.2805, -0.4304,  0.3524,  1.0000,\n",
      "           0.0000],\n",
      "         [ 0.3274,  0.1462,  0.7661, -1.2178, -0.4563,  0.0354,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 1., 3., 2.]])\n",
      "Rewards:  tensor([[  -2.3868,   -1.2197, -100.0000,   -0.8658]])\n",
      "Next states:  tensor([[[ 0.0088,  1.3933,  0.3039, -0.2871, -0.0118, -0.1026,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0850,  1.1263,  0.3656, -0.6658, -0.1650, -0.1767,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3832, -0.0538,  0.8811, -0.8416, -0.3617,  4.2807,  1.0000,\n",
      "           0.0000],\n",
      "         [ 0.3352,  0.1187,  0.7794, -1.2215, -0.4547,  0.0322,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1372, -0.0066,  0.0844, -0.2405]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1372, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1379, -0.0073,  0.0825, -0.2404]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0825]])\n",
      " \n",
      "Q_target =  tensor([[-0.6166]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.4793]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.30352488,  1.3440598 , -0.53199875, -0.5107975 ,  0.1680556 ,\n",
      "       -0.02337493,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.1207,  1.5609, -0.4085,  0.0063,  0.0803,  0.0059,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0528,  1.2532,  0.3362, -0.5966, -0.0950, -0.1566,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2098,  1.5189, -0.4763, -0.2267,  0.1480,  0.0844,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0884,  1.5486, -0.4266,  0.1541,  0.0957,  0.0424,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[2., 2., 2., 3.]])\n",
      "Rewards:  tensor([[ 0.1064,  1.0187, -1.8292,  1.6346]])\n",
      "Next states:  tensor([[[-0.1247,  1.5617, -0.4007,  0.0359,  0.0813,  0.0199,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0562,  1.2402,  0.3527, -0.5780, -0.1023, -0.1467,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2147,  1.5141, -0.5002, -0.2120,  0.1516,  0.0703,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0925,  1.5515, -0.4146,  0.1286,  0.0954, -0.0063,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1400, -0.0083,  0.0820, -0.2399]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.2399, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1400, -0.0083,  0.0820, -0.2400]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0820]])\n",
      " \n",
      "Q_target =  tensor([[0.5328]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.7728]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.30875787,  1.3319769 , -0.51963985, -0.53661084,  0.16439292,\n",
      "       -0.07325386,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.1449,  1.5567, -0.4218, -0.0982,  0.0910,  0.1045,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1374,  0.8882,  0.4525, -0.9413, -0.3357, -0.2732,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2929,  1.3665, -0.5075, -0.5077,  0.1704, -0.0124,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2966,  0.2522,  0.7322, -1.1715, -0.4621,  0.0547,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[2., 1., 2., 2.]])\n",
      "Rewards:  tensor([[-2.4255, -0.7396,  0.6237,  2.0442]])\n",
      "Next states:  tensor([[[-0.1493,  1.5552, -0.4456, -0.0699,  0.0954,  0.0887,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1417,  0.8665,  0.4411, -0.9651, -0.3468, -0.2227,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2982,  1.3556, -0.5320, -0.4841,  0.1692, -0.0234,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3043,  0.2266,  0.7648, -1.1369, -0.4599,  0.0432,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1391, -0.0077,  0.0830, -0.2390]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.0077, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1391, -0.0077,  0.0830, -0.2389]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0830]])\n",
      " \n",
      "Q_target =  tensor([[-1.4445]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-1.4367]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.31407508,  1.3192685 , -0.53023744, -0.5646496 ,  0.16290884,\n",
      "       -0.0296813 ,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.1085,  1.5587, -0.3895,  0.0887,  0.0809, -0.0714,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2422,  0.4596,  0.6378, -1.1035, -0.4613, -0.0386,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1724,  1.5498, -0.4639, -0.0878,  0.1193,  0.0904,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1286,  0.9298,  0.4618, -0.8895, -0.3084, -0.3121,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[1., 0., 3., 1.]])\n",
      "Rewards:  tensor([[-0.6860, -0.5871,  0.2713, -1.2496]])\n",
      "Next states:  tensor([[[-0.1125,  1.5600, -0.4010,  0.0604,  0.0797, -0.0243,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2486,  0.4342,  0.6378, -1.1302, -0.4632, -0.0386,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1769,  1.5473, -0.4546, -0.1134,  0.1219,  0.0525,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1330,  0.9093,  0.4525, -0.9146, -0.3220, -0.2733,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1392, -0.0099,  0.0828, -0.2391]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.2391, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1392, -0.0099,  0.0829, -0.2391]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0829]])\n",
      " \n",
      "Q_target =  tensor([[0.1412]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.3804]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.31932712,  1.3059654 , -0.52208626, -0.5909118 ,  0.15978937,\n",
      "       -0.06238938,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.2197,  1.5087, -0.5002, -0.2386,  0.1551,  0.0703,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0312,  1.4731, -0.4466,  0.3830,  0.0314,  0.0541,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2639,  1.4336, -0.4806, -0.4766,  0.1732, -0.0098,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2051,  1.5239, -0.4763, -0.2001,  0.1438,  0.0844,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 1., 2., 0.]])\n",
      "Rewards:  tensor([[-1.0337, -0.2099,  2.5533, -1.0715]])\n",
      "Next states:  tensor([[[-0.2247,  1.5028, -0.5002, -0.2653,  0.1586,  0.0703,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0357,  1.4811, -0.4552,  0.3557,  0.0358,  0.0884,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2687,  1.4235, -0.4797, -0.4492,  0.1734,  0.0046,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2098,  1.5189, -0.4763, -0.2267,  0.1480,  0.0844,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1404, -0.0103,  0.0818, -0.2380]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.0818, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1404, -0.0103,  0.0818, -0.2380]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0818]])\n",
      " \n",
      "Q_target =  tensor([[1.9573]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[1.8755]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.32487193,  1.2934875 , -0.55072105, -0.5541755 ,  0.1560315 ,\n",
      "       -0.07515772,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0706,  1.1839,  0.3756, -0.6590, -0.1282, -0.1491,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1539,  1.5541, -0.4671, -0.0464,  0.0992,  0.0749,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2688,  0.3583,  0.6698, -1.1498, -0.4672,  0.0018,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2112,  0.5815,  0.6219, -1.0301, -0.4458, -0.1169,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[2., 2., 0., 0.]])\n",
      "Rewards:  tensor([[ 1.6666, -0.1696, -0.6386, -0.8707]])\n",
      "Next states:  tensor([[[ 0.0742,  1.1694,  0.3667, -0.6484, -0.1364, -0.1647,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1585,  1.5540, -0.4633, -0.0075,  0.1036,  0.0879,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2755,  0.3318,  0.6698, -1.1765, -0.4671,  0.0018,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2173,  0.5578,  0.6219, -1.0567, -0.4516, -0.1169,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1387, -0.0095,  0.0858, -0.2388]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.0095, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1387, -0.0095,  0.0858, -0.2388]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0858]])\n",
      " \n",
      "Q_target =  tensor([[-1.0974]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-1.0879]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.3304801 ,  1.2804022 , -0.55865276, -0.5813499 ,  0.15387245,\n",
      "       -0.04318091,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.1908,  1.5372, -0.4647, -0.1661,  0.1321,  0.0321,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1769,  1.5473, -0.4546, -0.1134,  0.1219,  0.0525,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2112,  0.5815,  0.6219, -1.0301, -0.4458, -0.1169,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0709,  1.5325, -0.4489,  0.1995,  0.0769,  0.1162,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[1., 1., 0., 0.]])\n",
      "Rewards:  tensor([[-2.0363, -1.8185, -0.8707,  0.0286]])\n",
      "Next states:  tensor([[[-0.1956,  1.5329, -0.4753, -0.1942,  0.1359,  0.0759,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1815,  1.5441, -0.4635, -0.1413,  0.1264,  0.0891,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2173,  0.5578,  0.6219, -1.0567, -0.4516, -0.1169,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0754,  1.5364, -0.4489,  0.1728,  0.0827,  0.1162,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1397, -0.0114,  0.0849, -0.2384]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.0114, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1396, -0.0114,  0.0848, -0.2383]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0848]])\n",
      " \n",
      "Q_target =  tensor([[-1.5935]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-1.5821]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.33617854,  1.2666923 , -0.5700099 , -0.6093441 ,  0.15403792,\n",
      "        0.00330932,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.2234,  0.5335,  0.6145, -1.0814, -0.4558, -0.0838,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0532,  1.5110, -0.4334,  0.3058,  0.0534,  0.0547,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3193,  1.3060, -0.5221, -0.5909,  0.1598, -0.0624,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2553,  0.4095,  0.6786, -1.0996, -0.4653, -0.0404,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[1., 1., 2., 0.]])\n",
      "Rewards:  tensor([[ 0.3562, -0.6523,  1.8764, -0.7271]])\n",
      "Next states:  tensor([[[ 0.2295,  0.5086,  0.6040, -1.1043, -0.4574, -0.0332,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0576,  1.5173, -0.4450,  0.2788,  0.0585,  0.1013,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3249,  1.2935, -0.5507, -0.5542,  0.1560, -0.0752,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2621,  0.3841,  0.6786, -1.1263, -0.4673, -0.0404,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1392, -0.0132,  0.0855, -0.2384]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.0855, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1392, -0.0132,  0.0855, -0.2383]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0855]])\n",
      " \n",
      "Q_target =  tensor([[1.5157]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[1.4302]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.34208098,  1.2536103 , -0.59005964, -0.5813964 ,  0.15385121,\n",
      "       -0.00373428,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.3197,  0.1736,  0.7751, -1.1938, -0.4580, -0.0059,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2929,  1.3665, -0.5075, -0.5077,  0.1704, -0.0124,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0312,  1.4731, -0.4466,  0.3830,  0.0314,  0.0541,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1006,  1.5555, -0.3952,  0.0771,  0.0888, -0.0850,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[1., 2., 1., 0.]])\n",
      "Rewards:  tensor([[-0.8637,  0.6237, -0.2099,  0.7116]])\n",
      "Next states:  tensor([[[ 0.3274,  0.1462,  0.7661, -1.2178, -0.4563,  0.0354,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2982,  1.3556, -0.5320, -0.4841,  0.1692, -0.0234,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0357,  1.4811, -0.4552,  0.3557,  0.0358,  0.0884,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1046,  1.5567, -0.3952,  0.0504,  0.0845, -0.0849,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1395, -0.0133,  0.0870, -0.2382]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.2382, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1395, -0.0132,  0.0870, -0.2381]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0870]])\n",
      " \n",
      "Q_target =  tensor([[0.1767]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.4149]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.3479182 ,  1.2399536 , -0.581833  , -0.6067727 ,  0.1519616 ,\n",
      "       -0.03779225,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.2687,  1.4235, -0.4797, -0.4492,  0.1734,  0.0046,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0798,  1.5411, -0.4472,  0.2064,  0.0890,  0.1253,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0562,  1.2402,  0.3527, -0.5780, -0.1023, -0.1467,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1407,  1.5589, -0.4099, -0.0713,  0.0858,  0.0568,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 3., 1., 1.]])\n",
      "Rewards:  tensor([[ 0.2062,  0.9149, -1.1889, -2.0741]])\n",
      "Next states:  tensor([[[-0.2734,  1.4128, -0.4682, -0.4744,  0.1713, -0.0426,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0841,  1.5451, -0.4388,  0.1798,  0.0935,  0.0918,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0596,  1.2266,  0.3452, -0.6047, -0.1081, -0.1167,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1449,  1.5567, -0.4218, -0.0982,  0.0910,  0.1045,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1399, -0.0136,  0.0864, -0.2377]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.0864, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1399, -0.0136,  0.0864, -0.2377]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0864]])\n",
      " \n",
      "Q_target =  tensor([[1.7390]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[1.6527]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.35406375,  1.2271876 , -0.6119447 , -0.567113  ,  0.1493575 ,\n",
      "       -0.05208178,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.1115,  1.0070,  0.4130, -0.8025, -0.2509, -0.2379,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0312,  1.4731, -0.4466,  0.3830,  0.0314,  0.0541,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3197,  0.1736,  0.7751, -1.1938, -0.4580, -0.0059,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1449,  1.5567, -0.4218, -0.0982,  0.0910,  0.1045,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 1., 1., 2.]])\n",
      "Rewards:  tensor([[-2.6805, -0.2099, -0.8637, -2.4255]])\n",
      "Next states:  tensor([[[ 0.1156,  0.9883,  0.4243, -0.8308, -0.2651, -0.2847,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0357,  1.4811, -0.4552,  0.3557,  0.0358,  0.0884,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3274,  0.1462,  0.7661, -1.2178, -0.4563,  0.0354,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1493,  1.5552, -0.4456, -0.0699,  0.0954,  0.0887,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1391, -0.0132,  0.0898, -0.2381]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.2381, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1391, -0.0132,  0.0898, -0.2381]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0898]])\n",
      " \n",
      "Q_target =  tensor([[0.6673]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.9054]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.3601257 ,  1.21385   , -0.6014115 , -0.5923052 ,  0.14458443,\n",
      "       -0.09546123,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.3088,  1.3320, -0.5196, -0.5366,  0.1644, -0.0733,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0357,  1.4811, -0.4552,  0.3557,  0.0358,  0.0884,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2493,  1.4640, -0.4807, -0.3968,  0.1703, -0.0093,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3141,  1.3193, -0.5302, -0.5646,  0.1629, -0.0297,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[1., 2., 1., 3.]])\n",
      "Rewards:  tensor([[-1.5266, -1.1719, -1.9399,  0.0592]])\n",
      "Next states:  tensor([[[-0.3141,  1.3193, -0.5302, -0.5646,  0.1629, -0.0297,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0401,  1.4892, -0.4472,  0.3588,  0.0407,  0.0979,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2542,  1.4545, -0.4911, -0.4247,  0.1720,  0.0336,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3193,  1.3060, -0.5221, -0.5909,  0.1598, -0.0624,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1403, -0.0134,  0.0887, -0.2360]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1403, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1402, -0.0134,  0.0886, -0.2359]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0886]])\n",
      " \n",
      "Q_target =  tensor([[-0.1672]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.0270]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.36618766,  1.1999129 , -0.6014111 , -0.61897373,  0.13981138,\n",
      "       -0.09546108,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.3601,  1.2139, -0.6014, -0.5923,  0.1446, -0.0955,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0706,  1.1839,  0.3756, -0.6590, -0.1282, -0.1491,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0489,  1.5041, -0.4451,  0.3319,  0.0507,  0.1018,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0240,  1.3520,  0.3142, -0.4209, -0.0429, -0.1436,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 2., 3., 2.]])\n",
      "Rewards:  tensor([[-0.2550,  1.6666,  1.4733,  2.0011]])\n",
      "Next states:  tensor([[[-0.3662,  1.1999, -0.6014, -0.6190,  0.1398, -0.0955,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0742,  1.1694,  0.3667, -0.6484, -0.1364, -0.1647,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0532,  1.5110, -0.4334,  0.3058,  0.0534,  0.0547,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0272,  1.3435,  0.3302, -0.3809, -0.0495, -0.1320,  0.0000,\n",
      "           0.0000]]])\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Q_current:  tensor([[-0.1396, -0.0133,  0.0890, -0.2366]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.0890, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1396, -0.0133,  0.0890, -0.2366]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0890]])\n",
      " \n",
      "Q_target =  tensor([[2.3884]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[2.2994]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.3724455 ,  1.1867422 , -0.6206783 , -0.58489424,  0.13472791,\n",
      "       -0.10166956,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.1539,  1.5541, -0.4671, -0.0464,  0.0992,  0.0749,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3088,  1.3320, -0.5196, -0.5366,  0.1644, -0.0733,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0463,  1.2794,  0.3392, -0.5431, -0.0810, -0.1687,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1862,  1.5410, -0.4768, -0.1402,  0.1305,  0.0813,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[2., 1., 1., 3.]])\n",
      "Rewards:  tensor([[-0.1696, -1.5266, -1.0409,  0.4823]])\n",
      "Next states:  tensor([[[-0.1585,  1.5540, -0.4633, -0.0075,  0.1036,  0.0879,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3141,  1.3193, -0.5302, -0.5646,  0.1629, -0.0297,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0495,  1.2666,  0.3279, -0.5692, -0.0871, -0.1231,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1908,  1.5372, -0.4647, -0.1661,  0.1321,  0.0321,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1399, -0.0133,  0.0919, -0.2363]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.0919, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1399, -0.0133,  0.0919, -0.2363]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0919]])\n",
      " \n",
      "Q_target =  tensor([[0.9335]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.8416]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.37873745,  1.1735145 , -0.62415856, -0.5874516 ,  0.12971257,\n",
      "       -0.10030721,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.2247,  1.5028, -0.5002, -0.2653,  0.1586,  0.0703,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1815,  1.5441, -0.4635, -0.1413,  0.1264,  0.0891,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1046,  1.5567, -0.3952,  0.0504,  0.0845, -0.0849,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3479,  1.2400, -0.5818, -0.6068,  0.1520, -0.0378,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 2., 2., 2.]])\n",
      "Rewards:  tensor([[-1.0747, -1.6945, -0.2680,  1.6535]])\n",
      "Next states:  tensor([[[-0.2296,  1.4962, -0.5002, -0.2920,  0.1621,  0.0703,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1862,  1.5410, -0.4768, -0.1402,  0.1305,  0.0813,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1085,  1.5587, -0.3895,  0.0887,  0.0809, -0.0714,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3541,  1.2272, -0.6119, -0.5671,  0.1494, -0.0521,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1403, -0.0135,  0.0928, -0.2360]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1403, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1404, -0.0134,  0.0928, -0.2360]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0928]])\n",
      " \n",
      "Q_target =  tensor([[-0.1375]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.0028]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.3850294 ,  1.1596872 , -0.62415814, -0.61412024,  0.12469722,\n",
      "       -0.10030706,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0966,  1.5538, -0.4047,  0.1031,  0.0930, -0.0470,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0925,  1.5515, -0.4146,  0.1286,  0.0954, -0.0063,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2051,  1.5239, -0.4763, -0.2001,  0.1438,  0.0844,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1539,  1.5541, -0.4671, -0.0464,  0.0992,  0.0749,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 3., 0., 2.]])\n",
      "Rewards:  tensor([[ 1.6848,  1.6058, -1.0715, -0.1696]])\n",
      "Next states:  tensor([[[-0.1006,  1.5555, -0.3952,  0.0771,  0.0888, -0.0850,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0966,  1.5538, -0.4047,  0.1031,  0.0930, -0.0470,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2098,  1.5189, -0.4763, -0.2267,  0.1480,  0.0844,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1585,  1.5540, -0.4633, -0.0075,  0.1036,  0.0879,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1406, -0.0138,  0.0928, -0.2363]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1406, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1405, -0.0137,  0.0928, -0.2363]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0928]])\n",
      " \n",
      "Q_target =  tensor([[-0.1308]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.0097]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.39132136,  1.1452602 , -0.6241578 , -0.640789  ,  0.11968187,\n",
      "       -0.10030691,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0303,  1.3343,  0.3178, -0.4077, -0.0537, -0.0825,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3193,  1.3060, -0.5221, -0.5909,  0.1598, -0.0624,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0357,  1.4811, -0.4552,  0.3557,  0.0358,  0.0884,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1366,  1.5605, -0.4017, -0.0444,  0.0829,  0.0238,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 2., 2., 1.]])\n",
      "Rewards:  tensor([[-1.5717,  1.8764, -1.1719, -1.3843]])\n",
      "Next states:  tensor([[[ 0.0335,  1.3245,  0.3178, -0.4344, -0.0578, -0.0825,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3249,  1.2935, -0.5507, -0.5542,  0.1560, -0.0752,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0401,  1.4892, -0.4472,  0.3588,  0.0407,  0.0979,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1407,  1.5589, -0.4099, -0.0713,  0.0858,  0.0568,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1401, -0.0134,  0.0930, -0.2364]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1401, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1401, -0.0134,  0.0930, -0.2364]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0930]])\n",
      " \n",
      "Q_target =  tensor([[-0.1221]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.0180]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.39761344,  1.1302334 , -0.62415737, -0.66745776,  0.11466654,\n",
      "       -0.1003067 ,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.3586,  0.0327,  0.7798, -1.3009, -0.4475,  0.0323,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1815,  1.5441, -0.4635, -0.1413,  0.1264,  0.0891,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0754,  1.5364, -0.4489,  0.1728,  0.0827,  0.1162,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2050,  0.6046,  0.5917, -1.0348, -0.4399, -0.1240,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 2., 2., 2.]])\n",
      "Rewards:  tensor([[ 6.1137, -1.6945, -2.5632, -0.0334]])\n",
      "Next states:  tensor([[[ 0.3665,  0.0028,  0.7910, -1.3307, -0.4484, -0.0180,  1.0000,\n",
      "           0.0000],\n",
      "         [-0.1862,  1.5410, -0.4768, -0.1402,  0.1305,  0.0813,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0798,  1.5411, -0.4472,  0.2064,  0.0890,  0.1253,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2112,  0.5815,  0.6219, -1.0301, -0.4458, -0.1169,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1391, -0.0130,  0.0940, -0.2366]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.2366, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1389, -0.0122,  0.0946, -0.2364]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0946]])\n",
      " \n",
      "Q_target =  tensor([[0.6403]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.8769]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.40383655,  1.1146132 , -0.61551565, -0.69372916,  0.1079149 ,\n",
      "       -0.13503279,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.2966,  0.2522,  0.7322, -1.1715, -0.4621,  0.0547,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1075,  1.0250,  0.4130, -0.7758, -0.2390, -0.2379,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1449,  1.5567, -0.4218, -0.0982,  0.0910,  0.1045,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3508,  0.0620,  0.7798, -1.2743, -0.4491,  0.0323,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[2., 0., 2., 0.]])\n",
      "Rewards:  tensor([[ 2.0442, -1.8052, -2.4255, -2.5064]])\n",
      "Next states:  tensor([[[ 0.3043,  0.2266,  0.7648, -1.1369, -0.4599,  0.0432,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1115,  1.0070,  0.4130, -0.8025, -0.2509, -0.2379,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1493,  1.5552, -0.4456, -0.0699,  0.0954,  0.0887,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3586,  0.0327,  0.7798, -1.3009, -0.4475,  0.0323,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1384, -0.0127,  0.0945, -0.2356]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1384, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1384, -0.0127,  0.0945, -0.2356]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0945]])\n",
      " \n",
      "Q_target =  tensor([[0.0647]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.2032]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.41005975,  1.0983934 , -0.61551505, -0.72039956,  0.10116328,\n",
      "       -0.13503237,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0398,  1.3032,  0.3178, -0.4877, -0.0660, -0.0824,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0669,  1.1988,  0.3756, -0.6323, -0.1207, -0.1491,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2688,  0.3583,  0.6698, -1.1498, -0.4672,  0.0018,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3043,  0.2266,  0.7648, -1.1369, -0.4599,  0.0432,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 0., 0., 0.]])\n",
      "Rewards:  tensor([[-2.5052, -1.5943, -0.6386, -1.1432]])\n",
      "Next states:  tensor([[[ 0.0430,  1.2916,  0.3298, -0.5154, -0.0726, -0.1306,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0706,  1.1839,  0.3756, -0.6590, -0.1282, -0.1491,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2755,  0.3318,  0.6698, -1.1765, -0.4671,  0.0018,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3120,  0.2004,  0.7648, -1.1635, -0.4577,  0.0432,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1378, -0.0127,  0.0946, -0.2360]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1378, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1378, -0.0127,  0.0946, -0.2360]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0946]])\n",
      " \n",
      "Q_target =  tensor([[0.0778]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.2156]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.41628295,  1.0815744 , -0.61551446, -0.74707   ,  0.09441169,\n",
      "       -0.13503191,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.1241,  0.9498,  0.4516, -0.8607, -0.2928, -0.2686,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0119,  1.3863,  0.3117, -0.3142, -0.0185, -0.1339,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0180,  1.4470, -0.4510,  0.3609,  0.0208,  0.0885,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1035,  1.0424,  0.3834, -0.8009, -0.2271, -0.2490,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 1., 2., 2.]])\n",
      "Rewards:  tensor([[-2.6953, -1.1112, -3.9302,  1.1009]])\n",
      "Next states:  tensor([[[ 0.1286,  0.9298,  0.4618, -0.8895, -0.3084, -0.3121,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0148,  1.3786,  0.3035, -0.3403, -0.0235, -0.1008,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0224,  1.4560, -0.4455,  0.4020,  0.0256,  0.0961,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1075,  1.0250,  0.4130, -0.7758, -0.2390, -0.2379,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1388, -0.0133,  0.0937, -0.2358]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.0937, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1388, -0.0133,  0.0937, -0.2358]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0937]])\n",
      " \n",
      "Q_target =  tensor([[4.7287]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[4.6350]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.42246032,  1.0655813 , -0.61155313, -0.71042323,  0.08828539,\n",
      "       -0.12252583,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.1719,  0.7412,  0.5165, -0.9750, -0.4051, -0.1237,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3305,  1.2804, -0.5587, -0.5813,  0.1539, -0.0432,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0632,  1.2130,  0.3640, -0.6049, -0.1133, -0.1026,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0180,  1.4470, -0.4510,  0.3609,  0.0208,  0.0885,  0.0000,\n",
      "           0.0000]]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions:  tensor([[0., 1., 3., 2.]])\n",
      "Rewards:  tensor([[-0.9127, -1.6775, -2.3185, -3.9302]])\n",
      "Next states:  tensor([[[ 0.1770,  0.7187,  0.5165, -1.0016, -0.4112, -0.1237,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3362,  1.2667, -0.5700, -0.6093,  0.1540,  0.0033,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0669,  1.1988,  0.3756, -0.6323, -0.1207, -0.1491,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0224,  1.4560, -0.4455,  0.4020,  0.0256,  0.0961,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1387, -0.0128,  0.1000, -0.2352]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.1000, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1387, -0.0128,  0.1000, -0.2352]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1000]])\n",
      " \n",
      "Q_target =  tensor([[1.5627]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[1.4627]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.42886573,  1.049987  , -0.63356924, -0.6926839 ,  0.0813792 ,\n",
      "       -0.13812372,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.2346,  1.4891, -0.5002, -0.3186,  0.1656,  0.0703,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0148,  1.3786,  0.3035, -0.3403, -0.0235, -0.1008,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0495,  1.2666,  0.3279, -0.5692, -0.0871, -0.1231,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1207,  1.5609, -0.4085,  0.0063,  0.0803,  0.0059,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 3., 2.]])\n",
      "Rewards:  tensor([[-1.1378, -1.7064, -2.2790,  0.1064]])\n",
      "Next states:  tensor([[[-0.2396,  1.4813, -0.5002, -0.3453,  0.1691,  0.0703,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0178,  1.3704,  0.3035, -0.3670, -0.0286, -0.1008,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0528,  1.2532,  0.3362, -0.5966, -0.0950, -0.1566,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1247,  1.5617, -0.4007,  0.0359,  0.0813,  0.0199,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1389, -0.0131,  0.1014, -0.2355]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.0131, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1389, -0.0131,  0.1014, -0.2354]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1014]])\n",
      " \n",
      "Q_target =  tensor([[-0.9134]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.9003]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.43535352,  1.0337828 , -0.64391196, -0.71992975,  0.07655523,\n",
      "       -0.09647933,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.2929,  1.3665, -0.5075, -0.5077,  0.1704, -0.0124,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3724,  1.1867, -0.6207, -0.5849,  0.1347, -0.1017,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2358,  0.4844,  0.6378, -1.0769, -0.4594, -0.0386,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1417,  0.8665,  0.4411, -0.9651, -0.3468, -0.2227,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[2., 2., 0., 2.]])\n",
      "Rewards:  tensor([[ 0.6237,  0.8425, -0.5683,  2.6854]])\n",
      "Next states:  tensor([[[-0.2982,  1.3556, -0.5320, -0.4841,  0.1692, -0.0234,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3787,  1.1735, -0.6242, -0.5875,  0.1297, -0.1003,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2422,  0.4596,  0.6378, -1.1035, -0.4613, -0.0386,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1462,  0.8456,  0.4588, -0.9326, -0.3586, -0.2358,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1383, -0.0136,  0.1024, -0.2351]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.1024, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1383, -0.0135,  0.1024, -0.2351]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1024]])\n",
      " \n",
      "Q_target =  tensor([[3.5999]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[3.4975]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.44175917,  1.0180894 , -0.63633555, -0.6972756 ,  0.07237202,\n",
      "       -0.08366407,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.2893,  0.2786,  0.6848, -1.1929, -0.4648,  0.0450,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0576,  1.5173, -0.4450,  0.2788,  0.0585,  0.1013,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2247,  1.5028, -0.5002, -0.2653,  0.1586,  0.0703,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3120,  0.2004,  0.7648, -1.1635, -0.4577,  0.0432,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[2., 0., 0., 3.]])\n",
      "Rewards:  tensor([[ 0.6027,  0.2740, -1.0747, -2.4560]])\n",
      "Next states:  tensor([[[ 0.2966,  0.2522,  0.7322, -1.1715, -0.4621,  0.0547,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0620,  1.5230, -0.4450,  0.2521,  0.0636,  0.1012,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2296,  1.4962, -0.5002, -0.2920,  0.1621,  0.0703,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3197,  0.1736,  0.7751, -1.1938, -0.4580, -0.0059,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1385, -0.0139,  0.1071, -0.2353]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.0139, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1385, -0.0139,  0.1071, -0.2353]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1071]])\n",
      " \n",
      "Q_target =  tensor([[-1.1074]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-1.0935]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.44823664,  1.0017785 , -0.6453684 , -0.72481555,  0.07001697,\n",
      "       -0.04710066,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0966,  1.5538, -0.4047,  0.1031,  0.0930, -0.0470,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1326,  1.5615, -0.4017, -0.0178,  0.0817,  0.0238,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1956,  1.5329, -0.4753, -0.1942,  0.1359,  0.0759,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0706,  1.1839,  0.3756, -0.6590, -0.1282, -0.1491,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 0., 2., 2.]])\n",
      "Rewards:  tensor([[ 1.6848, -0.2597, -0.9829,  1.6666]])\n",
      "Next states:  tensor([[[-0.1006,  1.5555, -0.3952,  0.0771,  0.0888, -0.0850,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1366,  1.5605, -0.4017, -0.0444,  0.0829,  0.0238,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2003,  1.5284, -0.4813, -0.1977,  0.1396,  0.0747,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0742,  1.1694,  0.3667, -0.6484, -0.1364, -0.1647,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1395, -0.0160,  0.1061, -0.2352]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.0160, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1395, -0.0159,  0.1061, -0.2352]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1061]])\n",
      " \n",
      "Q_target =  tensor([[-1.1146]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-1.0986]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.45477453,  0.9848578 , -0.65295917, -0.75198936,  0.06919181,\n",
      "       -0.01650319,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 2.1117e-01,  5.8151e-01,  6.2193e-01, -1.0301e+00, -4.4575e-01,\n",
      "          -1.1692e-01,  0.0000e+00,  0.0000e+00],\n",
      "         [-6.2020e-02,  1.5230e+00, -4.4503e-01,  2.5215e-01,  6.3566e-02,\n",
      "           1.0123e-01,  0.0000e+00,  0.0000e+00],\n",
      "         [ 2.8243e-01,  3.0547e-01,  6.9423e-01, -1.1693e+00, -4.6705e-01,\n",
      "           8.6306e-04,  0.0000e+00,  0.0000e+00],\n",
      "         [-4.8915e-02,  1.5041e+00, -4.4509e-01,  3.3189e-01,  5.0707e-02,\n",
      "           1.0178e-01,  0.0000e+00,  0.0000e+00]]])\n",
      "Actions:  tensor([[0., 1., 1., 3.]])\n",
      "Rewards:  tensor([[-0.8707, -1.1369,  0.0689,  1.4733]])\n",
      "Next states:  tensor([[[ 0.2173,  0.5578,  0.6219, -1.0567, -0.4516, -0.1169,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0665,  1.5280, -0.4573,  0.2254,  0.0711,  0.1501,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2893,  0.2786,  0.6848, -1.1929, -0.4648,  0.0450,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0532,  1.5110, -0.4334,  0.3058,  0.0534,  0.0547,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1385, -0.0169,  0.1071, -0.2355]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.1071, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1385, -0.0169,  0.1071, -0.2355]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1071]])\n",
      " \n",
      "Q_target =  tensor([[2.5031]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[2.3959]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.46151668,  0.96875656, -0.67273134, -0.71554124,  0.06772622,\n",
      "       -0.02931143,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.3662,  1.1999, -0.6014, -0.6190,  0.1398, -0.0955,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2929,  1.3665, -0.5075, -0.5077,  0.1704, -0.0124,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1632,  1.5532, -0.4749, -0.0351,  0.1103,  0.1345,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1166,  1.5608, -0.4085,  0.0329,  0.0800,  0.0059,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[2., 2., 3., 0.]])\n",
      "Rewards:  tensor([[2.3003, 0.6237, 0.4286, 0.0532]])\n",
      "Next states:  tensor([[[-0.3724,  1.1867, -0.6207, -0.5849,  0.1347, -0.1017,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2982,  1.3556, -0.5320, -0.4841,  0.1692, -0.0234,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1678,  1.5518, -0.4639, -0.0611,  0.1148,  0.0904,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1207,  1.5609, -0.4085,  0.0063,  0.0803,  0.0059,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1395, -0.0168,  0.1092, -0.2345]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.1092, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1396, -0.0168,  0.1092, -0.2345]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1092]])\n",
      " \n",
      "Q_target =  tensor([[1.6800]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[1.5708]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.46837682,  0.9530842 , -0.6841975 , -0.6964733 ,  0.06593022,\n",
      "       -0.03592008,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0445,  1.4966, -0.4472,  0.3321,  0.0456,  0.0979,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1769,  1.5473, -0.4546, -0.1134,  0.1219,  0.0525,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2396,  1.4813, -0.5002, -0.3453,  0.1691,  0.0703,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1862,  1.5410, -0.4768, -0.1402,  0.1305,  0.0813,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[2., 1., 3., 3.]])\n",
      "Rewards:  tensor([[-1.3887, -1.8185, -0.2095,  0.4823]])\n",
      "Next states:  tensor([[[-0.0489,  1.5041, -0.4451,  0.3319,  0.0507,  0.1018,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1815,  1.5441, -0.4635, -0.1413,  0.1264,  0.0891,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2445,  1.4729, -0.4908, -0.3714,  0.1708,  0.0323,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1908,  1.5372, -0.4647, -0.1661,  0.1321,  0.0321,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1399, -0.0174,  0.1112, -0.2349]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.0174, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1399, -0.0173,  0.1112, -0.2348]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1112]])\n",
      " \n",
      "Q_target =  tensor([[-1.4822]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-1.4648]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.47532082,  0.93680435, -0.69472456, -0.7235621 ,  0.06624835,\n",
      "        0.00636253,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.1247,  1.5617, -0.4007,  0.0359,  0.0813,  0.0199,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2929,  1.3665, -0.5075, -0.5077,  0.1704, -0.0124,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1125,  1.5600, -0.4010,  0.0604,  0.0797, -0.0243,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1668,  0.7631,  0.5248, -0.9505, -0.3989, -0.1609,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 2., 1., 1.]])\n",
      "Rewards:  tensor([[ 1.0232,  0.6237, -0.5866, -0.3793]])\n",
      "Next states:  tensor([[[-0.1286,  1.5619, -0.3919,  0.0098,  0.0805, -0.0156,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2982,  1.3556, -0.5320, -0.4841,  0.1692, -0.0234,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1166,  1.5608, -0.4085,  0.0329,  0.0800,  0.0059,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1719,  0.7412,  0.5165, -0.9750, -0.4051, -0.1237,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1393, -0.0189,  0.1117, -0.2351]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1393, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1393, -0.0189,  0.1117, -0.2351]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1117]])\n",
      " \n",
      "Q_target =  tensor([[-0.6796]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.5403]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.48226485,  0.91992444, -0.69472456, -0.7502288 ,  0.06656647,\n",
      "        0.00636255,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0814,  1.1412,  0.3757, -0.6397, -0.1562, -0.2175,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0489,  1.5041, -0.4451,  0.3319,  0.0507,  0.1018,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1933,  0.6505,  0.5517, -1.0385, -0.4275, -0.1249,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3787,  1.1735, -0.6242, -0.5875,  0.1297, -0.1003,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[1., 3., 2., 0.]])\n",
      "Rewards:  tensor([[-1.2197,  1.4733,  1.7735, -0.2294]])\n",
      "Next states:  tensor([[[ 0.0850,  1.1263,  0.3656, -0.6658, -0.1650, -0.1767,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0532,  1.5110, -0.4334,  0.3058,  0.0534,  0.0547,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1991,  0.6279,  0.5917, -1.0081, -0.4337, -0.1240,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3850,  1.1597, -0.6242, -0.6141,  0.1247, -0.1003,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1395, -0.0187,  0.1123, -0.2353]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.0187, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1395, -0.0187,  0.1123, -0.2354]])\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen maximum Q - value=  tensor([[0.1123]])\n",
      " \n",
      "Q_target =  tensor([[-1.5910]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-1.5722]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.48928466,  0.9024307 , -0.704245  , -0.777602  ,  0.0688048 ,\n",
      "        0.04476653,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.4418,  1.0181, -0.6363, -0.6973,  0.0724, -0.0837,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1156,  0.9883,  0.4243, -0.8308, -0.2651, -0.2847,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3088,  1.3320, -0.5196, -0.5366,  0.1644, -0.0733,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2782,  1.4015, -0.4781, -0.5019,  0.1711, -0.0025,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[1., 0., 1., 0.]])\n",
      "Rewards:  tensor([[-1.2135, -1.9466, -1.5266, -0.8703]])\n",
      "Next states:  tensor([[[-0.4482,  1.0018, -0.6454, -0.7248,  0.0700, -0.0471,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1197,  0.9691,  0.4243, -0.8575, -0.2793, -0.2847,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3141,  1.3193, -0.5302, -0.5646,  0.1629, -0.0297,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2829,  1.3896, -0.4781, -0.5286,  0.1710, -0.0025,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1396, -0.0204,  0.1119, -0.2346]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.2346, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1396, -0.0204,  0.1119, -0.2345]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1119]])\n",
      " \n",
      "Q_target =  tensor([[-0.1750]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.0596]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.49623948,  0.8843392 , -0.69609314, -0.80409974,  0.06941007,\n",
      "        0.01210557,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.1046,  1.5567, -0.3952,  0.0504,  0.0845, -0.0849,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3249,  1.2935, -0.5507, -0.5542,  0.1560, -0.0752,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0814,  1.1412,  0.3757, -0.6397, -0.1562, -0.2175,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0180,  1.4470, -0.4510,  0.3609,  0.0208,  0.0885,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[2., 1., 1., 2.]])\n",
      "Rewards:  tensor([[-0.2680, -1.1824, -1.2197, -3.9302]])\n",
      "Next states:  tensor([[[-0.1085,  1.5587, -0.3895,  0.0887,  0.0809, -0.0714,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3305,  1.2804, -0.5587, -0.5813,  0.1539, -0.0432,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0850,  1.1263,  0.3656, -0.6658, -0.1650, -0.1767,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0224,  1.4560, -0.4455,  0.4020,  0.0256,  0.0961,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1400, -0.0210,  0.1117, -0.2350]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1400, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1400, -0.0210,  0.1116, -0.2350]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1116]])\n",
      " \n",
      "Q_target =  tensor([[-0.7021]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.5621]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.5031943 ,  0.86564755, -0.6960932 , -0.8307664 ,  0.07001536,\n",
      "        0.01210585,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-2.6725e-02,  1.4645e+00, -4.3707e-01,  3.7519e-01,  2.8708e-02,\n",
      "           6.2208e-02,  0.0000e+00,  0.0000e+00],\n",
      "         [ 1.2412e-01,  9.4977e-01,  4.5163e-01, -8.6069e-01, -2.9275e-01,\n",
      "          -2.6860e-01,  0.0000e+00,  0.0000e+00],\n",
      "         [-1.5392e-01,  1.5541e+00, -4.6711e-01, -4.6362e-02,  9.9159e-02,\n",
      "           7.4877e-02,  0.0000e+00,  0.0000e+00],\n",
      "         [ 2.8243e-01,  3.0547e-01,  6.9423e-01, -1.1693e+00, -4.6705e-01,\n",
      "           8.6306e-04,  0.0000e+00,  0.0000e+00]]])\n",
      "Actions:  tensor([[2., 3., 2., 1.]])\n",
      "Rewards:  tensor([[-2.6781, -2.6953, -0.1696,  0.0689]])\n",
      "Next states:  tensor([[[-0.0312,  1.4731, -0.4466,  0.3830,  0.0314,  0.0541,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1286,  0.9298,  0.4618, -0.8895, -0.3084, -0.3121,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1585,  1.5540, -0.4633, -0.0075,  0.1036,  0.0879,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2893,  0.2786,  0.6848, -1.1929, -0.4648,  0.0450,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1403, -0.0210,  0.1122, -0.2354]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.0210, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1403, -0.0210,  0.1122, -0.2354]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1122]])\n",
      " \n",
      "Q_target =  tensor([[-1.6590]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-1.6379]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.51023453,  0.84634954, -0.70681304, -0.8578255 ,  0.07277258,\n",
      "        0.05514462,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.4038,  1.1146, -0.6155, -0.6937,  0.1079, -0.1350,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2112,  0.5815,  0.6219, -1.0301, -0.4458, -0.1169,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1286,  0.9298,  0.4618, -0.8895, -0.3084, -0.3121,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3120,  0.2004,  0.7648, -1.1635, -0.4577,  0.0432,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 1., 3.]])\n",
      "Rewards:  tensor([[-0.0289, -0.8707, -1.2496, -2.4560]])\n",
      "Next states:  tensor([[[-0.4101,  1.0984, -0.6155, -0.7204,  0.1012, -0.1350,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2173,  0.5578,  0.6219, -1.0567, -0.4516, -0.1169,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1330,  0.9093,  0.4525, -0.9146, -0.3220, -0.2733,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3197,  0.1736,  0.7751, -1.1938, -0.4580, -0.0059,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1391, -0.0223,  0.1130, -0.2353]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1391, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1391, -0.0222,  0.1131, -0.2353]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1131]])\n",
      " \n",
      "Q_target =  tensor([[-0.9076]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.7685]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.51727474,  0.82645154, -0.706813  , -0.88449275,  0.0755298 ,\n",
      "        0.05514458,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0029,  1.4056,  0.2952, -0.2345, -0.0034, -0.0669,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2829,  1.3896, -0.4781, -0.5286,  0.1710, -0.0025,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2147,  1.5141, -0.5002, -0.2120,  0.1516,  0.0703,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3508,  0.0620,  0.7798, -1.2743, -0.4491,  0.0323,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 2., 0., 0.]])\n",
      "Rewards:  tensor([[-1.3631,  0.8718, -0.9864, -2.5064]])\n",
      "Next states:  tensor([[[ 5.8286e-03,  1.3998e+00,  2.9477e-01, -2.6015e-01, -6.6740e-03,\n",
      "          -6.6084e-02,  0.0000e+00,  0.0000e+00],\n",
      "         [-2.8778e-01,  1.3779e+00, -4.8406e-01, -5.2143e-01,  1.7101e-01,\n",
      "           9.0601e-05,  0.0000e+00,  0.0000e+00],\n",
      "         [-2.1971e-01,  1.5087e+00, -5.0024e-01, -2.3864e-01,  1.5508e-01,\n",
      "           7.0311e-02,  0.0000e+00,  0.0000e+00],\n",
      "         [ 3.5857e-01,  3.2689e-02,  7.7984e-01, -1.3009e+00, -4.4750e-01,\n",
      "           3.2271e-02,  0.0000e+00,  0.0000e+00]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1412, -0.0229,  0.1118, -0.2350]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.2350, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1412, -0.0228,  0.1118, -0.2350]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1118]])\n",
      " \n",
      "Q_target =  tensor([[-0.1424]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.0926]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.5242442 ,  0.8059719 , -0.69791013, -0.9102482 ,  0.07648312,\n",
      "        0.01906619,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.4289,  1.0500, -0.6336, -0.6927,  0.0814, -0.1381,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0998,  1.0604,  0.3919, -0.7751, -0.2146, -0.2840,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3193,  1.3060, -0.5221, -0.5909,  0.1598, -0.0624,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0925,  1.5515, -0.4146,  0.1286,  0.0954, -0.0063,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[1., 1., 2., 3.]])\n",
      "Rewards:  tensor([[-1.0139, -1.4533,  1.8764,  1.6058]])\n",
      "Next states:  tensor([[[-0.4354,  1.0338, -0.6439, -0.7199,  0.0766, -0.0965,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1035,  1.0424,  0.3834, -0.8009, -0.2271, -0.2490,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3249,  1.2935, -0.5507, -0.5542,  0.1560, -0.0752,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0966,  1.5538, -0.4047,  0.1031,  0.0930, -0.0470,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1415, -0.0227,  0.1118, -0.2344]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.1118, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1415, -0.0227,  0.1118, -0.2344]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1118]])\n",
      " \n",
      "Q_target =  tensor([[2.8372]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[2.7254]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.5311831 ,  0.78598136, -0.6952537 , -0.88853997,  0.07783358,\n",
      "        0.02700942,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.3479,  1.2400, -0.5818, -0.6068,  0.1520, -0.0378,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0886,  1.1107,  0.3731, -0.6931, -0.1754, -0.2068,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1156,  0.9883,  0.4243, -0.8308, -0.2651, -0.2847,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2051,  1.5239, -0.4763, -0.2001,  0.1438,  0.0844,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[2., 3., 0., 0.]])\n",
      "Rewards:  tensor([[ 1.6535, -2.6375, -1.9466, -1.0715]])\n",
      "Next states:  tensor([[[-0.3541,  1.2272, -0.6119, -0.5671,  0.1494, -0.0521,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0923,  1.0945,  0.3838, -0.7207, -0.1879, -0.2504,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1197,  0.9691,  0.4243, -0.8575, -0.2793, -0.2847,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2098,  1.5189, -0.4763, -0.2267,  0.1480,  0.0844,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1411, -0.0227,  0.1157, -0.2348]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.2348, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1410, -0.0227,  0.1158, -0.2348]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1158]])\n",
      " \n",
      "Q_target =  tensor([[-0.0791]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.1557]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.5380508 ,  0.76540536, -0.6863016 , -0.9144737 ,  0.07737512,\n",
      "       -0.00916935,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0620,  1.5230, -0.4450,  0.2521,  0.0636,  0.1012,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0798,  1.5411, -0.4472,  0.2064,  0.0890,  0.1253,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1933,  0.6505,  0.5517, -1.0385, -0.4275, -0.1249,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3662,  1.1999, -0.6014, -0.6190,  0.1398, -0.0955,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[1., 3., 2., 2.]])\n",
      "Rewards:  tensor([[-1.1369,  0.9149,  1.7735,  2.3003]])\n",
      "Next states:  tensor([[[-0.0665,  1.5280, -0.4573,  0.2254,  0.0711,  0.1501,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0841,  1.5451, -0.4388,  0.1798,  0.0935,  0.0918,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1991,  0.6279,  0.5917, -1.0081, -0.4337, -0.1240,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3724,  1.1867, -0.6207, -0.5849,  0.1347, -0.1017,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1418, -0.0232,  0.1155, -0.2347]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.1155, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1417, -0.0232,  0.1155, -0.2347]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1155]])\n",
      " \n",
      "Q_target =  tensor([[3.8589]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[3.7434]], grad_fn=<SubBackward0>)\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current state:  (array([-0.54497474,  0.7457275 , -0.6920345 , -0.8745522 ,  0.07701321,\n",
      "       -0.00723811,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-6.2020e-02,  1.5230e+00, -4.4503e-01,  2.5215e-01,  6.3566e-02,\n",
      "           1.0123e-01,  0.0000e+00,  0.0000e+00],\n",
      "         [ 9.2301e-02,  1.0945e+00,  3.8382e-01, -7.2068e-01, -1.8789e-01,\n",
      "          -2.5040e-01,  0.0000e+00,  0.0000e+00],\n",
      "         [ 2.8243e-01,  3.0547e-01,  6.9423e-01, -1.1693e+00, -4.6705e-01,\n",
      "           8.6306e-04,  0.0000e+00,  0.0000e+00],\n",
      "         [-3.0352e-01,  1.3441e+00, -5.3200e-01, -5.1080e-01,  1.6806e-01,\n",
      "          -2.3375e-02,  0.0000e+00,  0.0000e+00]]])\n",
      "Actions:  tensor([[1., 0., 1., 3.]])\n",
      "Rewards:  tensor([[-1.1369, -1.9765,  0.0689,  0.4516]])\n",
      "Next states:  tensor([[[-0.0665,  1.5280, -0.4573,  0.2254,  0.0711,  0.1501,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0960,  1.0778,  0.3838, -0.7474, -0.2004, -0.2504,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2893,  0.2786,  0.6848, -1.1929, -0.4648,  0.0450,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3088,  1.3320, -0.5196, -0.5366,  0.1644, -0.0733,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1410, -0.0227,  0.1212, -0.2349]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.2349, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1409, -0.0227,  0.1212, -0.2348]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1212]])\n",
      " \n",
      "Q_target =  tensor([[0.1980]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.4329]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.5518057 ,  0.7254517 , -0.6803635 , -0.90100354,  0.07431471,\n",
      "       -0.05396999,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 2.8243e-01,  3.0547e-01,  6.9423e-01, -1.1693e+00, -4.6705e-01,\n",
      "           8.6306e-04,  0.0000e+00,  0.0000e+00],\n",
      "         [-2.1474e-01,  1.5141e+00, -5.0024e-01, -2.1197e-01,  1.5156e-01,\n",
      "           7.0311e-02,  0.0000e+00,  0.0000e+00],\n",
      "         [ 3.3480e-02,  1.3245e+00,  3.1782e-01, -4.3441e-01, -5.7784e-02,\n",
      "          -8.2469e-02,  0.0000e+00,  0.0000e+00],\n",
      "         [ 2.6209e-01,  3.8412e-01,  6.7863e-01, -1.1263e+00, -4.6727e-01,\n",
      "          -4.0380e-02,  0.0000e+00,  0.0000e+00]]])\n",
      "Actions:  tensor([[1., 0., 0., 1.]])\n",
      "Rewards:  tensor([[ 0.0689, -0.9864, -1.5591,  0.1163]])\n",
      "Next states:  tensor([[[ 0.2893,  0.2786,  0.6848, -1.1929, -0.4648,  0.0450,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2197,  1.5087, -0.5002, -0.2386,  0.1551,  0.0703,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0366,  1.3141,  0.3178, -0.4611, -0.0619, -0.0824,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2688,  0.3583,  0.6698, -1.1498, -0.4672,  0.0018,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1406, -0.0225,  0.1213, -0.2343]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.2343, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1406, -0.0225,  0.1213, -0.2343]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1213]])\n",
      " \n",
      "Q_target =  tensor([[0.1419]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.3763]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.5585726 ,  0.70458883, -0.67231923, -0.9270256 ,  0.06999156,\n",
      "       -0.08646323,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.1632,  1.5532, -0.4749, -0.0351,  0.1103,  0.1345,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2639,  1.4336, -0.4806, -0.4766,  0.1732, -0.0098,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.4893,  0.9024, -0.7042, -0.7776,  0.0688,  0.0448,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.4548,  0.9849, -0.6530, -0.7520,  0.0692, -0.0165,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 2., 3., 2.]])\n",
      "Rewards:  tensor([[ 0.4286,  2.5533, -0.2858,  2.3970]])\n",
      "Next states:  tensor([[[-0.1678,  1.5518, -0.4639, -0.0611,  0.1148,  0.0904,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2687,  1.4235, -0.4797, -0.4492,  0.1734,  0.0046,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.4962,  0.8843, -0.6961, -0.8041,  0.0694,  0.0121,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.4615,  0.9688, -0.6727, -0.7155,  0.0677, -0.0293,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1420, -0.0225,  0.1203, -0.2326]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1420, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1420, -0.0225,  0.1204, -0.2326]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1204]])\n",
      " \n",
      "Q_target =  tensor([[-0.3759]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.2339]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.56533945,  0.68312633, -0.672319  , -0.95369375,  0.06566841,\n",
      "       -0.08646314,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.1286,  0.9298,  0.4618, -0.8895, -0.3084, -0.3121,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1539,  1.5541, -0.4671, -0.0464,  0.0992,  0.0749,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2553,  0.4095,  0.6786, -1.0996, -0.4653, -0.0404,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3249,  1.2935, -0.5507, -0.5542,  0.1560, -0.0752,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[1., 2., 0., 1.]])\n",
      "Rewards:  tensor([[-1.2496, -0.1696, -0.7271, -1.1824]])\n",
      "Next states:  tensor([[[ 0.1330,  0.9093,  0.4525, -0.9146, -0.3220, -0.2733,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1585,  1.5540, -0.4633, -0.0075,  0.1036,  0.0879,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2621,  0.3841,  0.6786, -1.1263, -0.4673, -0.0404,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3305,  1.2804, -0.5587, -0.5813,  0.1539, -0.0432,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1413, -0.0226,  0.1212, -0.2336]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.1212, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1413, -0.0226,  0.1212, -0.2336]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1212]])\n",
      " \n",
      "Q_target =  tensor([[1.7844]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[1.6633]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.5722896 ,  0.66203445, -0.68997943, -0.93720376,  0.06069562,\n",
      "       -0.09945582,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.1719,  0.7412,  0.5165, -0.9750, -0.4051, -0.1237,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3274,  0.1462,  0.7661, -1.2178, -0.4563,  0.0354,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3787,  1.1735, -0.6242, -0.5875,  0.1297, -0.1003,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0798,  1.5411, -0.4472,  0.2064,  0.0890,  0.1253,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 2., 0., 3.]])\n",
      "Rewards:  tensor([[-0.9127, -0.8658, -0.2294,  0.9149]])\n",
      "Next states:  tensor([[[ 0.1770,  0.7187,  0.5165, -1.0016, -0.4112, -0.1237,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3352,  0.1187,  0.7794, -1.2215, -0.4547,  0.0322,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3850,  1.1597, -0.6242, -0.6141,  0.1247, -0.1003,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0841,  1.5451, -0.4388,  0.1798,  0.0935,  0.0918,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1411, -0.0222,  0.1237, -0.2334]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.0222, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1410, -0.0222,  0.1238, -0.2334]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1238]])\n",
      " \n",
      "Q_target =  tensor([[-1.2726]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-1.2504]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.5793218 ,  0.64033   , -0.70029604, -0.96452844,  0.05780021,\n",
      "       -0.0579084 ,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.5381,  0.7654, -0.6863, -0.9145,  0.0774, -0.0092,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2687,  1.4235, -0.4797, -0.4492,  0.1734,  0.0046,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3601,  1.2139, -0.6014, -0.5923,  0.1446, -0.0955,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0240,  1.3520,  0.3142, -0.4209, -0.0429, -0.1436,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[2., 3., 0., 2.]])\n",
      "Rewards:  tensor([[ 3.7445,  0.2062, -0.2550,  2.0011]])\n",
      "Next states:  tensor([[[-0.5450,  0.7457, -0.6920, -0.8746,  0.0770, -0.0072,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2734,  1.4128, -0.4682, -0.4744,  0.1713, -0.0426,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3662,  1.1999, -0.6014, -0.6190,  0.1398, -0.0955,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0272,  1.3435,  0.3302, -0.3809, -0.0495, -0.1320,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1418, -0.0238,  0.1227, -0.2327]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.1227, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1418, -0.0238,  0.1228, -0.2327]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1228]])\n",
      " \n",
      "Q_target =  tensor([[3.5523]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[3.4296]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.58649886,  0.6195263 , -0.714374  , -0.9244782 ,  0.05450247,\n",
      "       -0.06595486,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.1770,  0.7187,  0.5165, -1.0016, -0.4112, -0.1237,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.5793,  0.6403, -0.7003, -0.9645,  0.0578, -0.0579,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2173,  0.5578,  0.6219, -1.0567, -0.4516, -0.1169,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2982,  1.3556, -0.5320, -0.4841,  0.1692, -0.0234,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[2., 2., 1., 0.]])\n",
      "Rewards:  tensor([[ 0.9956,  3.4308, -0.1918, -0.6982]])\n",
      "Next states:  tensor([[[ 0.1824,  0.6965,  0.5503, -0.9848, -0.4172, -0.1190,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.5865,  0.6195, -0.7144, -0.9245,  0.0545, -0.0660,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2234,  0.5335,  0.6145, -1.0814, -0.4558, -0.0838,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3035,  1.3441, -0.5320, -0.5108,  0.1681, -0.0234,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1410, -0.0234,  0.1285, -0.2331]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.2331, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1410, -0.0234,  0.1285, -0.2331]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1285]])\n",
      " \n",
      "Q_target =  tensor([[-0.0083]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.2248]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.5936166 ,  0.5981355 , -0.7069162 , -0.9505395 ,  0.0497009 ,\n",
      "       -0.09603108,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.2051,  1.5239, -0.4763, -0.2001,  0.1438,  0.0844,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1330,  0.9093,  0.4525, -0.9146, -0.3220, -0.2733,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.5032,  0.8656, -0.6961, -0.8308,  0.0700,  0.0121,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0180,  1.4470, -0.4510,  0.3609,  0.0208,  0.0885,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 1., 2.]])\n",
      "Rewards:  tensor([[-1.0715, -1.7417, -1.7701, -3.9302]])\n",
      "Next states:  tensor([[[-0.2098,  1.5189, -0.4763, -0.2267,  0.1480,  0.0844,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1374,  0.8882,  0.4525, -0.9413, -0.3357, -0.2732,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.5102,  0.8463, -0.7068, -0.8578,  0.0728,  0.0551,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0224,  1.4560, -0.4455,  0.4020,  0.0256,  0.0961,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1419, -0.0241,  0.1276, -0.2328]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1419, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1419, -0.0240,  0.1276, -0.2328]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1276]])\n",
      " \n",
      "Q_target =  tensor([[-0.5098]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.3679]], grad_fn=<SubBackward0>)\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current state:  (array([-0.60073435,  0.5761449 , -0.70691603, -0.9772081 ,  0.04489936,\n",
      "       -0.09603093,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0596,  1.2266,  0.3452, -0.6047, -0.1081, -0.1167,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0209,  1.3615,  0.3142, -0.3942, -0.0358, -0.1437,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1449,  1.5567, -0.4218, -0.0982,  0.0910,  0.1045,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3724,  1.1867, -0.6207, -0.5849,  0.1347, -0.1017,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[2., 0., 2., 2.]])\n",
      "Rewards:  tensor([[-0.4397, -1.8901, -2.4255,  0.8425]])\n",
      "Next states:  tensor([[[ 0.0632,  1.2130,  0.3640, -0.6049, -0.1133, -0.1026,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0240,  1.3520,  0.3142, -0.4209, -0.0429, -0.1436,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1493,  1.5552, -0.4456, -0.0699,  0.0954,  0.0887,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3787,  1.1735, -0.6242, -0.5875,  0.1297, -0.1003,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1424, -0.0244,  0.1273, -0.2332]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1424, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1424, -0.0244,  0.1273, -0.2332]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1273]])\n",
      " \n",
      "Q_target =  tensor([[-0.5420]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.3996]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.6078521 ,  0.55355453, -0.7069159 , -1.0038767 ,  0.04009782,\n",
      "       -0.09603076,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0463,  1.2794,  0.3392, -0.5431, -0.0810, -0.1687,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0303,  1.3343,  0.3178, -0.4077, -0.0537, -0.0825,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0045,  1.4208, -0.4549,  0.4400,  0.0052,  0.1030,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2197,  1.5087, -0.5002, -0.2386,  0.1551,  0.0703,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[1., 0., 1., 0.]])\n",
      "Rewards:  tensor([[-1.0409, -1.5717, -0.4159, -1.0337]])\n",
      "Next states:  tensor([[[ 0.0495,  1.2666,  0.3279, -0.5692, -0.0871, -0.1231,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0335,  1.3245,  0.3178, -0.4344, -0.0578, -0.0825,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0090,  1.4301, -0.4625,  0.4139,  0.0119,  0.1347,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2247,  1.5028, -0.5002, -0.2653,  0.1586,  0.0703,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1431, -0.0246,  0.1272, -0.2334]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.1272, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1431, -0.0246,  0.1272, -0.2334]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1272]])\n",
      " \n",
      "Q_target =  tensor([[1.9125]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[1.7853]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.6151726 ,  0.5314485 , -0.72637475, -0.9823526 ,  0.03449268,\n",
      "       -0.11210287,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.1330,  0.9093,  0.4525, -0.9146, -0.3220, -0.2733,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2358,  0.4844,  0.6378, -1.0769, -0.4594, -0.0386,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.5102,  0.8463, -0.7068, -0.8578,  0.0728,  0.0551,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.5450,  0.7457, -0.6920, -0.8746,  0.0770, -0.0072,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 3.]])\n",
      "Rewards:  tensor([[-1.7417, -0.5683, -1.0195,  0.0781]])\n",
      "Next states:  tensor([[[ 0.1374,  0.8882,  0.4525, -0.9413, -0.3357, -0.2732,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2422,  0.4596,  0.6378, -1.1035, -0.4613, -0.0386,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.5173,  0.8265, -0.7068, -0.8845,  0.0755,  0.0551,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.5518,  0.7255, -0.6804, -0.9010,  0.0743, -0.0540,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1422, -0.0234,  0.1310, -0.2327]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.2327, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1422, -0.0233,  0.1310, -0.2327]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1310]])\n",
      " \n",
      "Q_target =  tensor([[0.2826]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.5153]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.62241083,  0.5087591 , -0.71603703, -1.0082594 ,  0.02680855,\n",
      "       -0.15368256,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.3787,  1.1735, -0.6242, -0.5875,  0.1297, -0.1003,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1511,  0.8254,  0.5043, -0.9018, -0.3698, -0.2238,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1668,  0.7631,  0.5248, -0.9505, -0.3989, -0.1609,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1156,  0.9883,  0.4243, -0.8308, -0.2651, -0.2847,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 2., 1., 0.]])\n",
      "Rewards:  tensor([[-0.2294, -0.9141, -0.3793, -1.9466]])\n",
      "Next states:  tensor([[[-0.3850,  1.1597, -0.6242, -0.6141,  0.1247, -0.1003,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1564,  0.8052,  0.5356, -0.9005, -0.3803, -0.2102,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1719,  0.7412,  0.5165, -0.9750, -0.4051, -0.1237,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1197,  0.9691,  0.4243, -0.8575, -0.2793, -0.2847,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1418, -0.0237,  0.1309, -0.2327]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.0237, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1418, -0.0237,  0.1309, -0.2327]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1309]])\n",
      " \n",
      "Q_target =  tensor([[-1.1606]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-1.1369]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.629722  ,  0.4854672 , -0.7251924 , -1.0350984 ,  0.02095905,\n",
      "       -0.11699003,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0357,  1.4811, -0.4552,  0.3557,  0.0358,  0.0884,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1462,  0.8456,  0.4588, -0.9326, -0.3586, -0.2358,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0576,  1.5173, -0.4450,  0.2788,  0.0585,  0.1013,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.5936,  0.5981, -0.7069, -0.9505,  0.0497, -0.0960,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[2., 2., 0., 0.]])\n",
      "Rewards:  tensor([[-1.1719,  1.0980,  0.2740, -0.6362]])\n",
      "Next states:  tensor([[[-0.0401,  1.4892, -0.4472,  0.3588,  0.0407,  0.0979,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1511,  0.8254,  0.5043, -0.9018, -0.3698, -0.2238,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0620,  1.5230, -0.4450,  0.2521,  0.0636,  0.1012,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.6007,  0.5761, -0.7069, -0.9772,  0.0449, -0.0960,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1432, -0.0259,  0.1301, -0.2324]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.2324, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1432, -0.0259,  0.1301, -0.2324]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1301]])\n",
      " \n",
      "Q_target =  tensor([[-0.0664]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.1660]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.6369703 ,  0.46157336, -0.71730614, -1.0618635 ,  0.01353136,\n",
      "       -0.14855382,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.2197,  1.5087, -0.5002, -0.2386,  0.1551,  0.0703,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2173,  0.5578,  0.6219, -1.0567, -0.4516, -0.1169,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2542,  1.4545, -0.4911, -0.4247,  0.1720,  0.0336,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2050,  0.6046,  0.5917, -1.0348, -0.4399, -0.1240,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 1., 0., 2.]])\n",
      "Rewards:  tensor([[-1.0337, -0.1918, -1.0284, -0.0334]])\n",
      "Next states:  tensor([[[-0.2247,  1.5028, -0.5002, -0.2653,  0.1586,  0.0703,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2234,  0.5335,  0.6145, -1.0814, -0.4558, -0.0838,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2591,  1.4443, -0.4911, -0.4514,  0.1736,  0.0336,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2112,  0.5815,  0.6219, -1.0301, -0.4458, -0.1169,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1423, -0.0255,  0.1304, -0.2322]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.0255, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1423, -0.0255,  0.1304, -0.2322]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1304]])\n",
      " \n",
      "Q_target =  tensor([[-1.4957]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-1.4702]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.64431334,  0.4370727 , -0.72919023, -1.0888797 ,  0.00848516,\n",
      "       -0.10092403,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.1769,  1.5473, -0.4546, -0.1134,  0.1219,  0.0525,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0814,  1.1412,  0.3757, -0.6397, -0.1562, -0.2175,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.4753,  0.9368, -0.6947, -0.7236,  0.0662,  0.0064,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1824,  0.6965,  0.5503, -0.9848, -0.4172, -0.1190,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[1., 1., 0., 1.]])\n",
      "Rewards:  tensor([[-1.8185, -1.2197, -0.7902, -0.1090]])\n",
      "Next states:  tensor([[[-0.1815,  1.5441, -0.4635, -0.1413,  0.1264,  0.0891,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0850,  1.1263,  0.3656, -0.6658, -0.1650, -0.1767,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.4823,  0.9199, -0.6947, -0.7502,  0.0666,  0.0064,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1878,  0.6739,  0.5421, -1.0090, -0.4213, -0.0812,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1426, -0.0274,  0.1302, -0.2321]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.1302, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1426, -0.0274,  0.1302, -0.2321]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1302]])\n",
      " \n",
      "Q_target =  tensor([[3.7301]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[3.5999]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.65164626,  0.41329637, -0.72826445, -1.056705  ,  0.00351959,\n",
      "       -0.09931129,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0178,  1.3704,  0.3035, -0.3670, -0.0286, -0.1008,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1075,  1.0250,  0.4130, -0.7758, -0.2390, -0.2379,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0754,  1.5364, -0.4489,  0.1728,  0.0827,  0.1162,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.6152,  0.5314, -0.7264, -0.9824,  0.0345, -0.1121,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 0., 2., 3.]])\n",
      "Rewards:  tensor([[-2.6538, -1.8052, -2.5632,  0.1529]])\n",
      "Next states:  tensor([[[ 0.0209,  1.3615,  0.3142, -0.3942, -0.0358, -0.1437,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1115,  1.0070,  0.4130, -0.8025, -0.2509, -0.2379,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0798,  1.5411, -0.4472,  0.2064,  0.0890,  0.1253,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.6224,  0.5088, -0.7160, -1.0083,  0.0268, -0.1537,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1428, -0.0275,  0.1350, -0.2322]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.1350, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1428, -0.0275,  0.1350, -0.2322]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1350]])\n",
      " \n",
      "Q_target =  tensor([[2.8242]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[2.6892]], grad_fn=<SubBackward0>)\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current state:  (array([-0.6590849 ,  0.39030385, -0.73835313, -1.0218877 , -0.00192056,\n",
      "       -0.10880307,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.1166,  1.5608, -0.4085,  0.0329,  0.0800,  0.0059,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3305,  1.2804, -0.5587, -0.5813,  0.1539, -0.0432,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1286,  0.9298,  0.4618, -0.8895, -0.3084, -0.3121,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0706,  1.1839,  0.3756, -0.6590, -0.1282, -0.1491,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 1., 1., 2.]])\n",
      "Rewards:  tensor([[ 0.0532, -1.6775, -1.2496,  1.6666]])\n",
      "Next states:  tensor([[[-0.1207,  1.5609, -0.4085,  0.0063,  0.0803,  0.0059,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3362,  1.2667, -0.5700, -0.6093,  0.1540,  0.0033,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1330,  0.9093,  0.4525, -0.9146, -0.3220, -0.2733,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0742,  1.1694,  0.3667, -0.6484, -0.1364, -0.1647,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1426, -0.0274,  0.1387, -0.2322]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.0274, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1426, -0.0274,  0.1387, -0.2322]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1387]])\n",
      " \n",
      "Q_target =  tensor([[-2.4680]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-2.4406]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.66659796,  0.36670455, -0.7476843 , -1.0488669 , -0.00549223,\n",
      "       -0.07143324,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0303,  1.3343,  0.3178, -0.4077, -0.0537, -0.0825,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0841,  1.5451, -0.4388,  0.1798,  0.0935,  0.0918,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.6007,  0.5761, -0.7069, -0.9772,  0.0449, -0.0960,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1247,  1.5617, -0.4007,  0.0359,  0.0813,  0.0199,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 3., 0., 3.]])\n",
      "Rewards:  tensor([[-1.5717,  1.4499, -0.6680,  1.0232]])\n",
      "Next states:  tensor([[[ 0.0335,  1.3245,  0.3178, -0.4344, -0.0578, -0.0825,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0884,  1.5486, -0.4266,  0.1541,  0.0957,  0.0424,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.6079,  0.5536, -0.7069, -1.0039,  0.0401, -0.0960,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1286,  1.5619, -0.3919,  0.0098,  0.0805, -0.0156,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1432, -0.0307,  0.1383, -0.2318]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1432, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1432, -0.0306,  0.1383, -0.2318]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1383]])\n",
      " \n",
      "Q_target =  tensor([[-1.9335]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-1.7903]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.67411095,  0.34250537, -0.7476843 , -1.0755347 , -0.00906388,\n",
      "       -0.07143319,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.1006,  1.5555, -0.3952,  0.0771,  0.0888, -0.0850,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1493,  1.5552, -0.4456, -0.0699,  0.0954,  0.0887,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0489,  1.5041, -0.4451,  0.3319,  0.0507,  0.1018,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1724,  1.5498, -0.4639, -0.0878,  0.1193,  0.0904,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 2., 3., 3.]])\n",
      "Rewards:  tensor([[ 0.7116, -2.4519,  1.4733,  0.2713]])\n",
      "Next states:  tensor([[[-0.1046,  1.5567, -0.3952,  0.0504,  0.0845, -0.0849,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1539,  1.5541, -0.4671, -0.0464,  0.0992,  0.0749,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0532,  1.5110, -0.4334,  0.3058,  0.0534,  0.0547,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1769,  1.5473, -0.4546, -0.1134,  0.1219,  0.0525,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1463, -0.0315,  0.1377, -0.2320]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.1377, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1463, -0.0315,  0.1378, -0.2320]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1378]])\n",
      " \n",
      "Q_target =  tensor([[0.3887]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.2510]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.68162143,  0.31844795, -0.7474175 , -1.0692443 , -0.01264479,\n",
      "       -0.07161818,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.3586,  0.0327,  0.7798, -1.3009, -0.4475,  0.0323,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3088,  1.3320, -0.5196, -0.5366,  0.1644, -0.0733,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1115,  1.0070,  0.4130, -0.8025, -0.2509, -0.2379,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0998,  1.0604,  0.3919, -0.7751, -0.2146, -0.2840,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 1., 3., 1.]])\n",
      "Rewards:  tensor([[ 6.1137, -1.5266, -2.6805, -1.4533]])\n",
      "Next states:  tensor([[[ 0.3665,  0.0028,  0.7910, -1.3307, -0.4484, -0.0180,  1.0000,\n",
      "           0.0000],\n",
      "         [-0.3141,  1.3193, -0.5302, -0.5646,  0.1629, -0.0297,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1156,  0.9883,  0.4243, -0.8308, -0.2651, -0.2847,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1035,  1.0424,  0.3834, -0.8009, -0.2271, -0.2490,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1440, -0.0301,  0.1397, -0.2322]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.0301, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1439, -0.0293,  0.1404, -0.2320]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1404]])\n",
      " \n",
      "Q_target =  tensor([[-2.5071]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-2.4770]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.6892232 ,  0.29380247, -0.75887316, -1.0953672 , -0.01392911,\n",
      "       -0.02568629,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.1564,  0.8052,  0.5356, -0.9005, -0.3803, -0.2102,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.6297,  0.4855, -0.7252, -1.0351,  0.0210, -0.1170,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1247,  1.5617, -0.4007,  0.0359,  0.0813,  0.0199,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2422,  0.4596,  0.6378, -1.1035, -0.4613, -0.0386,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 3., 3., 0.]])\n",
      "Rewards:  tensor([[-1.4156, -0.1952,  1.0232, -0.5871]])\n",
      "Next states:  tensor([[[ 0.1616,  0.7844,  0.5356, -0.9272, -0.3908, -0.2102,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.6370,  0.4616, -0.7173, -1.0619,  0.0135, -0.1486,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1286,  1.5619, -0.3919,  0.0098,  0.0805, -0.0156,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2486,  0.4342,  0.6378, -1.1302, -0.4632, -0.0386,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1448, -0.0333,  0.1394, -0.2318]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.1394, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1447, -0.0333,  0.1395, -0.2318]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1395]])\n",
      " \n",
      "Q_target =  tensor([[2.0373]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[1.8978]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.69678295,  0.26967102, -0.75482816, -1.0725229 , -0.01506785,\n",
      "       -0.02277493,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.1247,  1.5617, -0.4007,  0.0359,  0.0813,  0.0199,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1330,  0.9093,  0.4525, -0.9146, -0.3220, -0.2733,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1493,  1.5552, -0.4456, -0.0699,  0.0954,  0.0887,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0669,  1.1988,  0.3756, -0.6323, -0.1207, -0.1491,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 0., 2., 0.]])\n",
      "Rewards:  tensor([[ 1.0232, -1.7417, -2.4519, -1.5943]])\n",
      "Next states:  tensor([[[-0.1286,  1.5619, -0.3919,  0.0098,  0.0805, -0.0156,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1374,  0.8882,  0.4525, -0.9413, -0.3357, -0.2732,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1539,  1.5541, -0.4671, -0.0464,  0.0992,  0.0749,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0706,  1.1839,  0.3756, -0.6590, -0.1282, -0.1491,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1452, -0.0342,  0.1413, -0.2322]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.1413, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1452, -0.0341,  0.1413, -0.2322]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1413]])\n",
      " \n",
      "Q_target =  tensor([[1.1291]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.9878]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.70418334,  0.24562863, -0.7395981 , -1.068554  , -0.01550131,\n",
      "       -0.00866907,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.2295,  0.5086,  0.6040, -1.1043, -0.4574, -0.0332,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1539,  1.5541, -0.4671, -0.0464,  0.0992,  0.0749,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0495,  1.2666,  0.3279, -0.5692, -0.0871, -0.1231,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0178,  1.3704,  0.3035, -0.3670, -0.0286, -0.1008,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[2., 2., 3., 3.]])\n",
      "Rewards:  tensor([[ 2.1440, -0.1696, -2.2790, -2.6538]])\n",
      "Next states:  tensor([[[ 0.2358,  0.4844,  0.6378, -1.0769, -0.4594, -0.0386,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1585,  1.5540, -0.4633, -0.0075,  0.1036,  0.0879,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0528,  1.2532,  0.3362, -0.5966, -0.0950, -0.1566,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0209,  1.3615,  0.3142, -0.3942, -0.0358, -0.1437,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1448, -0.0339,  0.1428, -0.2323]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.2323, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1447, -0.0339,  0.1429, -0.2324]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1429]])\n",
      " \n",
      "Q_target =  tensor([[-1.7358]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-1.5035]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.7115145 ,  0.22099008, -0.7309143 , -1.0950733 , -0.01767274,\n",
      "       -0.04342879,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0267,  1.4645, -0.4371,  0.3752,  0.0287,  0.0622,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1125,  1.5600, -0.4010,  0.0604,  0.0797, -0.0243,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1085,  1.5587, -0.3895,  0.0887,  0.0809, -0.0714,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0532,  1.5110, -0.4334,  0.3058,  0.0534,  0.0547,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[2., 1., 1., 1.]])\n",
      "Rewards:  tensor([[-2.6781, -0.5866, -0.6860, -0.6523]])\n",
      "Next states:  tensor([[[-0.0312,  1.4731, -0.4466,  0.3830,  0.0314,  0.0541,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1166,  1.5608, -0.4085,  0.0329,  0.0800,  0.0059,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1125,  1.5600, -0.4010,  0.0604,  0.0797, -0.0243,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0576,  1.5173, -0.4450,  0.2788,  0.0585,  0.1013,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1463, -0.0348,  0.1421, -0.2342]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1463, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1463, -0.0348,  0.1420, -0.2341]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1420]])\n",
      " \n",
      "Q_target =  tensor([[-2.3006]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-2.1543]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.7188456 ,  0.19575149, -0.7309143 , -1.1217403 , -0.01984417,\n",
      "       -0.04342874,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.1770,  0.7187,  0.5165, -1.0016, -0.4112, -0.1237,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0357,  1.4811, -0.4552,  0.3557,  0.0358,  0.0884,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3305,  1.2804, -0.5587, -0.5813,  0.1539, -0.0432,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1933,  0.6505,  0.5517, -1.0385, -0.4275, -0.1249,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[2., 2., 1., 2.]])\n",
      "Rewards:  tensor([[ 0.9956, -1.1719, -1.6775,  1.7735]])\n",
      "Next states:  tensor([[[ 0.1824,  0.6965,  0.5503, -0.9848, -0.4172, -0.1190,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0401,  1.4892, -0.4472,  0.3588,  0.0407,  0.0979,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3362,  1.2667, -0.5700, -0.6093,  0.1540,  0.0033,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1991,  0.6279,  0.5917, -1.0081, -0.4337, -0.1240,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1480, -0.0339,  0.1431, -0.2341]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.2341, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1480, -0.0338,  0.1430, -0.2341]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1430]])\n",
      " \n",
      "Q_target =  tensor([[-2.0031]])\n",
      " \n",
      "Error(Q_target - Q_current):  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 2/4000 [00:05<3:18:05,  2.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.7690]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.72607934,  0.16991502, -0.7186953 , -1.1483561 , -0.02446179,\n",
      "       -0.09235232,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.1462,  0.8456,  0.4588, -0.9326, -0.3586, -0.2358,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.6892,  0.2938, -0.7589, -1.0954, -0.0139, -0.0257,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2396,  1.4813, -0.5002, -0.3453,  0.1691,  0.0703,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1511,  0.8254,  0.5043, -0.9018, -0.3698, -0.2238,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[2., 2., 3., 2.]])\n",
      "Rewards:  tensor([[ 1.0980,  1.8992, -0.2095, -0.9141]])\n",
      "Next states:  tensor([[[ 0.1511,  0.8254,  0.5043, -0.9018, -0.3698, -0.2238,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.6968,  0.2697, -0.7548, -1.0725, -0.0151, -0.0228,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2445,  1.4729, -0.4908, -0.3714,  0.1708,  0.0323,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1564,  0.8052,  0.5356, -0.9005, -0.3803, -0.2102,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1476, -0.0331,  0.1433, -0.2360]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.1433, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1476, -0.0330,  0.1433, -0.2360]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1433]])\n",
      " \n",
      "Q_target =  tensor([[2.0730]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[1.9297]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.7331668 ,  0.14462264, -0.70466554, -1.1241785 , -0.02848976,\n",
      "       -0.08055921,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0998,  1.0604,  0.3919, -0.7751, -0.2146, -0.2840,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2112,  0.5815,  0.6219, -1.0301, -0.4458, -0.1169,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.5653,  0.6831, -0.6723, -0.9537,  0.0657, -0.0865,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.5450,  0.7457, -0.6920, -0.8746,  0.0770, -0.0072,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[1., 0., 2., 3.]])\n",
      "Rewards:  tensor([[-1.4533, -0.8707,  1.6645,  0.0781]])\n",
      "Next states:  tensor([[[ 0.1035,  1.0424,  0.3834, -0.8009, -0.2271, -0.2490,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2173,  0.5578,  0.6219, -1.0567, -0.4516, -0.1169,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.5723,  0.6620, -0.6900, -0.9372,  0.0607, -0.0995,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.5518,  0.7255, -0.6804, -0.9010,  0.0743, -0.0540,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1474, -0.0329,  0.1462, -0.2361]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1474, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1474, -0.0329,  0.1462, -0.2360]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1462]])\n",
      " \n",
      "Q_target =  tensor([[-2.7671]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-2.6197]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.7402543 ,  0.11873046, -0.7046656 , -1.1508467 , -0.03251772,\n",
      "       -0.08055912,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.3724,  1.1867, -0.6207, -0.5849,  0.1347, -0.1017,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.4225,  1.0656, -0.6116, -0.7104,  0.0883, -0.1225,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.6007,  0.5761, -0.7069, -0.9772,  0.0449, -0.0960,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2734,  1.4128, -0.4682, -0.4744,  0.1713, -0.0426,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[2., 2., 0., 1.]])\n",
      "Rewards:  tensor([[ 0.8425,  1.4637, -0.6680, -1.6642]])\n",
      "Next states:  tensor([[[-0.3787,  1.1735, -0.6242, -0.5875,  0.1297, -0.1003,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.4289,  1.0500, -0.6336, -0.6927,  0.0814, -0.1381,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.6079,  0.5536, -0.7069, -1.0039,  0.0401, -0.0960,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2782,  1.4015, -0.4781, -0.5019,  0.1711, -0.0025,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1518, -0.0329,  0.1454, -0.2352]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.2352, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1518, -0.0329,  0.1455, -0.2352]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1455]])\n",
      " \n",
      "Q_target =  tensor([[-2.5864]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-2.3512]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.74726593,  0.09224232, -0.6951673 , -1.1773915 , -0.03844533,\n",
      "       -0.11855225,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.2486,  0.4342,  0.6378, -1.1302, -0.4632, -0.0386,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0088,  1.3933,  0.3039, -0.2871, -0.0118, -0.1026,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3305,  1.2804, -0.5587, -0.5813,  0.1539, -0.0432,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2173,  0.5578,  0.6219, -1.0567, -0.4516, -0.1169,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[2., 3., 1., 1.]])\n",
      "Rewards:  tensor([[ 1.8329, -2.4486, -1.6775, -0.1918]])\n",
      "Next states:  tensor([[[ 0.2553,  0.4095,  0.6786, -1.0996, -0.4653, -0.0404,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0119,  1.3863,  0.3117, -0.3142, -0.0185, -0.1339,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3362,  1.2667, -0.5700, -0.6093,  0.1540,  0.0033,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2234,  0.5335,  0.6145, -1.0814, -0.4558, -0.0838,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1507, -0.0334,  0.1459, -0.2398]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.1459, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1507, -0.0334,  0.1459, -0.2398]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1459]])\n",
      " \n",
      "Q_target =  tensor([[0.4037]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.2578]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.754264  ,  0.06614929, -0.6937474 , -1.1598593 , -0.04442824,\n",
      "       -0.11965815,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.4684,  0.9531, -0.6842, -0.6965,  0.0659, -0.0359,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0178,  1.3704,  0.3035, -0.3670, -0.0286, -0.1008,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.4615,  0.9688, -0.6727, -0.7155,  0.0677, -0.0293,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1366,  1.5605, -0.4017, -0.0444,  0.0829,  0.0238,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[1., 3., 2., 1.]])\n",
      "Rewards:  tensor([[-1.5923, -2.6538,  1.5719, -1.3843]])\n",
      "Next states:  tensor([[[-0.4753,  0.9368, -0.6947, -0.7236,  0.0662,  0.0064,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0209,  1.3615,  0.3142, -0.3942, -0.0358, -0.1437,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.4684,  0.9531, -0.6842, -0.6965,  0.0659, -0.0359,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1407,  1.5589, -0.4099, -0.0713,  0.0858,  0.0568,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1517, -0.0334,  0.1454, -0.2391]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.0334, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1517, -0.0334,  0.1455, -0.2391]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1455]])\n",
      " \n",
      "Q_target =  tensor([[24.6824]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[24.7158]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.7605053 ,  0.04064957, -0.5998533 , -1.1337718 , -0.0631373 ,\n",
      "       -0.36755013,  1.        ,  1.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0709,  1.5325, -0.4489,  0.1995,  0.0769,  0.1162,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2003,  1.5284, -0.4813, -0.1977,  0.1396,  0.0747,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3120,  0.2004,  0.7648, -1.1635, -0.4577,  0.0432,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.4893,  0.9024, -0.7042, -0.7776,  0.0688,  0.0448,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 2., 3., 3.]])\n",
      "Rewards:  tensor([[ 0.0286,  0.0361, -2.4560, -0.2858]])\n",
      "Next states:  tensor([[[-0.0754,  1.5364, -0.4489,  0.1728,  0.0827,  0.1162,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2051,  1.5239, -0.4763, -0.2001,  0.1438,  0.0844,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3197,  0.1736,  0.7751, -1.1938, -0.4580, -0.0059,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.4962,  0.8843, -0.6961, -0.8041,  0.0694,  0.0121,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1514, -0.0008,  0.1467, -0.2398]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1514, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1513, -0.0008,  0.1467, -0.2398]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1467]])\n",
      " \n",
      "Q_target =  tensor([[0.5497]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.7011]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.76678896,  0.01593383, -0.601338  , -1.100381  , -0.08243529,\n",
      "       -0.37647218,  1.        ,  1.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.3787,  1.1735, -0.6242, -0.5875,  0.1297, -0.1003,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1862,  1.5410, -0.4768, -0.1402,  0.1305,  0.0813,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0029,  1.4056,  0.2952, -0.2345, -0.0034, -0.0669,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2422,  0.4596,  0.6378, -1.1035, -0.4613, -0.0386,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 3., 0., 0.]])\n",
      "Rewards:  tensor([[-0.2294,  0.4823, -1.3631, -0.5871]])\n",
      "Next states:  tensor([[[-0.3850,  1.1597, -0.6242, -0.6141,  0.1247, -0.1003,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1908,  1.5372, -0.4647, -0.1661,  0.1321,  0.0321,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0058,  1.3998,  0.2948, -0.2602, -0.0067, -0.0661,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2486,  0.4342,  0.6378, -1.1302, -0.4632, -0.0386,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1503, -0.0007,  0.1464, -0.2399]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1503, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1503, -0.0007,  0.1465, -0.2399]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1465]])\n",
      " \n",
      "Q_target =  tensor([[-100.]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-99.8497]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Score:  -98.30393247624025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|███▍                                                                                      | 151/4000 [04:27<1:53:33,  1.77s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16512\\766141446.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m                 \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstep_result\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m                 \u001b[0mnext_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m                 \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m                 \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16512\\1174236218.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, state, action, reward, next_state, done, step_size)\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mddqn\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[0mexperiences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m             \u001b[0merror\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16512\\1174236218.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, experiences)\u001b[0m\n\u001b[0;32m    148\u001b[0m         \u001b[1;31m##UPDATE THE TARGET NETWORK##\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mddqn\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# If DQN, update after each episode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoft_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqnetwork_local\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqnetwork_target\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtau\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# If DDQN, update after every update_every episodes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt_step\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mupdate_every\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16512\\1174236218.py\u001b[0m in \u001b[0;36msoft_update\u001b[1;34m(self, local_model, target_model, tau)\u001b[0m\n\u001b[0;32m    166\u001b[0m             \u001b[1;31m#for each target param - local param pair, update target param by the formula\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m             \u001b[1;31m# target_param = tau*local_param + (1-tau)*target_local\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 168\u001b[1;33m             \u001b[0mtarget_param\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtau\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mlocal_param\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mtau\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtarget_param\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    169\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m     \u001b[1;31m# Function to simulate a model in an environment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Seed = 1\")\n",
    "for i in envs:\n",
    "    print(\"ENVIRONMENT:-----------\", i)\n",
    "    env = gym.make(i)\n",
    "    res=[]\n",
    "    for j in algos:\n",
    "        print(\"Algorithm:\", j[2])\n",
    "        rewards = []\n",
    "        aver_reward = []\n",
    "        aver = deque(maxlen=100)\n",
    "        state_size = env.observation_space.shape[0]\n",
    "        action_size=env.action_space.n\n",
    "        batch_size = 4\n",
    "        seed = 1\n",
    "        agent = Agent(state_size, action_size, hidden_size, num_layers, batch_size, seed, ddqn=j[0], priority=j[1])\n",
    "        epsilon = epsilon_start                    # initialize epsilon\n",
    "       \n",
    "        for i_episode in tqdm(range(1, n_episodes+1)):\n",
    "            if i_episode < 3 or i_episode > 1998:\n",
    "                print(\"Iteration number: \", i_episode - 1)\n",
    "            state = env.reset()\n",
    "            score = 0\n",
    "#             max_t = 200\n",
    "            for t in range(max_t):\n",
    "                action = agent.act(state, epsilon)\n",
    "                step_result = env.step(action)\n",
    "                next_state, reward, done, _ = step_result[:4]\n",
    "                next_state = (next_state, {})\n",
    "                agent.step(state, action, reward, next_state, done, state_size)\n",
    "                \n",
    "                state = next_state\n",
    "                score = score + reward\n",
    "                if done:\n",
    "                    break \n",
    "            if i_episode < 3 or i_episode > 1998:\n",
    "                print(\"Score: \", score)\n",
    "\n",
    "            aver.append(score)     \n",
    "            aver_reward.append(np.mean(aver))\n",
    "            rewards.append(score)\n",
    "            epsilon = max(epsilon_end, epsilon_decay*epsilon) # decrease epsilon\n",
    "            \n",
    "        reward=\"model/Seed41_\"+i+\"_LSTMDQN_\"+str(n_episodes)+\"_\"+str(datetime.now().strftime(\"%Y%m%d%H%M%S\"))\n",
    "        torch.save(agent.qnetwork_local.state_dict(),reward+'.pt')\n",
    "        res.append(aver_reward)\n",
    "        print(\"----------------End Algorithm--------------------\")\n",
    "    \n",
    "    fig=plt.figure()   \n",
    "    \n",
    "    reward='plots/Seed42_'+i+'_LSTMDQN_result'+str(datetime.now().strftime(\"%Y%m%d%H%M%S\"))\n",
    "    df=pd.DataFrame({'DQN':res[0]})\n",
    "    df.to_csv(reward+'.csv')\n",
    "    print(\"------------------------End Environment-------------------\")\n",
    "    \n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.plot(df['DQN'], 'r', label='DQN')\n",
    "    \n",
    "    plt.title('Learning Curve '+i)\n",
    "\n",
    "    #Insert the legends in the plot\n",
    "    fig.legend(loc='lower right')\n",
    "    fig.savefig(reward+'.png', dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f810bdb",
   "metadata": {},
   "source": [
    "## 8. Demonstration with random policy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2caa44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the environment\n",
    "env_name = 'MountainCar-v0'  # Change this to the environment you want to simulate\n",
    "env = gym.make(env_name, render_mode = 'human')\n",
    "\n",
    "# Get environment parameters\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "# Initialize the agent\n",
    "agent = Agent(state_size, action_size, seed=65)  \n",
    "\n",
    "# Simulate the model in the environment with random actions\n",
    "scores = []\n",
    "n_episodes = 100  # Number of episodes for simulation\n",
    "max_t = 1000  # Maximum number of timesteps per episode\n",
    "\n",
    "for i_episode in tqdm(range(1, n_episodes+1)):\n",
    "    state = env.reset()\n",
    "    env.render()\n",
    "    score = 0\n",
    "    \n",
    "    for t in range(max_t):\n",
    "        action = agent.act(state, epsilon=0.5) \n",
    "        step_result = env.step(action)\n",
    "        next_state, reward, done, _ = step_result[:4]\n",
    "        state = next_state\n",
    "        score = score + reward\n",
    "        if done:\n",
    "            break\n",
    "    print(score)        \n",
    "    scores.append(score)\n",
    "\n",
    "# Close the environment\n",
    "env.close()\n",
    "\n",
    "# Print average score\n",
    "print(\"Average score with random actions:\", np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c569436d",
   "metadata": {},
   "source": [
    "## 9. Demonstration with learned policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1980aa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the environment\n",
    "env_name = 'MountainCar-v0'  # Change this to the environment you want to simulate\n",
    "env = gym.make(env_name, render_mode = 'human')\n",
    "\n",
    "# Get environment parameters\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "# Initialize the agent\n",
    "agent = Agent(state_size, action_size, seed=1)  \n",
    "# Load the model weights\n",
    "model_path = 'model/Seed1_MountainCar-v0_PriorityDDQN_4000_20240405161249.pt'  # Path to your trained model\n",
    "agent.qnetwork_local.load_state_dict(torch.load(model_path))\n",
    "agent.qnetwork_local.eval()\n",
    "\n",
    "# Simulate the model in the environment\n",
    "scores = []\n",
    "n_episodes = 100  # Number of episodes for simulation\n",
    "max_t = 1000  # Maximum number of timesteps per episode\n",
    "\n",
    "for i_episode in tqdm(range(1, n_episodes+1)):\n",
    "    state = env.reset()\n",
    "    env.render()\n",
    "    score = 0\n",
    "    \n",
    "    for t in range(max_t):\n",
    "        action = agent.act(state, epsilon=0.)  # Greedy action selection, no exploration\n",
    "        step_result = env.step(action)\n",
    "        next_state, reward, done, _ = step_result[:4]\n",
    "        state = next_state\n",
    "        score = score + reward\n",
    "        if done:\n",
    "            break\n",
    "    print(score)        \n",
    "    scores.append(score)\n",
    "\n",
    "# Close the environment\n",
    "env.close()\n",
    "\n",
    "# Print average score\n",
    "print(\"Average score:\", np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eaa2674",
   "metadata": {},
   "source": [
    "## 10. Draw the shaded plot for each case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "294a26e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard deviation:  0       163.474949\n",
      "1       100.444962\n",
      "2        75.915617\n",
      "3        62.241312\n",
      "4        65.174756\n",
      "           ...    \n",
      "3995     69.422866\n",
      "3996     70.535840\n",
      "3997     70.094424\n",
      "3998     73.780648\n",
      "3999     74.949975\n",
      "Length: 4000, dtype: float64\n",
      "Mean:  0      -250.903886\n",
      "1      -231.173764\n",
      "2      -220.472175\n",
      "3      -204.139179\n",
      "4      -195.567122\n",
      "           ...    \n",
      "3995   -179.953481\n",
      "3996   -180.219427\n",
      "3997   -180.415647\n",
      "3998   -182.397746\n",
      "3999   -182.755575\n",
      "Length: 4000, dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 1: Read Data from CSV\n",
    "df = pd.read_csv('plots/LSTMDQN_LunarLander-v2.csv')\n",
    "\n",
    "# Step 2: Remove the first row (Episodes, Run 1, Run 2, Run 3) to keep only the rewards data\n",
    "# df = df.drop(0)\n",
    "\n",
    "# Step 3: Convert the remaining DataFrame to numeric values\n",
    "df = df.apply(pd.to_numeric)\n",
    "\n",
    "# Step 4: Select only the three late columns (Run 1, Run 2, Run 3)\n",
    "later_columns = df.iloc[:, 1:4]\n",
    "\n",
    "# Step 5: Calculate Mean and Standard Deviation for the three later columns\n",
    "mean_values = later_columns.mean(axis=1)\n",
    "std_values = later_columns.std(axis=1)\n",
    "print(\"Standard deviation: \", std_values)\n",
    "print(\"Mean: \", mean_values)\n",
    "\n",
    "# Step 6: Plot the Data\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(mean_values, label='Mean Sum of Rewards', color = \"red\")\n",
    "plt.fill_between(mean_values.index, mean_values - std_values, mean_values + std_values, alpha=0.5, label='Standard Deviation', color = \"orange\")\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Sum of Rewards')\n",
    "plt.title('Shaded Plot of Sum of Rewards with Mean and Standard Deviation for LSTM Deep Q-Learning Algorithm, Lunar Lander')\n",
    "plt.savefig('plots/shaded_plot_LSTMDQN_LunarLander.png')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df187e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c548e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
