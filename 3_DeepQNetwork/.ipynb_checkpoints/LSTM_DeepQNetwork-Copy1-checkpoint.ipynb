{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2480ed9",
   "metadata": {},
   "source": [
    "# LSTM Deep Q Network for different environments "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc4a8ac",
   "metadata": {},
   "source": [
    "## 1. Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "606f2c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt #plotting library\n",
    "from matplotlib import animation #animated visualizations library\n",
    "from collections import namedtuple, deque \n",
    "#nametuple creates tuple subclasses with name fields, access elements by names instead of index\n",
    "#deque (double-ended queue) for adding and removing elements from both ends\n",
    "from tqdm import tqdm\n",
    "#add progress bars to Python code for easy monitoring progress of loops and tasks\n",
    "# %matplotlib inline \n",
    "import gym #environments for agents\n",
    "from datetime import datetime #manipulating dates and times\n",
    "import pandas as pd #work with structured data\n",
    "import torch #Pytorch supports tensor computations and neural networks\n",
    "import torch.nn as nn #Pytorch supports building neural networks\n",
    "import torch.nn.functional as F\n",
    "#common functions in neural network operations \n",
    "    # Activation functions (ReLU, sigmoid, tanh)\n",
    "    # Loss functions (cross_entropy, mse_loss)\n",
    "    #Utility functions for tensor manipulation (softmax, dropout, batch_norm, etc.)\n",
    "import torch.optim as optim #optimization algorithms for training neural networks\n",
    "import random #generate random numbers/selections\n",
    "from collections import namedtuple, deque \n",
    "import itertools \n",
    "# provides various functions for creating iterators and combining them for complex interators\n",
    "# includes cycle, chain, zip, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3593047",
   "metadata": {},
   "source": [
    "## 2. LSTM model for approximating Q-values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "5db8b6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 64\n",
    "num_layers = 3\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_size, hidden_size, num_layers, action_size, seed):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "\n",
    "        self.hidden_dim = hidden_size\n",
    "        self.layer_dim = num_layers\n",
    "        \n",
    "        #LSTM model\n",
    "        self.lstm = nn.LSTM(input_size=state_size, hidden_size = 64, num_layers = 3, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, action_size)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        # Reshape state to have 3 dimensions: batch_size, sequence_length, input_size\n",
    "        # Assuming state has shape (batch_size, input_size)\n",
    "        h0 = torch.zeros(self.layer_dim, state.size(0), self.hidden_dim).requires_grad_()\n",
    "        c0 = torch.zeros(self.layer_dim, state.size(0), self.hidden_dim).requires_grad_()\n",
    "        # Reshape lstm_out to remove sequence length dimension\n",
    "        lstm_out, (hn,cn) = self.lstm(state, (h0.detach(), c0.detach()))\n",
    "        \n",
    "        lstm_out = self.fc(lstm_out[:, -1, :]) \n",
    "        \n",
    "        return lstm_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba2eecb",
   "metadata": {},
   "source": [
    "## 3. Implement the replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "1d6df66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\") #Device Initialization\n",
    "\n",
    "class ReplayBuffer: #Fixed-size buffer to store experience tuples.\n",
    "    # Initialize a ReplayBuffer object.\n",
    "    def __init__(self, state_size, action_size, buffer_size, batch_size, priority):\n",
    "            #state_size(int): dimension of each state\n",
    "            #action_size (int): dimension of each action \n",
    "            #buffer_size (int): max size of buffer \n",
    "            #batch_size (int): size of 1 training batch\n",
    "            #seed (int): random seed\n",
    "        self.priority = priority        \n",
    "        #Pointer to keep track of position within the replay buffer where next experiene will be added\n",
    "        # ptr = 0, new experience added -> increment to ptr = 1\n",
    "        self. ptr = 0\n",
    "        #Check if buffer have been filled\n",
    "        self.n = 0 \n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.state_size = state_size\n",
    "        \n",
    "        state_size = env.observation_space.shape[0]\n",
    "        action_size = env.action_space.n\n",
    "\n",
    "        #create states tensor full of zeros with {buffer_size} rows & {state_size} columns\n",
    "        #store in device CPU, allocate RAM to it \n",
    "        self.states = torch.zeros((1,)+(buffer_size,)+(state_size,)).to(device)\n",
    "        #Similar to above \n",
    "        self.next_states = torch.zeros((1,)+(buffer_size,)+(state_size,)).to(device)\n",
    "        #Only 1 action taken per experience tuple stored\n",
    "        # => action size = buffer size rows but 1 column only needed to store\n",
    "        self.actions = torch.zeros(1, buffer_size, dtype=torch.long).to(device) \n",
    "        # Reward same with action\n",
    "        self.rewards = torch.zeros(1, buffer_size, dtype=torch.float).to(device) \n",
    "        \n",
    "        # Flag to indicate transition/end between an episode ('done' flag)\n",
    "        # True/False floating values (0.0 False, 1.0 True)\n",
    "        self.dones = torch.zeros(1, buffer_size, dtype=torch.float).to(device)\n",
    "        \n",
    "        # Error in case implement prioritize replay\n",
    "        self.error = np.zeros((1, buffer_size), dtype=float)\n",
    "        \n",
    "        # Priority\n",
    "        self.priority = priority\n",
    "    \n",
    "  # Add new experience to buffer\n",
    "    def add(self, state, action, reward, next_state, done, state_size):\n",
    "        \n",
    "        state_size = env.observation_space.shape[0]\n",
    "\n",
    "        # Convert state and next_state to appropriate types\n",
    "        state = torch.tensor(state[0], dtype=torch.float32).reshape(1, 1,state_size)\n",
    "        state = state.to(device)\n",
    "            \n",
    "\n",
    "        next_state = torch.tensor(next_state[0], dtype=torch.float32).reshape(1, 1, state_size)\n",
    "        next_state = next_state.to(device)\n",
    "            \n",
    "        # Convert action, reward, and done flag to tensors\n",
    "        #action = torch.as_tensor(action, dtype=torch.float32).to(device)\n",
    "        reward = torch.as_tensor(reward, dtype=torch.float32).to(device)\n",
    "        done = torch.as_tensor(done, dtype=torch.bool).to(device)\n",
    "        \n",
    "        # Store the data in the replay buffer\n",
    "        self.states[0][self.ptr] = state\n",
    "        self.next_states[0][self.ptr] = next_state\n",
    "        self.actions[0][self.ptr] = action\n",
    "        self.rewards[0][self.ptr] = reward\n",
    "        self.dones[0][self.ptr] = done\n",
    "        \n",
    "        \n",
    "       \n",
    "        # Increment the pointer\n",
    "        self.ptr = (self.ptr + 1) % self.buffer_size\n",
    "\n",
    "        # Reset pointer and flag when buffer is filled\n",
    "        if self.ptr == 0:\n",
    "            self.n = self.buffer_size\n",
    "        \n",
    "    #Sample a batch of experience from memory\n",
    "    def sample(self, get_all=False):\n",
    "        n = len(self) # Length of the Buffer\n",
    "        state_size = env.observation_space.shape[0]\n",
    "        action_size = env.action_space.n\n",
    "        # Return all experience stored in buffer, no sampling\n",
    "        if get_all:\n",
    "            return self.states[:n], self.actions[:n], self.rewards[:n], self.next_states[:n], self.dones[:n]\n",
    "        \n",
    "        # else, do sampling: \n",
    "        else:\n",
    "            if self.priority:     \n",
    "            #enable prioritized experience replay\n",
    "            #experience are sampled based on priorities probability distribution p = self.error\n",
    "                idx = np.random.choice(n, self.batch_size, replace=False, p=self.error)\n",
    "            else: #uniform sampling\n",
    "                idx = np.random.choice(n, self.batch_size, replace=False)\n",
    "            #Replace = False to only sample once for each element\n",
    "            \n",
    "            states = torch.empty(1, self.batch_size, state_size)\n",
    "            next_states = torch.empty(1, self.batch_size, state_size)\n",
    "            actions = torch.empty(1, self.batch_size)\n",
    "            rewards = torch.empty(1, self.batch_size)\n",
    "            dones = torch.empty(1, self.batch_size)\n",
    "            #Retrieve sampled experiences \n",
    "            states[0] = self.states[0][idx]\n",
    "            next_states[0] = self.next_states[0][idx]\n",
    "            actions[0] = self.actions[0][idx]\n",
    "            rewards[0] = self.rewards[0][idx]\n",
    "            dones[0] = self.dones[0][idx]\n",
    "            \n",
    "            return (states, actions, rewards, next_states, dones), idx\n",
    "    \n",
    "    # Update the error associated with experiences in replay buffer\n",
    "    def update_error(self, error, idx=None): #specify index number, if not specify, all are updated\n",
    "        error = torch.abs(error.detach()) #absolute value of errors, detach to prevent gradient computation to be attached to error tensor\n",
    "        error = error / error.sum() #Normalize, ensure all errors add up to 1\n",
    "        if idx is not None: #index are specified, then only update specified indices\n",
    "            self.error[0][idx] = error.cpu().numpy()\n",
    "        else: # not specify index, all are updated\n",
    "            self.error[0][:len(self)] = error.cpu().numpy()\n",
    "    \n",
    "    # \n",
    "    def __len__(self): \n",
    "        if self.n == 0:\n",
    "            return self.ptr\n",
    "        else:\n",
    "            return self.n #when buffer is filled self.n stored that size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e6742c",
   "metadata": {},
   "source": [
    "## 4. Set the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "5b7e89a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = 100000 #Replay buffer size\n",
    "# batch_size = 3 #Minibatch size\n",
    "gamma = 0.99 #Discount factor\n",
    "tau = 1 #Soft update target parameters, tau = 1 = copy completely\n",
    "alpha = 0.0005 #learning rate\n",
    "update_every = 4 #How often to update the network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4817f2b6",
   "metadata": {},
   "source": [
    "## 5. Agent learning implementation¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "a0d1af1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(): #Learning by interacting with the environment\n",
    "    def __init__(self, state_size, action_size, hidden_size, num_layers, batch_size, seed, ddqn, priority):\n",
    "        self.state_size = state_size #Dimension of each state\n",
    "        self.action_size = action_size #Dimension of action\n",
    "        self.seed = random.seed(seed) #Choose the random seed\n",
    "        self.ddqn = ddqn #Store whether agent uses DDQN\n",
    "        self.priority = priority #Whether uses experience replay\n",
    "        self.batch_size = batch_size\n",
    "        #Q-Network\n",
    "        self.qnetwork_local = QNetwork(state_size, hidden_size, num_layers, action_size, seed).to(device)\n",
    "        self.qnetwork_target = QNetwork(state_size, hidden_size, num_layers, action_size, seed).to(device)\n",
    "        \n",
    "        #Initializes optimizer for updating the weights of local Q-Network\n",
    "        self.optimizer = torch.optim.SGD(self.qnetwork_local.parameters(), lr=alpha)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(state_size, (action_size,), buffer_size, batch_size, priority)\n",
    "        # Initialize timestep (to keep track timesteps for updating target)\n",
    "        self.t_step = 0\n",
    "        \n",
    "    def step(self, state, action, reward, next_state, done, step_size):\n",
    "        # Convert states to torch tensors and add batch dimension\n",
    "\n",
    "        # Save experience in replay memory\n",
    "        self.memory.add(state, action, reward, next_state, done, step_size)\n",
    "\n",
    "        self.t_step += 1  # Increment the timestep counter\n",
    "\n",
    "        # Update the Q-network after each step in the DQN case\n",
    "        if not self.ddqn and len(self.memory) > self.batch_size:\n",
    "            experiences, idx = self.memory.sample()\n",
    "            error = self.learn(experiences)\n",
    "            self.memory.update_error(error, idx)\n",
    "\n",
    "        # In the DDQN case, update the Q-network after a certain number of steps\n",
    "        if self.ddqn and (self.t_step % update_every) == 0 and len(self.memory) > self.batch_size:\n",
    "            experiences, idx = self.memory.sample()\n",
    "            error = self.learn(experiences)\n",
    "            self.memory.update_error(error, idx)\n",
    "\n",
    "                \n",
    " \n",
    "    def act(self, state, epsilon):\n",
    "\n",
    "        state = torch.tensor(state[0], dtype=torch.float32)\n",
    "\n",
    "        \n",
    "        self.qnetwork_local.eval()  # Evaluation mode\n",
    "        with torch.no_grad():  # Disable gradient calculation when choosing action\n",
    "            state_input = state.view(1,1,-1) #treat this tensor as a single time step of a sequence for a batch size of 1\n",
    "            action_values = self.qnetwork_local(state_input)  # Pass preprocessed states into local Q-network\n",
    "        self.qnetwork_local.train()  # Set local Q network back to training mode after inference is complete\n",
    "\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() > epsilon:\n",
    "            # Return action with highest Q-value\n",
    "            action = np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            # Select a random action\n",
    "            action = random.choice(range(self.action_size))\n",
    "        action = min(max(action, 0), self.action_size - 1) # Ensure action is within valid range\n",
    "        \n",
    "        return action\n",
    "\n",
    "    \n",
    "    \n",
    "#     #Error update for prioritization experience replay\n",
    "#     def update_error(self):\n",
    "#         #Sample experience from replay memory\n",
    "#         states, actions, rewards, next_states, dones = self.memory.sample(get_all=True)\n",
    "#         with torch.no_grad(): #no gradient computations within block\n",
    "#             if self.ddqn: #DDQN is enabled, use Q Network for both local and target\n",
    "#                 #Q values of action taken in current state\n",
    "#                 old_val = self.qnetwork_local(states).gather(-1, actions)\n",
    "#                 #Determine actions with highest Q values in next states using local Q-network\n",
    "#                 next_actions = self.qnetwork_local(next_states).argmax(-1, keepdim=True)\n",
    "#                 # Compute Q-values of selected actions from target Q-network for next states\n",
    "#                 maxQ = self.qnetwork_target(next_states).gather(-1, next_actions)\n",
    "#                 #Calculate target as the normal formula\n",
    "#                 #dones is a binary tensor indicating the next state is terminal or not (store done flags)\n",
    "#                 #Use 1-dones to ensure discounted future rewards are considered only for non-terminal states\n",
    "#                 #At terminal done = 1, so 1 - dones = 0, cancels out the maxQ => Q terminal = 0\n",
    "#                 target = rewards+gamma*maxQ*(1-dones)\n",
    "#             else: # Normal DQN\n",
    "#                 #Difference in this DQN is obtained directly from target Q-network using max function\n",
    "#                 #instead of considering the action from local state\n",
    "#                 maxQ = self.qnetwork_target(next_states).max(-1, keepdim=True)[0]\n",
    "#                 target = rewards+gamma*maxQ*(1-dones)\n",
    "#                 old_val = self.qnetwork_local(states).gather(-1, actions)\n",
    "#             error = old_val - target\n",
    "#             self.memory.update_error(error)      \n",
    "            \n",
    "    #Update value parameters using batch of experience tuples\n",
    "    def learn(self, experiences):\n",
    "        #experiences (Tuple[torch.Variable]): tuple of (s, a, r, s', done) tuples \n",
    "        #gamma (float): discount factor\n",
    "        \n",
    "        \n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        ## compute and minimize the loss\n",
    "        self.optimizer.zero_grad() #Resets all gradients to zero, gradients from previous batches do not accumulate\n",
    "        \n",
    "        #Same as explained above\n",
    "        if self.ddqn: #DDQN\n",
    "            old_val = self.qnetwork_local(states).gather(-1, actions)\n",
    "            with torch.no_grad():\n",
    "                next_actions = self.qnetwork_local(next_states).argmax(-1, keepdim=True)\n",
    "                maxQ = self.qnetwork_target(next_states).gather(-1, next_actions)\n",
    "                target = rewards+gamma*maxQ*(1-dones)\n",
    "        else: # Normal DQN\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                maxQ = self.qnetwork_target(next_states).max(-1, keepdim=True)[0]\n",
    "\n",
    "                target = rewards+gamma*maxQ*(1-dones)\n",
    "\n",
    "            old_val = self.qnetwork_local(states).gather(-1, actions.long())   \n",
    "        \n",
    "            # mean-squared error for regression\n",
    "        loss = F.mse_loss(old_val, target) #Calculate loss of current and target Q-values\n",
    "        loss.backward() #Gradient of loss with respect to Q-network parameters are computed\n",
    "                        #and used to update the parameters\n",
    "        self.optimizer.step() #update the neural network\n",
    "                                #step method applies optimization to update parameters\n",
    "                                #steps:\n",
    "                                #1. Optimizer uses computed gradients to update parameters\n",
    "                                #2. Adam optimizer is applied to each parameter based on the gradients and learning rate\n",
    "                                #3. Gradients are cleared back to zero as first step for next training iteration\n",
    "\n",
    "        ##UPDATE THE TARGET NETWORK##\n",
    "        if not self.ddqn:  # If DQN, update after each episode\n",
    "            self.soft_update(self.qnetwork_local, self.qnetwork_target, tau) \n",
    "        else:  # If DDQN, update after every update_every episodes\n",
    "            if (self.t_step % update_every) == 0:\n",
    "                self.soft_update(self.qnetwork_local, self.qnetwork_target, tau)\n",
    "        \n",
    "        return old_val - target #temporal difference TD error between old Q and target Q to monitor training progress\n",
    "\n",
    "    \n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        #local_model: online model, actively being trained. Weights will be copied from here\n",
    "        #target_model: use to generate target Q-values during training. Weights will be copied to here\n",
    "        #tau: interpolation parameters determine rate at which parameters of target models are updated\n",
    "        #small tau slower update, big tau faster update, less stable\n",
    "        \n",
    "        #function iterates over parameters of both target model and local model using zip\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            #for each target param - local param pair, update target param by the formula\n",
    "            # target_param = tau*local_param + (1-tau)*target_local \n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "            \n",
    "    # Function to simulate a model in an environment\n",
    "    def simulate_model(env_name, model_path):\n",
    "        # Load the environment\n",
    "        env = gym.make(env_name)\n",
    "\n",
    "        # Get environment parameters\n",
    "        state_size = env.observation_space.shape[0]\n",
    "        action_size = env.action_space.n\n",
    "\n",
    "        # Initialize the agent\n",
    "        agent = Agent(state_size, action_size, seed) \n",
    "\n",
    "        # Load the model weights\n",
    "        agent.qnetwork_local.load_state_dict(torch.load(model_path))\n",
    "        agent.qnetwork_local.eval()\n",
    "\n",
    "        # Simulate the model in the environment\n",
    "        scores = []\n",
    "        n_episodes = 100  # Number of episodes for simulation\n",
    "        max_t = 1000  # Maximum number of timesteps per episode\n",
    "\n",
    "        for i_episode in tqdm(range(1, n_episodes+1)):\n",
    "            state = env.reset()\n",
    "            score = 0\n",
    "\n",
    "            for t in range(max_t):\n",
    "                action = agent.act(state, epsilon=0.)  # Greedy action selection, no exploration\n",
    "                state, reward, done, _ = env.step(action)\n",
    "                score += reward\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            scores.append(score)\n",
    "\n",
    "        # Close the environment\n",
    "        env.close()\n",
    "\n",
    "        # Print average score\n",
    "        print(\"Average score:\", np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c721e03",
   "metadata": {},
   "source": [
    "## 6. Training parameters and environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "4bc87b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = ['LunarLander-v2'] #list of environments 'MountainCar-v0',\n",
    "#list of implementing algos, each element consists of [DDQN is enabled?, Prioritized experience replay enabled?, type of algo]\n",
    "algos = [[False, False, 'DQN']] #, [True, False, 'DDQN'], [True, True, 'PriorityDDQN']]\n",
    "n_episodes = 4000 #number of training episodes\n",
    "max_t = 1000 #maximum number of timesteps\n",
    "epsilon_start = 0.7 #starting value of epsilon greedy\n",
    "epsilon_end = 0.01 #minimum value of epsilon\n",
    "epsilon_decay = 0.995 #rate at which epsilon decays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71578dd0",
   "metadata": {},
   "source": [
    "## 7. Training implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "e088cb1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed = 1\n",
      "ENVIRONMENT:----------- LunarLander-v2\n",
      "Algorithm: DQN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                               | 0/4000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shape mismatch: value array of shape (1,4) could not be broadcast to indexing result of shape (4,100000)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21240\\1723232659.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m                 \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstep_result\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m                 \u001b[0mnext_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m                 \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m                 \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21240\\3177552830.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, state, action, reward, next_state, done, step_size)\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[0mexperiences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[0merror\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;31m# In the DDQN case, update the Q-network after a certain number of steps\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21240\\468167793.py\u001b[0m in \u001b[0;36mupdate_error\u001b[1;34m(self, error, idx)\u001b[0m\n\u001b[0;32m    115\u001b[0m         \u001b[0merror\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merror\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#Normalize, ensure all errors add up to 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m#index are specified, then only update specified indices\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# not specify index, all are updated\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shape mismatch: value array of shape (1,4) could not be broadcast to indexing result of shape (4,100000)"
     ]
    }
   ],
   "source": [
    "print(\"Seed = 1\")\n",
    "for i in envs:\n",
    "    print(\"ENVIRONMENT:-----------\", i)\n",
    "    env = gym.make(i)\n",
    "    res=[]\n",
    "    for j in algos:\n",
    "        print(\"Algorithm:\", j[2])\n",
    "        rewards = []\n",
    "        aver_reward = []\n",
    "        aver = deque(maxlen=100)\n",
    "        state_size = env.observation_space.shape[0]\n",
    "        action_size=env.action_space.n\n",
    "        batch_size = 4\n",
    "        seed = 1\n",
    "        agent = Agent(state_size, action_size, hidden_size, num_layers, batch_size, seed, ddqn=j[0], priority=j[1])\n",
    "        epsilon = epsilon_start                    # initialize epsilon\n",
    "       \n",
    "        for i_episode in tqdm(range(1, n_episodes+1)):\n",
    "            state = env.reset()\n",
    "            score = 0\n",
    "            for t in range(max_t):\n",
    "                action = agent.act(state, epsilon)\n",
    "                step_result = env.step(action)\n",
    "                next_state, reward, done, _ = step_result[:4]\n",
    "                next_state = (next_state, {})\n",
    "                agent.step(state, action, reward, next_state, done, state_size)\n",
    "                \n",
    "                state = next_state\n",
    "                score = score + reward\n",
    "                if done:\n",
    "                    break \n",
    "\n",
    "            aver.append(score)     \n",
    "            aver_reward.append(np.mean(aver))\n",
    "            rewards.append(score)\n",
    "            epsilon = max(epsilon_end, epsilon_decay*epsilon) # decrease epsilon\n",
    "            \n",
    "        reward=\"model/\"+i+\"_\"+j[2]+\"_\"+str(n_episodes)+\"_\"+str(datetime.now().strftime(\"%Y%m%d%H%M%S\"))\n",
    "        torch.save(agent.qnetwork_local.state_dict(),reward+'.pt')\n",
    "        res.append(aver_reward)\n",
    "        print(\"----------------End Algorithm--------------------\")\n",
    "    \n",
    "    fig=plt.figure()   \n",
    "    \n",
    "    reward='plots/'+i+'_result'+str(datetime.now().strftime(\"%Y%m%d%H%M%S\"))\n",
    "    df=pd.DataFrame({'DQN':res[0]})\n",
    "    df.to_csv(reward+'.csv')\n",
    "    print(\"------------------------End Environment-------------------\")\n",
    "    \n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.plot(df['DQN'], 'r', label='DQN')\n",
    "    \n",
    "    plt.title('Learning Curve '+i)\n",
    "\n",
    "    #Insert the legends in the plot\n",
    "    fig.legend(loc='lower right')\n",
    "    fig.savefig(reward+'.png', dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f810bdb",
   "metadata": {},
   "source": [
    "## 8. Demonstration with random policy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "cf2caa44",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 5 required positional arguments: 'hidden_size', 'num_layers', 'batch_size', 'ddqn', and 'priority'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21240\\1647973928.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# Initialize the agent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0magent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m65\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# Simulate the model in the environment with random actions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() missing 5 required positional arguments: 'hidden_size', 'num_layers', 'batch_size', 'ddqn', and 'priority'"
     ]
    }
   ],
   "source": [
    "# Load the environment\n",
    "env_name = 'MountainCar-v0'  # Change this to the environment you want to simulate\n",
    "env = gym.make(env_name, render_mode = 'human')\n",
    "\n",
    "# Get environment parameters\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "# Initialize the agent\n",
    "agent = Agent(state_size, action_size, seed=65)  \n",
    "\n",
    "# Simulate the model in the environment with random actions\n",
    "scores = []\n",
    "n_episodes = 100  # Number of episodes for simulation\n",
    "max_t = 1000  # Maximum number of timesteps per episode\n",
    "\n",
    "for i_episode in tqdm(range(1, n_episodes+1)):\n",
    "    state = env.reset()\n",
    "    env.render()\n",
    "    score = 0\n",
    "    \n",
    "    for t in range(max_t):\n",
    "        action = agent.act(state, epsilon=0.5) \n",
    "        step_result = env.step(action)\n",
    "        next_state, reward, done, _ = step_result[:4]\n",
    "        state = next_state\n",
    "        score = score + reward\n",
    "        if done:\n",
    "            break\n",
    "    print(score)        \n",
    "    scores.append(score)\n",
    "\n",
    "# Close the environment\n",
    "env.close()\n",
    "\n",
    "# Print average score\n",
    "print(\"Average score with random actions:\", np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c569436d",
   "metadata": {},
   "source": [
    "## 9. Demonstration with learned policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "1980aa0a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 5 required positional arguments: 'hidden_size', 'num_layers', 'batch_size', 'ddqn', and 'priority'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21240\\2294387999.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# Initialize the agent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0magent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;31m# Load the model weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'model/Seed1_MountainCar-v0_PriorityDDQN_4000_20240405161249.pt'\u001b[0m  \u001b[1;31m# Path to your trained model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() missing 5 required positional arguments: 'hidden_size', 'num_layers', 'batch_size', 'ddqn', and 'priority'"
     ]
    }
   ],
   "source": [
    "# Load the environment\n",
    "env_name = 'MountainCar-v0'  # Change this to the environment you want to simulate\n",
    "env = gym.make(env_name, render_mode = 'human')\n",
    "\n",
    "# Get environment parameters\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "# Initialize the agent\n",
    "agent = Agent(state_size, action_size, seed=1)  \n",
    "# Load the model weights\n",
    "model_path = 'model/Seed1_MountainCar-v0_PriorityDDQN_4000_20240405161249.pt'  # Path to your trained model\n",
    "agent.qnetwork_local.load_state_dict(torch.load(model_path))\n",
    "agent.qnetwork_local.eval()\n",
    "\n",
    "# Simulate the model in the environment\n",
    "scores = []\n",
    "n_episodes = 100  # Number of episodes for simulation\n",
    "max_t = 1000  # Maximum number of timesteps per episode\n",
    "\n",
    "for i_episode in tqdm(range(1, n_episodes+1)):\n",
    "    state = env.reset()\n",
    "    env.render()\n",
    "    score = 0\n",
    "    \n",
    "    for t in range(max_t):\n",
    "        action = agent.act(state, epsilon=0.)  # Greedy action selection, no exploration\n",
    "        step_result = env.step(action)\n",
    "        next_state, reward, done, _ = step_result[:4]\n",
    "        state = next_state\n",
    "        score = score + reward\n",
    "        if done:\n",
    "            break\n",
    "    print(score)        \n",
    "    scores.append(score)\n",
    "\n",
    "# Close the environment\n",
    "env.close()\n",
    "\n",
    "# Print average score\n",
    "print(\"Average score:\", np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eaa2674",
   "metadata": {},
   "source": [
    "## 10. Draw the shaded plot for each case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294a26e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Read Data from CSV\n",
    "df = pd.read_csv('plots/DQN_LunarLander-v2_reward.csv')\n",
    "\n",
    "# Step 2: Remove the first row (Episodes, Run 1, Run 2, Run 3) to keep only the rewards data\n",
    "# df = df.drop(0)\n",
    "\n",
    "# Step 3: Convert the remaining DataFrame to numeric values\n",
    "df = df.apply(pd.to_numeric)\n",
    "\n",
    "# Step 4: Select only the three late columns (Run 1, Run 2, Run 3)\n",
    "later_columns = df.iloc[:, 1:4]\n",
    "\n",
    "# Step 5: Calculate Mean and Standard Deviation for the three later columns\n",
    "mean_values = later_columns.mean(axis=1)\n",
    "std_values = later_columns.std(axis=1)\n",
    "print(\"Standard deviation: \", std_values)\n",
    "print(\"Mean: \", mean_values)\n",
    "\n",
    "# Step 6: Plot the Data\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(mean_values, label='Mean Sum of Rewards', color = \"red\")\n",
    "plt.fill_between(mean_values.index, mean_values - std_values, mean_values + std_values, alpha=0.5, label='Standard Deviation', color = \"orange\")\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Sum of Rewards')\n",
    "plt.title('Shaded Plot of Sum of Rewards with Mean and Standard Deviation for Deep Q-Learning Algorithm, Lunar Lander')\n",
    "plt.savefig('plots/shaded_plot_DQN_LunarLander.png')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df187e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c548e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
