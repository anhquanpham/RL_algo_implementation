{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5ce9e09",
   "metadata": {
    "id": "d5ce9e09"
   },
   "source": [
    "# Deep SARSA for different environments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371ede72",
   "metadata": {
    "id": "371ede72"
   },
   "source": [
    "## 1. Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ec291fa",
   "metadata": {
    "id": "8ec291fa"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt #plotting library\n",
    "from matplotlib import animation #animated visualizations library\n",
    "from collections import namedtuple, deque\n",
    "#nametuple creates tuple subclasses with name fields, access elements by names instead of index\n",
    "#deque (double-ended queue) for adding and removing elements from both ends\n",
    "from tqdm import tqdm\n",
    "#add progress bars to Python code for easy monitoring progress of loops and tasks\n",
    "# %matplotlib inline\n",
    "import gym #environments for agents\n",
    "from datetime import datetime #manipulating dates and times\n",
    "import pandas as pd #work with structured data\n",
    "import torch #Pytorch supports tensor computations and neural networks\n",
    "import torch.nn as nn #Pytorch supports building neural networks\n",
    "import torch.nn.functional as Function\n",
    "#common functions in neural network operations\n",
    "    # Activation functions (ReLU, sigmoid, tanh)\n",
    "    # Loss functions (cross_entropy, mse_loss)\n",
    "    #Utility functions for tensor manipulation (softmax, dropout, batch_norm, etc.)\n",
    "import torch.optim as optim #optimization algorithms for training neural networks\n",
    "import random #generate random numbers/selections\n",
    "from collections import namedtuple, deque\n",
    "import itertools\n",
    "# provides various functions for creating iterators and combining them for complex interators\n",
    "# includes cycle, chain, zip, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fb343a",
   "metadata": {
    "id": "f0fb343a"
   },
   "source": [
    "## 2. Neural network model for approximating Q-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5425e056",
   "metadata": {
    "id": "5425e056"
   },
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    #Actor Model\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=64, fc2_units=64):\n",
    "        #Parameters\n",
    "#             state_size (int): Dimensionality of input state space\n",
    "#             action_size (int): Dimensionality of output action space\n",
    "#             seed (int): Random seed for reproducibility\n",
    "#             fc1_units (int): Number of neurons (units) in first fully connected hidden layer\n",
    "#             fc2_units (int): Number of neurons (unit) in second fully connected hidden layer\n",
    "\n",
    "        #Initialization\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed) # Set the random seed\n",
    "        state_size = env.observation_space.shape[0]\n",
    "        #Ensure random numbers generate are reproducible\n",
    "        #Running same code with the same seed will produce the same sequence of random numbers\n",
    "\n",
    "        # nn.Linear creates fully connected layers (input units, output units)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        # Network to map state with action values (Q-values)\n",
    "        #inside method, input state pass through each layer\n",
    "        #ReLU activation functions are applied to the outputs of hidden layers\n",
    "#         print('state:', state.shape)\n",
    "        #x output of 1 layer is input to the next\n",
    "        x = Function.relu(self.fc1(state))\n",
    "        x = Function.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dc5a87",
   "metadata": {
    "id": "44dc5a87"
   },
   "source": [
    "## 3. Set the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42884eda",
   "metadata": {
    "id": "42884eda"
   },
   "outputs": [],
   "source": [
    "gamma = 1 #Discount factor\n",
    "tau = 1 #Soft update target parameters, tau = 1 = copy completely\n",
    "alpha = 0.005 #learning rate\n",
    "update_every = 4 #How often to update the network\n",
    "device = torch.device(\"cpu\") #Device Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffaade0",
   "metadata": {
    "id": "3ffaade0"
   },
   "source": [
    "## 4. Agent learning implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33a98cfb",
   "metadata": {
    "id": "33a98cfb"
   },
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "\n",
    "        self.qnetwork_local = QNetwork(state_size, action_size, seed).to(device)\n",
    "        #self.qnetwork_target = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=alpha)\n",
    "        self.t_step = 0\n",
    "\n",
    "    def step(self, state, action, reward, next_state, next_action, done, state_size):\n",
    "        self.t_step += 1\n",
    "        self.learn(state, action, reward, next_state, next_action, done, state_size)\n",
    "\n",
    "    def act(self, state, epsilon):\n",
    "        if isinstance(state, np.ndarray):\n",
    "            if state.dtype == np.object_:\n",
    "                # Convert elements of the NumPy array to compatible types\n",
    "                state = np.array([self.convert_to_compatible_type(elem) for elem in state])\n",
    "            # Convert the modified NumPy array to a PyTorch tensor for neural network input\n",
    "            state = torch.as_tensor(state, dtype=torch.float32).to(device)\n",
    "        elif isinstance(state, list) or isinstance(state, tuple):\n",
    "            # Convert the state to a NumPy array and handle ragged nested sequences\n",
    "            state = self.handle_ragged_nested_sequence(state)\n",
    "            # Convert the modified NumPy array to a PyTorch tensor for neural network input\n",
    "            state = torch.as_tensor(state, dtype=torch.float32).to(device)\n",
    "        elif isinstance(state, torch.Tensor):\n",
    "            # Ensure the tensor is in the correct device and data type\n",
    "            state = state.to(device)\n",
    "        else:\n",
    "            raise TypeError(\"Input state should be a NumPy array, list, tuple, or torch.Tensor.\")\n",
    "\n",
    "        # Ensure the state has the correct shape for the neural network input\n",
    "        state = state.unsqueeze(0)  # Add a batch dimension\n",
    "\n",
    "        self.qnetwork_local.eval()  # Evaluation mode\n",
    "        with torch.no_grad():  # Disable gradient calculation when choosing action\n",
    "            action_values = self.qnetwork_local(state)  # Pass preprocessed states into local Q-network\n",
    "        self.qnetwork_local.train()  # Set local Q network back to training mode after inference is complete\n",
    "\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() > epsilon:\n",
    "            # Return action with highest Q-value\n",
    "            action = np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            # Select a random action\n",
    "            action = random.choice(range(self.action_size))\n",
    "        action = min(max(action, 0), self.action_size - 1) # Ensure action is within valid range\n",
    "\n",
    "        return action\n",
    "\n",
    "    def convert_to_compatible_type(self, elem):\n",
    "        # Convert the element to a compatible type if it is a numpy.object_\n",
    "        if isinstance(elem, str):\n",
    "            return float(elem)\n",
    "        elif isinstance(elem, np.ndarray):\n",
    "            return elem.astype(np.float32)\n",
    "        else:\n",
    "            return float(elem)\n",
    "\n",
    "    def handle_ragged_nested_sequence(self, state):\n",
    "        if not state:\n",
    "            return np.array([])  # Return an empty NumPy array if the state is empty\n",
    "        else:\n",
    "            # Convert dictionaries to lists of values\n",
    "            state = [list(d.values()) if isinstance(d, dict) else d for d in state]\n",
    "            # Determine the maximum length among all nested sequences\n",
    "            max_length = max(len(sublist) for sublist in state)\n",
    "            # Pad shorter sequences with zeros to make them of equal length\n",
    "            state = [sublist + [0] * (max_length - len(sublist)) if len(sublist) < max_length else sublist for sublist in state]\n",
    "            # Convert the nested sequence to a NumPy array\n",
    "            state = np.array(state)\n",
    "            return state\n",
    "\n",
    "    def learn(self, state, action, reward, next_state, next_action, done, state_size):\n",
    "\n",
    "        state_size = env.observation_space.shape[0]\n",
    "\n",
    "        # Function to flatten ragged nested sequences into a flat array of compatible type\n",
    "        def flatten_nested_sequence(elem):\n",
    "            flattened = []\n",
    "            for e in elem:\n",
    "                if isinstance(e, (list, tuple)):\n",
    "                    flattened.extend(flatten_nested_sequence(e))\n",
    "                elif isinstance(e, np.ndarray):\n",
    "                    if e.dtype == np.object_:\n",
    "                        raise ValueError(\"Cannot handle np.ndarray of type numpy.object_\")\n",
    "                    else:\n",
    "                        flattened.append(e)\n",
    "                else:\n",
    "                    flattened.append(e)\n",
    "            return flattened\n",
    "\n",
    "        # Convert state and next_state to appropriate types\n",
    "        if isinstance(state, np.ndarray):\n",
    "            state = flatten_nested_sequence(state)\n",
    "            state = torch.as_tensor(state, dtype=torch.float32).to(device)\n",
    "        elif isinstance(state, torch.Tensor):\n",
    "            state = state.to(device)\n",
    "        else:\n",
    "            state = torch.zeros((1, state_size), dtype=torch.float32).to(device)\n",
    "\n",
    "        if isinstance(next_state, np.ndarray):\n",
    "            next_state = flatten_nested_sequence(next_state)\n",
    "            next_state = torch.as_tensor(next_state, dtype=torch.float32).to(device)\n",
    "        elif isinstance(next_state, torch.Tensor):\n",
    "            next_state = next_state.to(device)\n",
    "        else:\n",
    "            next_state = torch.zeros((1, state_size), dtype=torch.float32).to(device)\n",
    "        # Convert action, reward, and done flag to tensors\n",
    "        #action = torch.as_tensor(action, dtype=torch.float32).to(device)\n",
    "        reward = torch.as_tensor(reward, dtype=torch.float32).to(device)\n",
    "        #done = torch.as_tensor(done, dtype=torch.bool).to(device)\n",
    "\n",
    "        if state.size() == torch.Size([1, 8]):\n",
    "            # Convert the second type of state to the first type\n",
    "            state = state.view(-1)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        if i_episode < 3 or i_episode > 1998:\n",
    "            print(\"State: \", state)\n",
    "            print(\"State size: \", state.size())\n",
    "            print(\"Action: \", action)\n",
    "            print(\"All Q_current): \", self.qnetwork_local(state))\n",
    "        Q_current = self.qnetwork_local(state)[action]\n",
    "        \n",
    "        if i_episode < 3 or i_episode > 1998:    \n",
    "            print(\"Q(current_action): \", Q_current)\n",
    "\n",
    "        #print(Q_current.shape)\n",
    "        \n",
    "        if i_episode < 3 or i_episode > 1998:\n",
    "            print(\"Next state: \", next_state)\n",
    "            print(\"Next action:  \", next_action)\n",
    "            print(\"All Q_next: \",self.qnetwork_local(next_state))\n",
    "        \n",
    "        Q_next = self.qnetwork_local(next_state)[next_action]\n",
    "        \n",
    "        if i_episode < 3 or i_episode > 1998:\n",
    "            print(\"Q(next_state): \", Q_next)\n",
    "            print(\"Reward\", reward)\n",
    "        \n",
    "        Q_target = reward + gamma * Q_next * (1 - done)\n",
    "        \n",
    "        if i_episode < 3 or i_episode > 1998:\n",
    "            print(\"Q(target_action): \", Q_target)\n",
    "            print(\"Error (Q-target_action - Q_current_action): \", Q_target - Q_current)\n",
    "            print(\" \")\n",
    "\n",
    "        loss = Function.mse_loss(Q_current, Q_target)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # #Update the target network\n",
    "        # self.soft_update(self.qnetwork_local, self.qnetwork_target, tau)\n",
    "\n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        #local_model: online model, actively being trained. Weights will be copied from here\n",
    "        #target_model: use to generate target Q-values during training. Weights will be copied to here\n",
    "        #tau: interpolation parameters determine rate at which parameters of target models are updated\n",
    "        #small tau slower update, big tau faster update, less stable\n",
    "\n",
    "        #function iterates over parameters of both target model and local model using zip\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            #for each target param - local param pair, update target param by the formula\n",
    "            # target_param = tau*local_param + (1-tau)*target_local\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "\n",
    "\n",
    "    # Function to simulate a model in an environment\n",
    "    def simulate_model(env_name, model_path):\n",
    "        # Load the environment\n",
    "        env = gym.make(env_name)\n",
    "\n",
    "        # Get environment parameters\n",
    "        state_size = env.observation_space.shape[0]\n",
    "        action_size = env.action_space.n\n",
    "\n",
    "        # Initialize the agent\n",
    "        agent = Agent(state_size, action_size, seed)\n",
    "\n",
    "        # Load the model weights\n",
    "        agent.qnetwork_local.load_state_dict(torch.load(model_path))\n",
    "        agent.qnetwork_local.eval()\n",
    "\n",
    "        # Simulate the model in the environment\n",
    "        scores = []\n",
    "        n_episodes = 100  # Number of episodes for simulation\n",
    "        max_t = 1000  # Maximum number of timesteps per episode\n",
    "\n",
    "        for i_episode in tqdm(range(1, n_episodes+1)):\n",
    "            state = env.reset()\n",
    "            score = 0\n",
    "\n",
    "            for t in range(max_t):\n",
    "                action = agent.act(state, epsilon = 0)\n",
    "                step_result = env.step(action)\n",
    "                next_state, reward, done, _ = step_result[:4]\n",
    "                next_action = agent.act(next_state, epsilon)\n",
    "                agent.step(state,action,reward,next_state,next_action, done, state_size)\n",
    "                state = next_state\n",
    "                action = next_action\n",
    "                score = score + reward\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            scores.append(score)\n",
    "\n",
    "        # Close the environment\n",
    "        env.close()\n",
    "\n",
    "        # Print average score\n",
    "        print(\"Average score:\", np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ff55dd",
   "metadata": {
    "id": "01ff55dd"
   },
   "source": [
    "## 5. Training parameters and environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "341ccf54",
   "metadata": {
    "id": "341ccf54"
   },
   "outputs": [],
   "source": [
    "envs = ['LunarLander-v2', 'MountainCar-v0'] #list of environments ,\n",
    "seeds = [1,37,42] #list of seeds , 37, 42\n",
    "n_episodes = 2000 #number of training episodes\n",
    "max_t = 1000 #maximum number of timesteps\n",
    "epsilon_start = 1 #starting value of epsilon greedy\n",
    "epsilon_end = 0.01 #minimum value of epsilon\n",
    "epsilon_decay = 0.995 #rate at which epsilon decays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7096f2a3",
   "metadata": {
    "id": "7096f2a3"
   },
   "source": [
    "## 6. Training implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8283e7ee",
   "metadata": {
    "id": "8283e7ee",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algorithm: Deep SARSA\n",
      "ENVIRONMENT:----------- LunarLander-v2\n",
      "Seed =  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                               | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number:  0\n",
      "State:  tensor([0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([-0.0122,  0.0065, -0.0236, -0.0899], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.0122, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-6.2571e-04,  1.4014e+00, -3.1653e-02, -2.2485e-01,  7.2394e-04,\n",
      "         7.0954e-03,  0.0000e+00,  0.0000e+00])\n",
      "Next action:   3\n",
      "All Q_next:  tensor([-0.0058,  0.0767, -0.0193, -0.1091], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.1091, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-2.0661)\n",
      "Q(target_action):  tensor(-2.1753, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-2.1631, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-6.2571e-04,  1.4014e+00, -3.1653e-02, -2.2485e-01,  7.2394e-04,\n",
      "         7.0954e-03,  0.0000e+00,  0.0000e+00])\n",
      "State size:  torch.Size([8])\n",
      "Action:  3\n",
      "All Q_current):  tensor([-0.0163,  0.0577, -0.0056, -0.0178], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.0178, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-8.7423e-04,  1.3957e+00, -2.3562e-02, -2.5179e-01, -5.4419e-04,\n",
      "        -2.5365e-02,  0.0000e+00,  0.0000e+00])\n",
      "Next action:   3\n",
      "All Q_next:  tensor([-0.0161,  0.0585, -0.0080, -0.0170], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.0170, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-2.0278)\n",
      "Q(target_action):  tensor(-2.0448, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-2.0270, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-8.7423e-04,  1.3957e+00, -2.3562e-02, -2.5179e-01, -5.4419e-04,\n",
      "        -2.5365e-02,  0.0000e+00,  0.0000e+00])\n",
      "State size:  torch.Size([8])\n",
      "Action:  3\n",
      "All Q_current):  tensor([-0.0250,  0.0487, -0.0025,  0.0377], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(0.0377, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-1.0343e-03,  1.3895e+00, -1.2485e-02, -2.7829e-01, -4.0318e-03,\n",
      "        -6.9759e-02,  0.0000e+00,  0.0000e+00])\n",
      "Next action:   1\n",
      "All Q_next:  tensor([-0.0244,  0.0498, -0.0053,  0.0392], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(0.0498, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-2.3203)\n",
      "Q(target_action):  tensor(-2.2706, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-2.3082, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-1.0343e-03,  1.3895e+00, -1.2485e-02, -2.7829e-01, -4.0318e-03,\n",
      "        -6.9759e-02,  0.0000e+00,  0.0000e+00])\n",
      "State size:  torch.Size([8])\n",
      "Action:  1\n",
      "All Q_current):  tensor([-0.0384,  0.0952, -0.0080,  0.0313], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(0.0952, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-1.2846e-03,  1.3826e+00, -2.3801e-02, -3.0531e-01, -5.2477e-03,\n",
      "        -2.4319e-02,  0.0000e+00,  0.0000e+00])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([-0.0402,  0.0942, -0.0063,  0.0314], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.0402, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-2.2313)\n",
      "Q(target_action):  tensor(-2.2715, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-2.3667, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-1.2846e-03,  1.3826e+00, -2.3801e-02, -3.0531e-01, -5.2477e-03,\n",
      "        -2.4319e-02,  0.0000e+00,  0.0000e+00])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([-0.0176,  0.0934, -0.0064,  0.0303], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.0176, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0015,  1.3751, -0.0238, -0.3320, -0.0065, -0.0243,  0.0000,  0.0000])\n",
      "Next action:   3\n",
      "All Q_next:  tensor([-0.0182,  0.0937, -0.0070,  0.0311], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(0.0311, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-2.0342)\n",
      "Q(target_action):  tensor(-2.0031, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-1.9855, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0015,  1.3751, -0.0238, -0.3320, -0.0065, -0.0243,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  3\n",
      "All Q_current):  tensor([-0.0171,  0.0905, -0.0059,  0.0459], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(0.0459, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0017,  1.3671, -0.0134, -0.3582, -0.0098, -0.0659,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([-0.0164,  0.0922, -0.0081,  0.0479], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.0164, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-2.1164)\n",
      "Q(target_action):  tensor(-2.1328, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-2.1787, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0017,  1.3671, -0.0134, -0.3582, -0.0098, -0.0659,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([-0.0025,  0.0906, -0.0087,  0.0454], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.0025, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0019,  1.3584, -0.0134, -0.3849, -0.0131, -0.0659,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-0.0029,  0.0912, -0.0089,  0.0465], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.0089, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-2.1285)\n",
      "Q(target_action):  tensor(-2.1374, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-2.1349, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0019,  1.3584, -0.0134, -0.3849, -0.0131, -0.0659,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-0.0106,  0.0848,  0.0109,  0.0378], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(0.0109, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0022,  1.3500, -0.0256, -0.3720, -0.0170, -0.0788,  0.0000,  0.0000])\n",
      "Next action:   1\n",
      "All Q_next:  tensor([-0.0112,  0.0842,  0.0109,  0.0375], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(0.0842, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(1.3694)\n",
      "Q(target_action):  tensor(1.4536, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(1.4427, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0022,  1.3500, -0.0256, -0.3720, -0.0170, -0.0788,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  1\n",
      "All Q_current):  tensor([-0.0190,  0.0663,  0.0375,  0.0301], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(0.0663, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0026,  1.3411, -0.0377, -0.3987, -0.0185, -0.0302,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([-0.0206,  0.0654,  0.0393,  0.0303], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.0206, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-2.0510)\n",
      "Q(target_action):  tensor(-2.0716, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-2.1380, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0026,  1.3411, -0.0377, -0.3987, -0.0185, -0.0302,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([-0.0164,  0.0379,  0.0615,  0.0279], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.0164, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0029,  1.3315, -0.0377, -0.4254, -0.0200, -0.0302,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([-0.0168,  0.0382,  0.0616,  0.0288], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.0168, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.8493)\n",
      "Q(target_action):  tensor(-1.8661, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-1.8497, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0029,  1.3315, -0.0377, -0.4254, -0.0200, -0.0302,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([-0.0144,  0.0146,  0.0804,  0.0264], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.0144, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0033,  1.3213, -0.0377, -0.4521, -0.0215, -0.0302,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([-0.0147,  0.0151,  0.0805,  0.0271], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.0147, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.7908)\n",
      "Q(target_action):  tensor(-1.8055, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-1.7911, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0033,  1.3213, -0.0377, -0.4521, -0.0215, -0.0302,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([-0.0124, -0.0051,  0.0968,  0.0243], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.0124, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0037,  1.3105, -0.0377, -0.4787, -0.0230, -0.0302,  0.0000,  0.0000])\n",
      "Next action:   3\n",
      "All Q_next:  tensor([-0.0125, -0.0047,  0.0969,  0.0251], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(0.0251, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.7320)\n",
      "Q(target_action):  tensor(-1.7069, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-1.6945, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0037,  1.3105, -0.0377, -0.4787, -0.0230, -0.0302,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  3\n",
      "All Q_current):  tensor([-0.0171, -0.0231,  0.1107,  0.0287], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(0.0287, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0040,  1.2992, -0.0266, -0.5054, -0.0268, -0.0745,  0.0000,  0.0000])\n",
      "Next action:   3\n",
      "All Q_next:  tensor([-0.0169, -0.0220,  0.1097,  0.0297], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(0.0297, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.8510)\n",
      "Q(target_action):  tensor(-1.8214, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-1.8501, grad_fn=<SubBackward0>)\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State:  tensor([-0.0040,  1.2992, -0.0266, -0.5054, -0.0268, -0.0745,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  3\n",
      "All Q_current):  tensor([-0.0207, -0.0385,  0.1218,  0.0333], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(0.0333, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0042,  1.2872, -0.0145, -0.5328, -0.0329, -0.1231,  0.0000,  0.0000])\n",
      "Next action:   1\n",
      "All Q_next:  tensor([-0.0206, -0.0374,  0.1210,  0.0345], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.0374, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-2.1364)\n",
      "Q(target_action):  tensor(-2.1737, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-2.2071, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0042,  1.2872, -0.0145, -0.5328, -0.0329, -0.1231,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  1\n",
      "All Q_current):  tensor([-0.0264, -0.0385,  0.1312,  0.0257], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.0385, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0045,  1.2746, -0.0254, -0.5594, -0.0369, -0.0794,  0.0000,  0.0000])\n",
      "Next action:   3\n",
      "All Q_next:  tensor([-0.0271, -0.0385,  0.1326,  0.0263], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(0.0263, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.8702)\n",
      "Q(target_action):  tensor(-1.8438, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-1.8053, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0045,  1.2746, -0.0254, -0.5594, -0.0369, -0.0794,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  3\n",
      "All Q_current):  tensor([-0.0305, -0.0488,  0.1419,  0.0275], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(0.0275, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0047,  1.2614, -0.0144, -0.5863, -0.0430, -0.1235,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-0.0303, -0.0477,  0.1412,  0.0286], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(0.1412, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.9822)\n",
      "Q(target_action):  tensor(-1.8410, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-1.8684, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0047,  1.2614, -0.0144, -0.5863, -0.0430, -0.1235,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-0.0370, -0.0583,  0.1606,  0.0202], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(0.1606, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0048,  1.2486, -0.0032, -0.5708, -0.0488, -0.1151,  0.0000,  0.0000])\n",
      "Next action:   1\n",
      "All Q_next:  tensor([-0.0377, -0.0582,  0.1594,  0.0191], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.0582, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(1.9752)\n",
      "Q(target_action):  tensor(1.9170, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(1.7564, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0048,  1.2486, -0.0032, -0.5708, -0.0488, -0.1151,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  1\n",
      "All Q_current):  tensor([-0.0452, -0.0765,  0.1865,  0.0110], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.0765, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0050,  1.2351, -0.0142, -0.5972, -0.0523, -0.0706,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-0.0460, -0.0764,  0.1879,  0.0116], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(0.1879, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.6957)\n",
      "Q(target_action):  tensor(-1.5078, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-1.4314, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0050,  1.2351, -0.0142, -0.5972, -0.0523, -0.0706,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-0.0539, -0.0998,  0.2209,  0.0037], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(0.2209, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0050,  1.2225,  0.0075, -0.5623, -0.0550, -0.0540,  0.0000,  0.0000])\n",
      "Next action:   3\n",
      "All Q_next:  tensor([-0.0545, -0.0992,  0.2176,  0.0019], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(0.0019, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(4.2019)\n",
      "Q(target_action):  tensor(4.2038, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(3.9829, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0050,  1.2225,  0.0075, -0.5623, -0.0550, -0.0540,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  3\n",
      "All Q_current):  tensor([-0.0637, -0.1193,  0.2602, -0.0168], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.0168, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0048,  1.2092,  0.0198, -0.5891, -0.0602, -0.1030,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([-0.0634, -0.1187,  0.2607, -0.0155], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.0634, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.9266)\n",
      "Q(target_action):  tensor(-1.9900, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-1.9733, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0048,  1.2092,  0.0198, -0.5891, -0.0602, -0.1030,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([-0.0635, -0.1367,  0.3014, -0.0379], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.0635, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0047,  1.1954,  0.0198, -0.6158, -0.0653, -0.1029,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-0.0644, -0.1365,  0.3024, -0.0380], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(0.3024, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.7948)\n",
      "Q(target_action):  tensor(-1.4924, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-1.4289, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0047,  1.1954,  0.0198, -0.6158, -0.0653, -0.1029,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-0.0683, -0.1534,  0.3492, -0.0584], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(0.3492, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0046,  1.1818,  0.0160, -0.6052, -0.0708, -0.1102,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-0.0685, -0.1525,  0.3473, -0.0580], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(0.3473, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(1.5802)\n",
      "Q(target_action):  tensor(1.9275, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(1.5782, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0046,  1.1818,  0.0160, -0.6052, -0.0708, -0.1102,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-0.0701, -0.1681,  0.3918, -0.0768], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(0.3918, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0046,  1.1688,  0.0060, -0.5771, -0.0771, -0.1256,  0.0000,  0.0000])\n",
      "Next action:   3\n",
      "All Q_next:  tensor([-0.0700, -0.1668,  0.3881, -0.0756], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.0756, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(3.1981)\n",
      "Q(target_action):  tensor(3.1225, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(2.7307, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0046,  1.1688,  0.0060, -0.5771, -0.0771, -0.1256,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  3\n",
      "All Q_current):  tensor([-0.0730, -0.1830,  0.4457, -0.1026], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.1026, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0045,  1.1552,  0.0139, -0.6039, -0.0850, -0.1574,  0.0000,  0.0000])\n",
      "Next action:   1\n",
      "All Q_next:  tensor([-0.0739, -0.1833,  0.4487, -0.1028], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.1833, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-2.1613)\n",
      "Q(target_action):  tensor(-2.3446, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-2.2421, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0045,  1.1552,  0.0139, -0.6039, -0.0850, -0.1574,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  1\n",
      "All Q_current):  tensor([-0.0791, -0.1830,  0.5027, -0.1383], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.1830, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0045,  1.1410,  0.0026, -0.6306, -0.0906, -0.1120,  0.0000,  0.0000])\n",
      "Next action:   3\n",
      "All Q_next:  tensor([-0.0800, -0.1828,  0.5030, -0.1389], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.1389, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.8229)\n",
      "Q(target_action):  tensor(-1.9618, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-1.7789, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0045,  1.1410,  0.0026, -0.6306, -0.0906, -0.1120,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  3\n",
      "All Q_current):  tensor([-0.0837, -0.1938,  0.5552, -0.1638], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.1638, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0045,  1.1262,  0.0135, -0.6580, -0.0984, -0.1560,  0.0000,  0.0000])\n",
      "Next action:   3\n",
      "All Q_next:  tensor([-0.0847, -0.1943,  0.5599, -0.1647], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.1647, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-2.0785)\n",
      "Q(target_action):  tensor(-2.2432, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-2.0794, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0045,  1.1262,  0.0135, -0.6580, -0.0984, -0.1560,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  3\n",
      "All Q_current):  tensor([-0.0883, -0.2049,  0.6106, -0.1888], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.1888, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0044,  1.1109,  0.0211, -0.6847, -0.1077, -0.1864,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([-0.0891, -0.2054,  0.6150, -0.1898], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.0891, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-2.1141)\n",
      "Q(target_action):  tensor(-2.2032, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-2.0144, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0044,  1.1109,  0.0211, -0.6847, -0.1077, -0.1864,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([-0.0808, -0.2151,  0.6662, -0.2215], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.0808, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0043,  1.0949,  0.0211, -0.7114, -0.1170, -0.1864,  0.0000,  0.0000])\n",
      "Next action:   3\n",
      "All Q_next:  tensor([-0.0816, -0.2153,  0.6689, -0.2223], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.2223, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.9994)\n",
      "Q(target_action):  tensor(-2.2217, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-2.1409, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0043,  1.0949,  0.0211, -0.7114, -0.1170, -0.1864,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  3\n",
      "All Q_current):  tensor([-0.0864, -0.2245,  0.7130, -0.2402], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.2402, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0041,  1.0783,  0.0315, -0.7389, -0.1284, -0.2283,  0.0000,  0.0000])\n",
      "Next action:   1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Q_next:  tensor([-0.0872, -0.2254,  0.7193, -0.2418], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.2254, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-2.3013)\n",
      "Q(target_action):  tensor(-2.5266, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-2.2864, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0041,  1.0783,  0.0315, -0.7389, -0.1284, -0.2283,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  1\n",
      "All Q_current):  tensor([-0.0939, -0.2173,  0.7605, -0.2703], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.2173, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0039,  1.0611,  0.0214, -0.7651, -0.1378, -0.1880,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-0.0945, -0.2164,  0.7597, -0.2706], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(0.7597, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.8356)\n",
      "Q(target_action):  tensor(-1.0759, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.8586, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0039,  1.0611,  0.0214, -0.7651, -0.1378, -0.1880,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-0.1012, -0.2175,  0.8152, -0.2997], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(0.8152, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0036,  1.0442,  0.0390, -0.7486, -0.1468, -0.1797,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([-0.1001, -0.2151,  0.8060, -0.2969], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.1001, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(2.0621)\n",
      "Q(target_action):  tensor(1.9621, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(1.1469, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0036,  1.0442,  0.0390, -0.7486, -0.1468, -0.1797,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([-0.1146, -0.2181,  0.8779, -0.3286], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.1146, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0033,  1.0268,  0.0390, -0.7753, -0.1558, -0.1797,  0.0000,  0.0000])\n",
      "Next action:   3\n",
      "All Q_next:  tensor([-0.1155, -0.2183,  0.8811, -0.3297], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.3297, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.8201)\n",
      "Q(target_action):  tensor(-2.1497, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-2.0352, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0033,  1.0268,  0.0390, -0.7753, -0.1558, -0.1797,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  3\n",
      "All Q_current):  tensor([-0.1405, -0.2199,  0.9434, -0.3465], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.3465, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0030,  1.0088,  0.0474, -0.8027, -0.1665, -0.2138,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([-0.1421, -0.2210,  0.9515, -0.3488], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.1421, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-2.0755)\n",
      "Q(target_action):  tensor(-2.2175, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-1.8710, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0030,  1.0088,  0.0474, -0.8027, -0.1665, -0.2138,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([-0.1536, -0.2228,  1.0169, -0.3760], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.1536, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0026,  0.9902,  0.0474, -0.8294, -0.1772, -0.2138,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-0.1547, -0.2229,  1.0206, -0.3771], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(1.0206, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.8689)\n",
      "Q(target_action):  tensor(-0.8483, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.6947, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0026,  0.9902,  0.0474, -0.8294, -0.1772, -0.2138,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-0.1716, -0.2262,  1.1012, -0.4070], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(1.1012, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0021,  0.9718,  0.0590, -0.8152, -0.1879, -0.2132,  0.0000,  0.0000])\n",
      "Next action:   3\n",
      "All Q_next:  tensor([-0.1701, -0.2241,  1.0907, -0.4034], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.4034, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(1.8053)\n",
      "Q(target_action):  tensor(1.4019, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(0.3007, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0021,  0.9718,  0.0590, -0.8152, -0.1879, -0.2132,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  3\n",
      "All Q_current):  tensor([-0.1872, -0.2279,  1.1775, -0.4360], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.4360, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0016,  0.9529,  0.0682, -0.8432, -0.2004, -0.2513,  0.0000,  0.0000])\n",
      "Next action:   1\n",
      "All Q_next:  tensor([-0.1895, -0.2293,  1.1901, -0.4397], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.2293, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-2.2595)\n",
      "Q(target_action):  tensor(-2.4888, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-2.0528, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0016,  0.9529,  0.0682, -0.8432, -0.2004, -0.2513,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  1\n",
      "All Q_current):  tensor([-0.2091, -0.2120,  1.2785, -0.4882], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.2120, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0011,  0.9334,  0.0564, -0.8684, -0.2106, -0.2030,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([-0.2092, -0.2107,  1.2730, -0.4871], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.2092, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.5224)\n",
      "Q(target_action):  tensor(-1.7317, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-1.5197, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0011,  0.9334,  0.0564, -0.8684, -0.2106, -0.2030,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([-0.2123, -0.2116,  1.3569, -0.5308], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.2123, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-6.3066e-04,  9.1329e-01,  5.6410e-02, -8.9511e-01, -2.2072e-01,\n",
      "        -2.0298e-01,  0.0000e+00,  0.0000e+00])\n",
      "Next action:   1\n",
      "All Q_next:  tensor([-0.2140, -0.2116,  1.3625, -0.5331], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.2116, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.6662)\n",
      "Q(target_action):  tensor(-1.8779, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-1.6655, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-6.3066e-04,  9.1329e-01,  5.6410e-02, -8.9511e-01, -2.2072e-01,\n",
      "        -2.0298e-01,  0.0000e+00,  0.0000e+00])\n",
      "State size:  torch.Size([8])\n",
      "Action:  1\n",
      "All Q_current):  tensor([-0.2330, -0.1945,  1.4407, -0.5771], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.1945, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-2.3880e-04,  8.9261e-01,  4.7351e-02, -9.2046e-01, -2.2899e-01,\n",
      "        -1.6540e-01,  0.0000e+00,  0.0000e+00])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-0.2332, -0.1935,  1.4358, -0.5760], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(1.4358, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.2683)\n",
      "Q(target_action):  tensor(0.1675, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(0.3621, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-2.3880e-04,  8.9261e-01,  4.7351e-02, -9.2046e-01, -2.2899e-01,\n",
      "        -1.6540e-01,  0.0000e+00,  0.0000e+00])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-0.2499, -0.1716,  1.4905, -0.6135], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(1.4905, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 1.0939e-04,  8.7202e-01,  4.3649e-02, -9.1646e-01, -2.3794e-01,\n",
      "        -1.7904e-01,  0.0000e+00,  0.0000e+00])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-0.2491, -0.1706,  1.4834, -0.6102], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(1.4834, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(1.2817)\n",
      "Q(target_action):  tensor(2.7651, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(1.2746, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 1.0939e-04,  8.7202e-01,  4.3649e-02, -9.1646e-01, -2.3794e-01,\n",
      "        -1.7904e-01,  0.0000e+00,  0.0000e+00])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-0.2649, -0.1496,  1.5340, -0.6452], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(1.5340, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 6.8989e-04,  8.5156e-01,  6.6358e-02, -9.1062e-01, -2.4638e-01,\n",
      "        -1.6877e-01,  0.0000e+00,  0.0000e+00])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-0.2632, -0.1478,  1.5212, -0.6409], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(1.5212, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(1.3484)\n",
      "Q(target_action):  tensor(2.8696, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(1.3356, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 6.8989e-04,  8.5156e-01,  6.6358e-02, -9.1062e-01, -2.4638e-01,\n",
      "        -1.6877e-01,  0.0000e+00,  0.0000e+00])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-0.2775, -0.1281,  1.5686, -0.6734], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(1.5686, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.0014,  0.8311,  0.0781, -0.9087, -0.2548, -0.1679,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([-0.2765, -0.1268,  1.5591, -0.6698], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.2765, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(0.9995)\n",
      "Q(target_action):  tensor(0.7230, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.8456, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.0014,  0.8311,  0.0781, -0.9087, -0.2548, -0.1679,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([-0.2782, -0.1066,  1.5532, -0.6872], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.2782, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.0021,  0.8101,  0.0781, -0.9354, -0.2632, -0.1679,  0.0000,  0.0000])\n",
      "Next action:   1\n",
      "All Q_next:  tensor([-0.2801, -0.1058,  1.5579, -0.6896], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.1058, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.3960)\n",
      "Q(target_action):  tensor(-1.5018, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-1.2236, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.0021,  0.8101,  0.0781, -0.9354, -0.2632, -0.1679,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  1\n",
      "All Q_current):  tensor([-0.2960, -0.0728,  1.5530, -0.7083], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.0728, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.0027,  0.7885,  0.0692, -0.9604, -0.2697, -0.1305,  0.0000,  0.0000])\n",
      "Next action:   1\n",
      "All Q_next:  tensor([-0.2958, -0.0720,  1.5456, -0.7059], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.0720, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-0.9518)\n",
      "Q(target_action):  tensor(-1.0238, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.9511, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.0027,  0.7885,  0.0692, -0.9604, -0.2697, -0.1305,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  1\n",
      "All Q_current):  tensor([-0.3105, -0.0418,  1.5405, -0.7226], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.0418, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.0033,  0.7664,  0.0611, -0.9853, -0.2745, -0.0956,  0.0000,  0.0000])\n",
      "Next action:   0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Q_next:  tensor([-0.3103, -0.0411,  1.5333, -0.7203], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.3103, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-0.7243)\n",
      "Q(target_action):  tensor(-1.0346, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.9928, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.0033,  0.7664,  0.0611, -0.9853, -0.2745, -0.0956,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([-0.3099, -0.0278,  1.5282, -0.7322], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.3099, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.0038,  0.7436,  0.0611, -1.0120, -0.2793, -0.0956,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-0.3120, -0.0266,  1.5322, -0.7348], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(1.5322, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-0.8653)\n",
      "Q(target_action):  tensor(0.6669, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(0.9769, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.0038,  0.7436,  0.0611, -1.0120, -0.2793, -0.0956,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-0.2987, -0.0145,  1.4808, -0.7323], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(1.4808, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.0045,  0.7211,  0.0748, -1.0031, -0.2841, -0.0971,  0.0000,  0.0000])\n",
      "Next action:   1\n",
      "All Q_next:  tensor([-0.2968, -0.0139,  1.4673, -0.7264], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.0139, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(2.2633)\n",
      "Q(target_action):  tensor(2.2495, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(0.7687, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.0045,  0.7211,  0.0748, -1.0031, -0.2841, -0.0971,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  1\n",
      "All Q_current):  tensor([-0.2861, -0.0131,  1.4581, -0.7323], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.0131, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.0052,  0.6980,  0.0674, -1.0281, -0.2874, -0.0652,  0.0000,  0.0000])\n",
      "Next action:   3\n",
      "All Q_next:  tensor([-0.2860, -0.0125,  1.4506, -0.7296], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.7296, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-0.4860)\n",
      "Q(target_action):  tensor(-1.2157, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-1.2025, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.0052,  0.6980,  0.0674, -1.0281, -0.2874, -0.0652,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  3\n",
      "All Q_current):  tensor([-0.2737, -0.0254,  1.4311, -0.7163], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.7163, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.0059,  0.6742,  0.0789, -1.0570, -0.2931, -0.1142,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-0.2777, -0.0236,  1.4478, -0.7245], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(1.4478, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.1924)\n",
      "Q(target_action):  tensor(0.2554, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(0.9716, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.0059,  0.6742,  0.0789, -1.0570, -0.2931, -0.1142,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-0.2627, -0.0348,  1.3849, -0.6897], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(1.3849, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.0070,  0.6507,  0.1129, -1.0465, -0.2980, -0.0982,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([-0.2601, -0.0337,  1.3664, -0.6824], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.2601, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(2.3036)\n",
      "Q(target_action):  tensor(2.0436, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(0.6587, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.0070,  0.6507,  0.1129, -1.0465, -0.2980, -0.0982,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([-0.2546, -0.0435,  1.3390, -0.6600], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.2546, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.0081,  0.6266,  0.1129, -1.0731, -0.3029, -0.0982,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-0.2563, -0.0427,  1.3410, -0.6616], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(1.3410, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-0.7321)\n",
      "Q(target_action):  tensor(0.6089, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(0.8635, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.0081,  0.6266,  0.1129, -1.0731, -0.3029, -0.0982,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-0.2429, -0.0508,  1.2862, -0.6333], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(1.2862, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.0094,  0.6027,  0.1394, -1.0628, -0.3074, -0.0898,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([-0.2411, -0.0499,  1.2711, -0.6274], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.2411, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(2.3586)\n",
      "Q(target_action):  tensor(2.1175, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(0.8313, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.0094,  0.6027,  0.1394, -1.0628, -0.3074, -0.0898,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([-0.2381, -0.0572,  1.2522, -0.6109], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.2381, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.0108,  0.5782,  0.1394, -1.0894, -0.3119, -0.0898,  0.0000,  0.0000])\n",
      "Next action:   1\n",
      "All Q_next:  tensor([-0.2401, -0.0563,  1.2529, -0.6120], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.0563, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-0.6473)\n",
      "Q(target_action):  tensor(-0.7037, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.4655, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.0108,  0.5782,  0.1394, -1.0894, -0.3119, -0.0898,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  1\n",
      "All Q_current):  tensor([-0.2417, -0.0583,  1.2359, -0.5980], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.0583, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.0121,  0.5531,  0.1317, -1.1142, -0.3147, -0.0561,  0.0000,  0.0000])\n",
      "Next action:   1\n",
      "All Q_next:  tensor([-0.2421, -0.0575,  1.2284, -0.5952], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.0575, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-0.1777)\n",
      "Q(target_action):  tensor(-0.2352, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.1768, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.0121,  0.5531,  0.1317, -1.1142, -0.3147, -0.0561,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  1\n",
      "All Q_current):  tensor([-0.2435, -0.0593,  1.2131, -0.5827], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.0593, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.0133,  0.5275,  0.1218, -1.1390, -0.3154, -0.0141,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-0.2434, -0.0585,  1.2033, -0.5789], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(1.2033, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(0.1119)\n",
      "Q(target_action):  tensor(1.3151, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(1.3744, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.0133,  0.5275,  0.1218, -1.1390, -0.3154, -0.0141,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-0.2437, -0.0475,  1.1490, -0.5582], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(1.1490, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.0146,  0.5024,  0.1286, -1.1156, -0.3169, -0.0305,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([-0.2412, -0.0470,  1.1339, -0.5512], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.2412, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(4.3041)\n",
      "Q(target_action):  tensor(4.0629, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(2.9139, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.0146,  0.5024,  0.1286, -1.1156, -0.3169, -0.0305,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([-0.2634, -0.0382,  1.1651, -0.5548], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.2634, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.0158,  0.4767,  0.1286, -1.1422, -0.3185, -0.0305,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([-0.2653, -0.0371,  1.1638, -0.5550], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.2653, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-0.2382)\n",
      "Q(target_action):  tensor(-0.5035, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.2401, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.0158,  0.4767,  0.1286, -1.1422, -0.3185, -0.0305,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([-0.2858, -0.0288,  1.1925, -0.5585], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.2858, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.0171,  0.4504,  0.1286, -1.1689, -0.3200, -0.0305,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-0.2876, -0.0278,  1.1910, -0.5590], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(1.1910, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-0.1799)\n",
      "Q(target_action):  tensor(1.0111, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(1.2969, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.0171,  0.4504,  0.1286, -1.1689, -0.3200, -0.0305,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-0.2945, -0.0202,  1.1840, -0.5526], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(1.1840, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.0184,  0.4241,  0.1325, -1.1703, -0.3219, -0.0385,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([-0.2938, -0.0195,  1.1754, -0.5495], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.2938, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(1.9460)\n",
      "Q(target_action):  tensor(1.6523, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(0.4683, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.0184,  0.4241,  0.1325, -1.1703, -0.3219, -0.0385,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([-0.3043, -0.0125,  1.1808, -0.5470], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.3043, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.0197,  0.3972,  0.1325, -1.1970, -0.3238, -0.0385,  0.0000,  0.0000])\n",
      "Next action:   1\n",
      "All Q_next:  tensor([-0.3061, -0.0114,  1.1803, -0.5475], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.0114, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-0.1593)\n",
      "Q(target_action):  tensor(-0.1708, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(0.1335, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.0197,  0.3972,  0.1325, -1.1970, -0.3238, -0.0385,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  1\n",
      "All Q_current):  tensor([-0.3145, -0.0064,  1.1850, -0.5450], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.0064, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.0209,  0.3697,  0.1217, -1.2209, -0.3234,  0.0091,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([-0.3133, -0.0062,  1.1736, -0.5406], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.3133, grad_fn=<SelectBackward0>)\n",
      "Reward "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                       | 1/2000 [00:00<32:32,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4915)\n",
      "Q(target_action):  tensor(0.1783, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(0.1846, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.0209,  0.3697,  0.1217, -1.2209, -0.3234,  0.0091,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([-3.2258e-01,  2.1012e-04,  1.1780e+00, -5.3861e-01],\n",
      "       grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.3226, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.0222,  0.3416,  0.1217, -1.2475, -0.3229,  0.0091,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-3.2445e-01,  1.1576e-03,  1.1769e+00, -5.3876e-01],\n",
      "       grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(1.1769, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(0.1861)\n",
      "Q(target_action):  tensor(1.3631, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(1.6857, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.0222,  0.3416,  0.1217, -1.2475, -0.3229,  0.0091,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-0.3170,  0.0056,  1.1386, -0.5248], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(1.1386, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.0237,  0.3142,  0.1495, -1.2189, -0.3225,  0.0092,  0.0000,  0.0000])\n",
      "Next action:   1\n",
      "All Q_next:  tensor([-0.3122,  0.0059,  1.1152, -0.5166], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(0.0059, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(5.0088)\n",
      "Q(target_action):  tensor(5.0147, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(3.8761, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.0237,  0.3142,  0.1495, -1.2189, -0.3225,  0.0092,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  1\n",
      "All Q_current):  tensor([-0.3090, -0.0147,  1.1582, -0.5234], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.0147, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.0251,  0.2862,  0.1415, -1.2440, -0.3203,  0.0436,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-0.3086, -0.0146,  1.1494, -0.5201], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(1.1494, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(0.5747)\n",
      "Q(target_action):  tensor(1.7241, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(1.7388, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.0251,  0.2862,  0.1415, -1.2440, -0.3203,  0.0436,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-0.3042, -0.0225,  1.1557, -0.5180], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(1.1557, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.0265,  0.2582,  0.1396, -1.2432, -0.3188,  0.0293,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-0.3033, -0.0220,  1.1465, -0.5143], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(1.1465, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(2.7152)\n",
      "Q(target_action):  tensor(3.8617, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(2.7060, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.0265,  0.2582,  0.1396, -1.2432, -0.3188,  0.0293,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-0.2993, -0.0291,  1.1522, -0.5125], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(1.1522, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.0281,  0.2312,  0.1606, -1.2030, -0.3179,  0.0182,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([-0.2938, -0.0284,  1.1260, -0.5029], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.2938, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(6.1983)\n",
      "Q(target_action):  tensor(5.9046, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(4.7523, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.0281,  0.2312,  0.1606, -1.2030, -0.3179,  0.0182,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([-0.3212, -0.0323,  1.2073, -0.5234], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.3212, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.0297,  0.2035,  0.1606, -1.2297, -0.3170,  0.0182,  0.0000,  0.0000])\n",
      "Next action:   3\n",
      "All Q_next:  tensor([-0.3229, -0.0317,  1.2067, -0.5236], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.5236, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(0.1687)\n",
      "Q(target_action):  tensor(-0.3549, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.0337, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.0297,  0.2035,  0.1606, -1.2297, -0.3170,  0.0182,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  3\n",
      "All Q_current):  tensor([-0.3495, -0.0356,  1.2833, -0.5422], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.5422, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.0314,  0.1752,  0.1693, -1.2577, -0.3179, -0.0182,  0.0000,  0.0000])\n",
      "Next action:   1\n",
      "All Q_next:  tensor([-0.3539, -0.0344,  1.2920, -0.5464], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.0344, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-0.2519)\n",
      "Q(target_action):  tensor(-0.2863, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(0.2559, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.0314,  0.1752,  0.1693, -1.2577, -0.3179, -0.0182,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  1\n",
      "All Q_current):  tensor([-0.3804, -0.0411,  1.3626, -0.5615], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.0411, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.0330,  0.1463,  0.1620, -1.2831, -0.3173,  0.0127,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-0.3804, -0.0411,  1.3547, -0.5589], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(1.3547, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(0.4152)\n",
      "Q(target_action):  tensor(1.7700, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(1.8111, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.0330,  0.1463,  0.1620, -1.2831, -0.3173,  0.0127,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-0.4012, -0.0328,  1.3856, -0.5629], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(1.3856, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.0350,  0.1175,  0.1919, -1.2809, -0.3159,  0.0271,  0.0000,  0.0000])\n",
      "Next action:   1\n",
      "All Q_next:  tensor([-0.3988, -0.0321,  1.3671, -0.5586], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.0321, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(2.3858)\n",
      "Q(target_action):  tensor(2.3537, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(0.9680, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.0350,  0.1175,  0.1919, -1.2809, -0.3159,  0.0271,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  1\n",
      "All Q_current):  tensor([-0.4200, -0.0319,  1.4132, -0.5669], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.0319, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.0368,  0.0881,  0.1828, -1.3059, -0.3126,  0.0655,  0.0000,  0.0000])\n",
      "Next action:   3\n",
      "All Q_next:  tensor([-0.4190, -0.0320,  1.4026, -0.5633], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.5633, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(0.6653)\n",
      "Q(target_action):  tensor(0.1020, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(0.1339, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.0368,  0.0881,  0.1828, -1.3059, -0.3126,  0.0655,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  3\n",
      "All Q_current):  tensor([-0.4358, -0.0308,  1.4477, -0.5737], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.5737, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.0388,  0.0581,  0.1915, -1.3347, -0.3113,  0.0272,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([-0.4421, -0.0300,  1.4581, -0.5790], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.4421, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-0.3035)\n",
      "Q(target_action):  tensor(-0.7456, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.1719, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.0388,  0.0581,  0.1915, -1.3347, -0.3113,  0.0272,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([-0.4553, -0.0288,  1.5012, -0.5913], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.4553, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.0407,  0.0274,  0.1915, -1.3614, -0.3099,  0.0272,  1.0000,  0.0000])\n",
      "Next action:   3\n",
      "All Q_next:  tensor([-0.5296,  0.0119,  1.5473, -0.6216], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.6216, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(9.5702)\n",
      "Q(target_action):  tensor(8.9486, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(9.4039, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.0407,  0.0274,  0.1915, -1.3614, -0.3099,  0.0272,  1.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  3\n",
      "All Q_current):  tensor([-0.5067,  0.0220,  1.6304, -0.7199], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.7199, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.0424, -0.0023,  0.1567, -1.3183, -0.2953,  0.2859,  1.0000,  0.0000])\n",
      "Next action:   1\n",
      "All Q_next:  tensor([-0.4740,  0.0179,  1.5410, -0.6770], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(0.0179, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(6.8047)\n",
      "Q(target_action):  tensor(6.8226, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(7.5425, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.0424, -0.0023,  0.1567, -1.3183, -0.2953,  0.2859,  1.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  1\n",
      "All Q_current):  tensor([-0.4268, -0.0468,  1.5657, -0.6717], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.0468, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.0450, -0.0278,  0.0820, -0.7196, -0.1853,  4.9413,  1.0000,  0.0000])\n",
      "Next action:   3\n",
      "All Q_next:  tensor([-0.4852, -0.0159,  1.3656, -0.7759], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.7759, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-100.)\n",
      "Q(target_action):  tensor(-100., grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-99.9531, grad_fn=<SubBackward0>)\n",
      " \n",
      "Iteration number:  1\n",
      "State:  tensor([0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "State size:  torch.Size([8])\n",
      "Action:  1\n",
      "All Q_current):  tensor([-0.2304, -0.0966,  0.7699, -0.3622], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.0966, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0102,  1.4014, -0.5239, -0.2234,  0.0139,  0.1601,  0.0000,  0.0000])\n",
      "Next action:   3\n",
      "All Q_next:  tensor([-0.2977, -0.2111,  1.6084, -0.6246], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.6246, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-2.2632)\n",
      "Q(target_action):  tensor(-2.8877, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-2.7911, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0102,  1.4014, -0.5239, -0.2234,  0.0139,  0.1601,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  3\n",
      "All Q_current):  tensor([-0.2699, -0.2739,  1.6032, -0.5998], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.5998, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0153,  1.3958, -0.5143, -0.2499,  0.0200,  0.1215,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([-0.2733, -0.2753,  1.6169, -0.6050], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.2733, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-0.3084)\n",
      "Q(target_action):  tensor(-0.5817, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(0.0180, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0153,  1.3958, -0.5143, -0.2499,  0.0200,  0.1215,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([-0.2509, -0.3323,  1.6120, -0.5820], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.2509, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0204,  1.3896, -0.5143, -0.2765,  0.0260,  0.1216,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-0.2528, -0.3341,  1.6218, -0.5856], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(1.6218, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.2086)\n",
      "Q(target_action):  tensor(0.4131, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(0.6640, grad_fn=<SubBackward0>)\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State:  tensor([-0.0204,  1.3896, -0.5143, -0.2765,  0.0260,  0.1216,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-0.2222, -0.3837,  1.5964, -0.5599], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(1.5964, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0256,  1.3838, -0.5243, -0.2596,  0.0317,  0.1132,  0.0000,  0.0000])\n",
      "Next action:   3\n",
      "All Q_next:  tensor([-0.2211, -0.3821,  1.5876, -0.5567], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.5567, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-0.3997)\n",
      "Q(target_action):  tensor(-0.9564, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-2.5529, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0256,  1.3838, -0.5243, -0.2596,  0.0317,  0.1132,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  3\n",
      "All Q_current):  tensor([-0.1918, -0.4190,  1.5082, -0.5083], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.5083, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0307,  1.3773, -0.5162, -0.2864,  0.0357,  0.0805,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-0.1946, -0.4222,  1.5226, -0.5135], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(1.5226, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-0.3237)\n",
      "Q(target_action):  tensor(1.1989, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(1.7071, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0307,  1.3773, -0.5162, -0.2864,  0.0357,  0.0805,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-0.1663, -0.4497,  1.4194, -0.4553], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(1.4194, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0359,  1.3708, -0.5237, -0.2888,  0.0394,  0.0743,  0.0000,  0.0000])\n",
      "Next action:   1\n",
      "All Q_next:  tensor([-0.1668, -0.4501,  1.4199, -0.4554], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.4501, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-0.8090)\n",
      "Q(target_action):  tensor(-1.2591, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-2.6784, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0359,  1.3708, -0.5237, -0.2888,  0.0394,  0.0743,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  1\n",
      "All Q_current):  tensor([-0.1444, -0.4617,  1.2909, -0.3968], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.4617, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0412,  1.3637, -0.5334, -0.3153,  0.0451,  0.1131,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-0.1442, -0.4645,  1.2956, -0.3980], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(1.2956, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-2.0598)\n",
      "Q(target_action):  tensor(-0.7642, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.3025, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0412,  1.3637, -0.5334, -0.3153,  0.0451,  0.1131,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-0.1250, -0.4747,  1.1914, -0.3490], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(1.1914, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0464,  1.3569, -0.5329, -0.3019,  0.0509,  0.1163,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([-0.1243, -0.4717,  1.1835, -0.3471], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.1243, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(0.4960)\n",
      "Q(target_action):  tensor(0.3717, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.8197, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0464,  1.3569, -0.5329, -0.3019,  0.0509,  0.1163,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([-0.1017, -0.4778,  1.0832, -0.3028], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.1017, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0517,  1.3496, -0.5329, -0.3286,  0.0567,  0.1163,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-0.1024, -0.4821,  1.0918, -0.3054], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(1.0918, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.2216)\n",
      "Q(target_action):  tensor(-0.1298, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.0281, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0517,  1.3496, -0.5329, -0.3286,  0.0567,  0.1163,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-0.0834, -0.4869,  1.0052, -0.2683], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(1.0052, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0570,  1.3425, -0.5338, -0.3128,  0.0627,  0.1189,  0.0000,  0.0000])\n",
      "Next action:   3\n",
      "All Q_next:  tensor([-0.0830, -0.4832,  0.9972, -0.2666], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.2666, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(0.5228)\n",
      "Q(target_action):  tensor(0.2562, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.7490, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0570,  1.3425, -0.5338, -0.3128,  0.0627,  0.1189,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  3\n",
      "All Q_current):  tensor([-0.0669, -0.4846,  0.9117, -0.2286], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.2286, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0622,  1.3349, -0.5250, -0.3385,  0.0668,  0.0829,  0.0000,  0.0000])\n",
      "Next action:   1\n",
      "All Q_next:  tensor([-0.0684, -0.4911,  0.9279, -0.2345], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.4911, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-0.3030)\n",
      "Q(target_action):  tensor(-0.7941, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.5655, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0622,  1.3349, -0.5250, -0.3385,  0.0668,  0.0829,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  1\n",
      "All Q_current):  tensor([-0.0546, -0.4902,  0.8531, -0.2046], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.4902, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0675,  1.3267, -0.5371, -0.3653,  0.0734,  0.1316,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([-0.0533, -0.4948,  0.8542, -0.2037], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.0533, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-2.3845)\n",
      "Q(target_action):  tensor(-2.4377, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-1.9475, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0675,  1.3267, -0.5371, -0.3653,  0.0734,  0.1316,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([-0.0295, -0.5022,  0.7945, -0.1783], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.0295, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0728,  1.3179, -0.5372, -0.3919,  0.0800,  0.1316,  0.0000,  0.0000])\n",
      "Next action:   1\n",
      "All Q_next:  tensor([-0.0292, -0.5088,  0.8050, -0.1818], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.5088, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.3441)\n",
      "Q(target_action):  tensor(-1.8530, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-1.8235, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0728,  1.3179, -0.5372, -0.3919,  0.0800,  0.1316,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  1\n",
      "All Q_current):  tensor([-0.0211, -0.5077,  0.7476, -0.1591], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.5077, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0782,  1.3085, -0.5489, -0.4192,  0.0889,  0.1789,  0.0000,  0.0000])\n",
      "Next action:   3\n",
      "All Q_next:  tensor([-0.0195, -0.5126,  0.7479, -0.1582], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.1582, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-2.5905)\n",
      "Q(target_action):  tensor(-2.7487, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-2.2409, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0782,  1.3085, -0.5489, -0.4192,  0.0889,  0.1789,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  3\n",
      "All Q_current):  tensor([-0.0114, -0.5180,  0.6965, -0.1271], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.1271, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0835,  1.2984, -0.5408, -0.4459,  0.0962,  0.1462,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([-0.0117, -0.5265,  0.7132, -0.1334], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.0117, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-0.8126)\n",
      "Q(target_action):  tensor(-0.8244, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.6972, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0835,  1.2984, -0.5408, -0.4459,  0.0962,  0.1462,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([-3.1101e-04, -5.3210e-01,  6.6973e-01, -1.0999e-01],\n",
      "       grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.0003, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0889,  1.2878, -0.5408, -0.4725,  0.1035,  0.1462,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([ 3.0729e-04, -5.3930e-01,  6.7930e-01, -1.1312e-01],\n",
      "       grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(0.6793, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.4346)\n",
      "Q(target_action):  tensor(-0.7553, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.7550, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0889,  1.2878, -0.5408, -0.4725,  0.1035,  0.1462,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([ 0.0062, -0.5457,  0.6511, -0.0943], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(0.6511, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0942,  1.2774, -0.5432, -0.4657,  0.1110,  0.1490,  0.0000,  0.0000])\n",
      "Next action:   3\n",
      "All Q_next:  tensor([ 0.0057, -0.5432,  0.6471, -0.0943], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.0943, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(0.2269)\n",
      "Q(target_action):  tensor(0.1326, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.5185, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0942,  1.2774, -0.5432, -0.4657,  0.1110,  0.1490,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  3\n",
      "All Q_current):  tensor([ 0.0106, -0.5474,  0.6143, -0.0730], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.0730, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0995,  1.2663, -0.5312, -0.4912,  0.1160,  0.0999,  0.0000,  0.0000])\n",
      "Next action:   3\n",
      "All Q_next:  tensor([ 0.0105, -0.5573,  0.6342, -0.0799], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.0799, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-0.2659)\n",
      "Q(target_action):  tensor(-0.3459, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.2728, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0995,  1.2663, -0.5312, -0.4912,  0.1160,  0.0999,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  3\n",
      "All Q_current):  tensor([ 0.0149, -0.5612,  0.6049, -0.0607], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.0607, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.1047,  1.2547, -0.5234, -0.5168,  0.1194,  0.0681,  0.0000,  0.0000])\n",
      "Next action:   3\n",
      "All Q_next:  tensor([ 0.0153, -0.5704,  0.6212, -0.0659], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.0659, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-0.4635)\n",
      "Q(target_action):  tensor(-0.5295, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.4687, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.1047,  1.2547, -0.5234, -0.5168,  0.1194,  0.0681,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  3\n",
      "All Q_current):  tensor([ 0.0192, -0.5740,  0.5949, -0.0485], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.0485, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.1098,  1.2425, -0.5115, -0.5420,  0.1203,  0.0194,  0.0000,  0.0000])\n",
      "Next action:   3\n",
      "All Q_next:  tensor([ 0.0193, -0.5843,  0.6148, -0.0551], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.0551, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(0.0744)\n",
      "Q(target_action):  tensor(0.0193, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(0.0678, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.1098,  1.2425, -0.5115, -0.5420,  0.1203,  0.0194,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  3\n",
      "All Q_current):  tensor([ 0.0228, -0.5878,  0.5913, -0.0393], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.0393, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.1148,  1.2297, -0.5017, -0.5675,  0.1193, -0.0206,  0.0000,  0.0000])\n",
      "Next action:   3\n",
      "All Q_next:  tensor([ 0.0231, -0.5980,  0.6104, -0.0453], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.0453, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(0.0756)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q(target_action):  tensor(0.0302, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(0.0695, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.1148,  1.2297, -0.5017, -0.5675,  0.1193, -0.0206,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  3\n",
      "All Q_current):  tensor([ 0.0263, -0.6012,  0.5893, -0.0311], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.0311, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.1198,  1.2164, -0.4933, -0.5934,  0.1166, -0.0550,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([ 0.0267, -0.6115,  0.6077, -0.0366], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(0.0267, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(0.1163)\n",
      "Q(target_action):  tensor(0.1431, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(0.1741, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.1198,  1.2164, -0.4933, -0.5934,  0.1166, -0.0550,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([ 0.0283, -0.6144,  0.5880, -0.0223], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(0.0283, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.1247,  1.2024, -0.4933, -0.6200,  0.1138, -0.0550,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([ 0.0294, -0.6227,  0.5981, -0.0243], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(0.5981, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-0.4551)\n",
      "Q(target_action):  tensor(0.1431, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(0.1148, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.1247,  1.2024, -0.4933, -0.6200,  0.1138, -0.0550,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([ 0.0317, -0.6251,  0.5784, -0.0108], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(0.5784, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.1298,  1.1885, -0.5000, -0.6184,  0.1110, -0.0568,  0.0000,  0.0000])\n",
      "Next action:   1\n",
      "All Q_next:  tensor([ 0.0312, -0.6242,  0.5782, -0.0117], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.6242, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(1.0240)\n",
      "Q(target_action):  tensor(0.3998, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.1786, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.1298,  1.1885, -0.5000, -0.6184,  0.1110, -0.0568,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  1\n",
      "All Q_current):  tensor([ 0.0329, -0.6251,  0.5572,  0.0010], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.6251, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.1349,  1.1739, -0.5112, -0.6469,  0.1104, -0.0107,  0.0000,  0.0000])\n",
      "Next action:   1\n",
      "All Q_next:  tensor([ 0.0351, -0.6310,  0.5566,  0.0034], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.6310, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.5134)\n",
      "Q(target_action):  tensor(-2.1444, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-1.5192, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.1349,  1.1739, -0.5112, -0.6469,  0.1104, -0.0107,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  1\n",
      "All Q_current):  tensor([ 0.0366, -0.6317,  0.5379,  0.0148], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.6317, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.1401,  1.1588, -0.5213, -0.6742,  0.1120,  0.0302,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([ 0.0384, -0.6371,  0.5389,  0.0162], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(0.0384, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.5129)\n",
      "Q(target_action):  tensor(-1.4745, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.8427, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.1401,  1.1588, -0.5213, -0.6742,  0.1120,  0.0302,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([ 0.0471, -0.6423,  0.5246,  0.0262], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(0.0471, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.1453,  1.1430, -0.5213, -0.7009,  0.1135,  0.0302,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([ 0.0484, -0.6504,  0.5339,  0.0243], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(0.5339, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-0.7752)\n",
      "Q(target_action):  tensor(-0.2413, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.2885, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.1453,  1.1430, -0.5213, -0.7009,  0.1135,  0.0302,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([ 0.0542, -0.6557,  0.5258,  0.0325], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(0.5258, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.1505,  1.1281, -0.5265, -0.6637,  0.1153,  0.0357,  0.0000,  0.0000])\n",
      "Next action:   3\n",
      "All Q_next:  tensor([ 0.0515, -0.6424,  0.5138,  0.0315], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(0.0315, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(3.5630)\n",
      "Q(target_action):  tensor(3.5946, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(3.0688, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.1505,  1.1281, -0.5265, -0.6637,  0.1153,  0.0357,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  3\n",
      "All Q_current):  tensor([ 0.0574, -0.6523,  0.5455,  0.0125], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(0.0125, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.1557,  1.1126, -0.5180, -0.6901,  0.1153,  0.0017,  0.0000,  0.0000])\n",
      "Next action:   1\n",
      "All Q_next:  tensor([ 0.0584, -0.6632,  0.5623,  0.0077], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.6632, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-0.1412)\n",
      "Q(target_action):  tensor(-0.8044, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.8169, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.1557,  1.1126, -0.5180, -0.6901,  0.1153,  0.0017,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  1\n",
      "All Q_current):  tensor([ 0.0631, -0.6684,  0.5908, -0.0146], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.6684, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.1610,  1.0964, -0.5256, -0.7177,  0.1170,  0.0326,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([ 0.0651, -0.6748,  0.5946, -0.0145], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(0.0651, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.3327)\n",
      "Q(target_action):  tensor(-1.2676, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.5992, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.1610,  1.0964, -0.5256, -0.7177,  0.1170,  0.0326,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([ 0.0746, -0.6827,  0.6220, -0.0345], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(0.0746, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.1662,  1.0797, -0.5256, -0.7444,  0.1186,  0.0326,  0.0000,  0.0000])\n",
      "Next action:   1\n",
      "All Q_next:  tensor([ 0.0761, -0.6912,  0.6321, -0.0371], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.6912, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-0.7501)\n",
      "Q(target_action):  tensor(-1.4413, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-1.5159, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.1662,  1.0797, -0.5256, -0.7444,  0.1186,  0.0326,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  1\n",
      "All Q_current):  tensor([ 0.0721, -0.6906,  0.6527, -0.0554], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.6906, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.1715,  1.0623, -0.5344, -0.7722,  0.1220,  0.0689,  0.0000,  0.0000])\n",
      "Next action:   1\n",
      "All Q_next:  tensor([ 0.0742, -0.6966,  0.6560, -0.0555], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.6966, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.5349)\n",
      "Q(target_action):  tensor(-2.2315, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-1.5409, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.1715,  1.0623, -0.5344, -0.7722,  0.1220,  0.0689,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  1\n",
      "All Q_current):  tensor([ 0.0705, -0.6962,  0.6744, -0.0718], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.6962, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.1769,  1.0443, -0.5431, -0.7992,  0.1272,  0.1037,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([ 0.0725, -0.7019,  0.6777, -0.0722], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(0.6777, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.5784)\n",
      "Q(target_action):  tensor(-0.9006, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.2045, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.1769,  1.0443, -0.5431, -0.7992,  0.1272,  0.1037,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([ 0.0696, -0.7029,  0.6972, -0.0872], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(0.6972, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.1825,  1.0269, -0.5682, -0.7744,  0.1317,  0.0891,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([ 0.0665, -0.6936,  0.6877, -0.0867], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(0.6877, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(1.4578)\n",
      "Q(target_action):  tensor(2.1455, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(1.4483, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.1825,  1.0269, -0.5682, -0.7744,  0.1317,  0.0891,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([ 0.0639, -0.6946,  0.7050, -0.1000], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(0.7050, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.1884,  1.0103, -0.5908, -0.7364,  0.1357,  0.0797,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([ 0.0598, -0.6808,  0.6895, -0.0984], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(0.6895, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(2.4571)\n",
      "Q(target_action):  tensor(3.1466, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(2.4416, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.1884,  1.0103, -0.5908, -0.7364,  0.1357,  0.0797,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([ 0.0572, -0.6823,  0.7051, -0.1101], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(0.7051, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.1941,  0.9939, -0.5798, -0.7325,  0.1405,  0.0963,  0.0000,  0.0000])\n",
      "Next action:   1\n",
      "All Q_next:  tensor([ 0.0568, -0.6800,  0.7023, -0.1112], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.6800, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(1.7264)\n",
      "Q(target_action):  tensor(1.0464, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(0.3413, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.1941,  0.9939, -0.5798, -0.7325,  0.1405,  0.0963,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  1\n",
      "All Q_current):  tensor([ 0.0542, -0.6845,  0.7207, -0.1225], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.6845, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2000,  0.9768, -0.5908, -0.7604,  0.1475,  0.1413,  0.0000,  0.0000])\n",
      "Next action:   3\n",
      "All Q_next:  tensor([ 0.0559, -0.6906,  0.7237, -0.1227], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.1227, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-2.0508)\n",
      "Q(target_action):  tensor(-2.1734, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-1.4889, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2000,  0.9768, -0.5908, -0.7604,  0.1475,  0.1413,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Q_current):  tensor([ 0.0527, -0.7019,  0.7414, -0.1248], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.1248, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2057,  0.9591, -0.5790, -0.7856,  0.1522,  0.0932,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([ 0.0539, -0.7135,  0.7617, -0.1314], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(0.0539, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-0.1883)\n",
      "Q(target_action):  tensor(-0.1344, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.0096, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2057,  0.9591, -0.5790, -0.7856,  0.1522,  0.0932,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([ 0.0511, -0.7238,  0.7779, -0.1332], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(0.0511, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2115,  0.9408, -0.5790, -0.8123,  0.1569,  0.0932,  0.0000,  0.0000])\n",
      "Next action:   3\n",
      "All Q_next:  tensor([ 0.0527, -0.7324,  0.7896, -0.1367], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.1367, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-0.9642)\n",
      "Q(target_action):  tensor(-1.1009, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-1.1520, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2115,  0.9408, -0.5790, -0.8123,  0.1569,  0.0932,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  3\n",
      "All Q_current):  tensor([ 0.0405, -0.7422,  0.8013, -0.1305], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.1305, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2171,  0.9220, -0.5687, -0.8381,  0.1594,  0.0511,  0.0000,  0.0000])\n",
      "Next action:   1\n",
      "All Q_next:  tensor([ 0.0412, -0.7549,  0.8220, -0.1363], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.7549, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-0.1024)\n",
      "Q(target_action):  tensor(-0.8573, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.7268, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2171,  0.9220, -0.5687, -0.8381,  0.1594,  0.0511,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  1\n",
      "All Q_current):  tensor([ 0.0313, -0.7580,  0.8317, -0.1351], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.7580, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2228,  0.9025, -0.5768, -0.8651,  0.1636,  0.0838,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([ 0.0326, -0.7644,  0.8362, -0.1359], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(0.8362, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.3882)\n",
      "Q(target_action):  tensor(-0.5520, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(0.2060, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2228,  0.9025, -0.5768, -0.8651,  0.1636,  0.0838,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([ 0.0232, -0.7658,  0.8412, -0.1344], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(0.8412, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2288,  0.8833, -0.5991, -0.8563,  0.1671,  0.0708,  0.0000,  0.0000])\n",
      "Next action:   3\n",
      "All Q_next:  tensor([ 0.0214, -0.7621,  0.8381, -0.1347], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.1347, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(0.5345)\n",
      "Q(target_action):  tensor(0.3998, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.4414, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2288,  0.8833, -0.5991, -0.8563,  0.1671,  0.0708,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  3\n",
      "All Q_current):  tensor([ 0.0125, -0.7621,  0.8357, -0.1292], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.1292, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2347,  0.8634, -0.5898, -0.8819,  0.1688,  0.0328,  0.0000,  0.0000])\n",
      "Next action:   1\n",
      "All Q_next:  tensor([ 0.0128, -0.7741,  0.8552, -0.1340], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.7741, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-0.0144)\n",
      "Q(target_action):  tensor(-0.7885, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.6593, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2347,  0.8634, -0.5898, -0.8819,  0.1688,  0.0328,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  1\n",
      "All Q_current):  tensor([ 0.0057, -0.7692,  0.8524, -0.1339], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.7692, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2407,  0.8430, -0.6000, -0.9094,  0.1725,  0.0741,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([ 0.0067, -0.7750,  0.8555, -0.1335], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(0.8555, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.4431)\n",
      "Q(target_action):  tensor(-0.5876, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(0.1816, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2407,  0.8430, -0.6000, -0.9094,  0.1725,  0.0741,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([ 2.6729e-04, -7.6871e-01,  8.4986e-01, -1.3309e-01],\n",
      "       grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(0.8499, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2468,  0.8230, -0.6141, -0.8892,  0.1761,  0.0723,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-0.0018, -0.7605,  0.8402, -0.1327], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(0.8402, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(1.9759)\n",
      "Q(target_action):  tensor(2.8160, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(1.9662, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2468,  0.8230, -0.6141, -0.8892,  0.1761,  0.0723,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-0.0074, -0.7547,  0.8349, -0.1322], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(0.8349, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2531,  0.8038, -0.6346, -0.8542,  0.1795,  0.0685,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([-0.0106, -0.7421,  0.8189, -0.1308], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.0106, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(2.6617)\n",
      "Q(target_action):  tensor(2.6511, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(1.8162, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2531,  0.8038, -0.6346, -0.8542,  0.1795,  0.0685,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([-0.0292, -0.7439,  0.8358, -0.1334], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.0292, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2594,  0.7840, -0.6346, -0.8808,  0.1830,  0.0685,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-0.0288, -0.7524,  0.8478, -0.1359], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(0.8478, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-0.8035)\n",
      "Q(target_action):  tensor(0.0442, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(0.0734, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2594,  0.7840, -0.6346, -0.8808,  0.1830,  0.0685,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-0.0453, -0.7536,  0.8625, -0.1382], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(0.8625, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2659,  0.7643, -0.6527, -0.8752,  0.1859,  0.0597,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([-0.0468, -0.7518,  0.8612, -0.1386], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.0468, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(0.4337)\n",
      "Q(target_action):  tensor(0.3869, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.4756, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2659,  0.7643, -0.6527, -0.8752,  0.1859,  0.0597,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([-0.0575, -0.7509,  0.8687, -0.1398], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.0575, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2724,  0.7440, -0.6527, -0.9019,  0.1889,  0.0597,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([-0.0578, -0.7595,  0.8807, -0.1423], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.0578, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-0.7556)\n",
      "Q(target_action):  tensor(-0.8134, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.7558, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2724,  0.7440, -0.6527, -0.9019,  0.1889,  0.0597,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([-0.0674, -0.7584,  0.8878, -0.1435], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.0674, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2789,  0.7231, -0.6527, -0.9286,  0.1919,  0.0597,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([-0.0678, -0.7670,  0.8998, -0.1459], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.0678, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-0.7432)\n",
      "Q(target_action):  tensor(-0.8110, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.7436, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2789,  0.7231, -0.6527, -0.9286,  0.1919,  0.0597,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([-0.0765, -0.7660,  0.9062, -0.1471], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.0765, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2854,  0.7016, -0.6527, -0.9552,  0.1949,  0.0597,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([-0.0770, -0.7745,  0.9182, -0.1495], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.0770, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-0.7321)\n",
      "Q(target_action):  tensor(-0.8091, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.7327, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2854,  0.7016, -0.6527, -0.9552,  0.1949,  0.0597,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([-0.0848, -0.7735,  0.9242, -0.1506], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.0848, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2919,  0.6796, -0.6527, -0.9819,  0.1979,  0.0597,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([-0.0854, -0.7820,  0.9362, -0.1531], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.0854, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-0.7229)\n",
      "Q(target_action):  tensor(-0.8083, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.7235, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2919,  0.6796, -0.6527, -0.9819,  0.1979,  0.0597,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([-0.0923, -0.7811,  0.9417, -0.1541], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.0923, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2984,  0.6569, -0.6527, -1.0086,  0.2009,  0.0597,  0.0000,  0.0000])\n",
      "Next action:   3\n",
      "All Q_next:  tensor([-0.0931, -0.7894,  0.9538, -0.1566], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.1566, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-0.7159)\n",
      "Q(target_action):  tensor(-0.8725, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.7802, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2984,  0.6569, -0.6527, -1.0086,  0.2009,  0.0597,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  3\n",
      "All Q_current):  tensor([-0.1083, -0.7897,  0.9560, -0.1503], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.1503, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.3048,  0.6336, -0.6434, -1.0343,  0.2019,  0.0217,  0.0000,  0.0000])\n",
      "Next action:   3\n",
      "All Q_next:  tensor([-0.1099, -0.8004,  0.9752, -0.1552], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.1552, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(0.0271)\n",
      "Q(target_action):  tensor(-0.1281, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(0.0222, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.3048,  0.6336, -0.6434, -1.0343,  0.2019,  0.0217,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  3\n",
      "All Q_current):  tensor([-0.1239, -0.8006,  0.9771, -0.1492], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.1492, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.3112,  0.6098, -0.6361, -1.0594,  0.2015, -0.0094,  0.0000,  0.0000])\n",
      "Next action:   1\n",
      "All Q_next:  tensor([-0.1256, -0.8106,  0.9949, -0.1535], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.8106, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(0.1062)\n",
      "Q(target_action):  tensor(-0.7044, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.5552, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.3112,  0.6098, -0.6361, -1.0594,  0.2015, -0.0094,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  1\n",
      "All Q_current):  tensor([-0.1373, -0.8056,  0.9960, -0.1535], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.8056, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.3176,  0.5853, -0.6471, -1.0880,  0.2033,  0.0367,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([-0.1378, -0.8112,  1.0000, -0.1531], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.1378, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.3626)\n",
      "Q(target_action):  tensor(-1.5004, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.6948, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.3176,  0.5853, -0.6471, -1.0880,  0.2033,  0.0367,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([-0.1403, -0.8118,  1.0046, -0.1534], grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q(current_action):  tensor(-0.1403, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.3241,  0.5602, -0.6471, -1.1146,  0.2051,  0.0367,  0.0000,  0.0000])\n",
      "Next action:   3\n",
      "All Q_next:  tensor([-0.1417, -0.8201,  1.0168, -0.1557], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.1557, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-0.6104)\n",
      "Q(target_action):  tensor(-0.7662, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.6259, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.3241,  0.5602, -0.6471, -1.1146,  0.2051,  0.0367,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  3\n",
      "All Q_current):  tensor([-0.1522, -0.8215,  1.0185, -0.1495], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.1495, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.3305,  0.5346, -0.6398, -1.1402,  0.2055,  0.0062,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-0.1544, -0.8319,  1.0363, -0.1536], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(1.0363, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-0.0462)\n",
      "Q(target_action):  tensor(0.9901, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(1.1397, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.3305,  0.5346, -0.6398, -1.1402,  0.2055,  0.0062,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-0.1656, -0.8285,  1.0150, -0.1346], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(1.0150, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.3371,  0.5090, -0.6613, -1.1344,  0.2052, -0.0045,  0.0000,  0.0000])\n",
      "Next action:   3\n",
      "All Q_next:  tensor([-0.1672, -0.8267,  1.0138, -0.1349], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.1349, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(0.9498)\n",
      "Q(target_action):  tensor(0.8149, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.2001, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.3371,  0.5090, -0.6613, -1.1344,  0.2052, -0.0045,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  3\n",
      "All Q_current):  tensor([-0.1778, -0.8233,  0.9906, -0.1150], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.1150, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.3436,  0.4829, -0.6537, -1.1600,  0.2034, -0.0357,  0.0000,  0.0000])\n",
      "Next action:   1\n",
      "All Q_next:  tensor([-0.1802, -0.8332,  1.0075, -0.1184], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.8332, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(0.0859)\n",
      "Q(target_action):  tensor(-0.7473, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.6323, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.3436,  0.4829, -0.6537, -1.1600,  0.2034, -0.0357,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  1\n",
      "All Q_current):  tensor([-0.1881, -0.8234,  0.9858, -0.1072], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.8234, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-3.5025e-01,  4.5622e-01, -6.6258e-01, -1.1875e+00,  2.0345e-01,\n",
      "         1.4516e-04,  0.0000e+00,  0.0000e+00])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([-0.1892, -0.8291,  0.9906, -0.1067], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.1892, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.1035)\n",
      "Q(target_action):  tensor(-1.2927, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.4693, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-3.5025e-01,  4.5622e-01, -6.6258e-01, -1.1875e+00,  2.0345e-01,\n",
      "         1.4516e-04,  0.0000e+00,  0.0000e+00])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([-0.1903, -0.8240,  0.9732, -0.0966], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.1903, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-3.5688e-01,  4.2890e-01, -6.6258e-01, -1.2141e+00,  2.0346e-01,\n",
      "         1.4528e-04,  0.0000e+00,  0.0000e+00])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-0.1922, -0.8318,  0.9841, -0.0980], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(0.9841, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-0.6151)\n",
      "Q(target_action):  tensor(0.3690, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(0.5593, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-3.5688e-01,  4.2890e-01, -6.6258e-01, -1.2141e+00,  2.0346e-01,\n",
      "         1.4528e-04,  0.0000e+00,  0.0000e+00])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-0.1860, -0.8237,  0.9589, -0.0876], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(0.9589, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.3637,  0.4023, -0.6792, -1.1800,  0.2036,  0.0023,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-0.1862, -0.8114,  0.9431, -0.0870], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(0.9431, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(3.4211)\n",
      "Q(target_action):  tensor(4.3641, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(3.4053, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.3637,  0.4023, -0.6792, -1.1800,  0.2036,  0.0023,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-0.1809, -0.8030,  0.9203, -0.0776], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(0.9203, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.3705,  0.3765, -0.6864, -1.1494,  0.2042,  0.0122,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-0.1809, -0.7915,  0.9050, -0.0772], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(0.9050, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(3.3172)\n",
      "Q(target_action):  tensor(4.2222, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(3.3019, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.3705,  0.3765, -0.6864, -1.1494,  0.2042,  0.0122,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-0.1762, -0.7837,  0.8847, -0.0689], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(0.8847, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.3776,  0.3510, -0.7038, -1.1341,  0.2046,  0.0081,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([-0.1772, -0.7773,  0.8773, -0.0686], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.1772, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(1.3415)\n",
      "Q(target_action):  tensor(1.1642, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(0.2795, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.3776,  0.3510, -0.7038, -1.1341,  0.2046,  0.0081,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([-0.1768, -0.7727,  0.8640, -0.0617], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.1768, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.3846,  0.3249, -0.7038, -1.1607,  0.2050,  0.0081,  0.0000,  0.0000])\n",
      "Next action:   3\n",
      "All Q_next:  tensor([-0.1782, -0.7820,  0.8738, -0.0619], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.0619, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.1075)\n",
      "Q(target_action):  tensor(-1.1694, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.9926, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.3846,  0.3249, -0.7038, -1.1607,  0.2050,  0.0081,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  3\n",
      "All Q_current):  tensor([-0.1914, -0.7792,  0.8562, -0.0446], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.0446, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.3916,  0.2982, -0.6953, -1.1865,  0.2037, -0.0265,  0.0000,  0.0000])\n",
      "Next action:   1\n",
      "All Q_next:  tensor([-0.1932, -0.7910,  0.8705, -0.0458], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.7910, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-0.5499)\n",
      "Q(target_action):  tensor(-1.3409, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-1.2963, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.3916,  0.2982, -0.6953, -1.1865,  0.2037, -0.0265,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  1\n",
      "All Q_current):  tensor([-0.2025, -0.7748,  0.8535, -0.0440], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.7748, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.3986,  0.2708, -0.7030, -1.2138,  0.2039,  0.0047,  0.0000,  0.0000])\n",
      "Next action:   3\n",
      "All Q_next:  tensor([-0.2038, -0.7816,  0.8577, -0.0425], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.0425, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.7730)\n",
      "Q(target_action):  tensor(-1.8155, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-1.0407, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.3986,  0.2708, -0.7030, -1.2138,  0.2039,  0.0047,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  3\n",
      "All Q_current):  tensor([-0.2141, -0.7777,  0.8433, -0.0301], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.0301, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.4055,  0.2429, -0.6943, -1.2397,  0.2024, -0.0307,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-0.2163, -0.7890,  0.8564, -0.0306], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(0.8564, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-0.7831)\n",
      "Q(target_action):  tensor(0.0734, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(0.1035, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.4055,  0.2429, -0.6943, -1.2397,  0.2024, -0.0307,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-0.2262, -0.7847,  0.8401, -0.0173], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(0.8401, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.4125,  0.2155, -0.6965, -1.2217,  0.2014, -0.0198,  0.0000,  0.0000])\n",
      "Next action:   1\n",
      "All Q_next:  tensor([-0.2257, -0.7771,  0.8297, -0.0174], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.7771, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(1.9975)\n",
      "Q(target_action):  tensor(1.2204, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(0.3803, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.4125,  0.2155, -0.6965, -1.2217,  0.2014, -0.0198,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  1\n",
      "All Q_current):  tensor([-0.2340, -0.7799,  0.8264, -0.0075], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.7799, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.4196,  0.1873, -0.7074, -1.2495,  0.2026,  0.0250,  0.0000,  0.0000])\n",
      "Next action:   1\n",
      "All Q_next:  tensor([-0.2356, -0.7857,  0.8281, -0.0049], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.7857, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-2.5239)\n",
      "Q(target_action):  tensor(-3.3096, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-2.5296, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.4196,  0.1873, -0.7074, -1.2495,  0.2026,  0.0250,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  1\n",
      "All Q_current):  tensor([-0.2431, -0.7876,  0.8248,  0.0043], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.7876, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.4267,  0.1586, -0.7180, -1.2775,  0.2060,  0.0684,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-0.2449, -0.7934,  0.8263,  0.0070], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(0.8263, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-2.9057)\n",
      "Q(target_action):  tensor(-2.0793, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-1.2917, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.4267,  0.1586, -0.7180, -1.2775,  0.2060,  0.0684,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-0.2496, -0.8145,  0.8567,  0.0105], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(0.8567, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.4340,  0.1303, -0.7305, -1.2594,  0.2095,  0.0698,  0.0000,  0.0000])\n",
      "Next action:   3\n",
      "All Q_next:  tensor([-0.2499, -0.8069,  0.8461,  0.0109], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(0.0109, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(0.5120)\n",
      "Q(target_action):  tensor(0.5229, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.3338, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.4340,  0.1303, -0.7305, -1.2594,  0.2095,  0.0698,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  3\n",
      "All Q_current):  tensor([-0.2557, -0.8246,  0.8651,  0.0196], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(0.0196, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.4412,  0.1014, -0.7228, -1.2849,  0.2114,  0.0374,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-0.2586, -0.8361,  0.8775,  0.0200], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(0.8775, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.9988)\n",
      "Q(target_action):  tensor(-1.1213, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-1.1409, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.4412,  0.1014, -0.7228, -1.2849,  0.2114,  0.0374,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                       | 2/2000 [00:01<31:49,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Q_current):  tensor([-0.2595, -0.8560,  0.9217,  0.0107], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(0.9217, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.4484,  0.0727, -0.7216, -1.2741,  0.2139,  0.0499,  0.0000,  0.0000])\n",
      "Next action:   3\n",
      "All Q_next:  tensor([-0.2591, -0.8508,  0.9136,  0.0105], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(0.0105, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(0.2904)\n",
      "Q(target_action):  tensor(0.3009, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.6208, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.4484,  0.0727, -0.7216, -1.2741,  0.2139,  0.0499,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  3\n",
      "All Q_current):  tensor([-0.2623, -0.8662,  0.9382,  0.0117], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(0.0117, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.4555,  0.0435, -0.7119, -1.2992,  0.2144,  0.0095,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([-0.2652, -0.8789,  0.9531,  0.0116], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.2652, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-2.1350)\n",
      "Q(target_action):  tensor(-2.4002, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-2.4119, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.4555,  0.0435, -0.7119, -1.2992,  0.2144,  0.0095,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([-0.2326, -0.8869,  0.9838, -0.0147], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.2326, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.4626,  0.0137, -0.7119, -1.3259,  0.2149,  0.0095,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-0.2340, -0.8978,  0.9938, -0.0142], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(0.9938, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-2.9162)\n",
      "Q(target_action):  tensor(-1.9224, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-1.6898, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.4626,  0.0137, -0.7119, -1.3259,  0.2149,  0.0095,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-0.2258, -0.9146,  1.0519, -0.0411], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(1.0519, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.4699, -0.0155, -0.7314, -1.2963,  0.2153,  0.0085,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-0.2261, -0.9028,  1.0369, -0.0404], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(1.0369, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(0.5738)\n",
      "Q(target_action):  tensor(1.6106, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(0.5588, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.4699, -0.0155, -0.7314, -1.2963,  0.2153,  0.0085,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-0.2187, -0.9176,  1.0889, -0.0644], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(1.0889, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.4776, -0.0440, -0.7639, -1.2674,  0.2150, -0.0050,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([-0.2200, -0.9077,  1.0760, -0.0633], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.2200, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-0.3559)\n",
      "Q(target_action):  tensor(-0.5759, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-1.6648, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.4776, -0.0440, -0.7639, -1.2674,  0.2150, -0.0050,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([-0.1937, -0.9100,  1.0900, -0.0813], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.1937, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.4852, -0.0731, -0.7639, -1.2941,  0.2148, -0.0050,  0.0000,  1.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([-0.1945, -0.9645,  1.0928, -0.0546], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.1945, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(6.6252)\n",
      "Q(target_action):  tensor(6.4308, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(6.6244, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.4852, -0.0731, -0.7639, -1.2941,  0.2148, -0.0050,  0.0000,  1.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([-0.1995, -0.9735,  1.1006, -0.0726], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.1995, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.4919, -0.1010, -0.6561, -1.2367,  0.2024, -0.2525,  0.0000,  1.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([-0.1979, -0.9867,  1.1495, -0.0905], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.1979, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(10.3717)\n",
      "Q(target_action):  tensor(10.1738, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(10.3733, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.4919, -0.1010, -0.6561, -1.2367,  0.2024, -0.2525,  0.0000,  1.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([-0.2074, -0.9949,  1.1587, -0.1072], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-0.2074, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.4965, -0.1180, -0.0717, -0.4007,  0.0625, -4.2293,  0.0000,  1.0000])\n",
      "Next action:   3\n",
      "All Q_next:  tensor([-0.3180, -1.5276,  2.1928, -0.3820], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-0.3820, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-100.)\n",
      "Q(target_action):  tensor(-100., grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-99.7926, grad_fn=<SubBackward0>)\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████▉| 1998/2000 [24:11<00:01,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number:  1998\n",
      "State:  tensor([0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "State size:  torch.Size([8])\n",
      "Action:  3\n",
      "All Q_current):  tensor([ -96.5311, -106.7595, -127.2639, -122.9472], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-122.9472, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0045,  1.4037, -0.2201, -0.1723,  0.0034,  0.0153,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([-114.6745, -128.2934, -135.9356, -142.5062], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-114.6745, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-0.4479)\n",
      "Q(target_action):  tensor(-115.1224, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(7.8249, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0045,  1.4037, -0.2201, -0.1723,  0.0034,  0.0153,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([-115.7107, -128.5629, -136.2163, -142.4072], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-115.7107, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0067,  1.3993, -0.2201, -0.1990,  0.0042,  0.0153,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([-115.4373, -128.1976, -134.9475, -141.9774], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-115.4373, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.3484)\n",
      "Q(target_action):  tensor(-116.7857, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-1.0750, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0067,  1.3993, -0.2201, -0.1990,  0.0042,  0.0153,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([-116.3746, -128.4410, -135.2012, -141.8900], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-116.3746, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0088,  1.3942, -0.2201, -0.2256,  0.0049,  0.0153,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([-116.0964, -128.0692, -133.9232, -141.4567], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-116.0964, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.4209)\n",
      "Q(target_action):  tensor(-117.5173, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-1.1427, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0088,  1.3942, -0.2201, -0.2256,  0.0049,  0.0153,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([-116.9438, -128.2889, -134.1523, -141.3796], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-116.9438, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0110,  1.3885, -0.2201, -0.2523,  0.0057,  0.0153,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([-116.5900, -127.8516, -132.9020, -140.8520], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-116.5900, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.4713)\n",
      "Q(target_action):  tensor(-118.0613, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-1.1175, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0110,  1.3885, -0.2201, -0.2523,  0.0057,  0.0153,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([-117.3316, -128.0302, -133.1211, -140.7540], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-117.3316, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0132,  1.3822, -0.2201, -0.2790,  0.0065,  0.0153,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([-116.7945, -127.4390, -131.9530, -139.9971], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-116.7945, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.5037)\n",
      "Q(target_action):  tensor(-118.2982, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.9667, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0132,  1.3822, -0.2201, -0.2790,  0.0065,  0.0153,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([-117.4624, -127.5999, -132.1506, -139.9103], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-117.4624, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0154,  1.3754, -0.2201, -0.3056,  0.0072,  0.0153,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([-116.9163, -127.0004, -130.9736, -139.1467], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-116.9163, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.5211)\n",
      "Q(target_action):  tensor(-118.4374, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.9750, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0154,  1.3754, -0.2201, -0.3056,  0.0072,  0.0153,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([-117.5176, -127.1455, -131.1518, -139.0698], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-117.5176, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0176,  1.3679, -0.2201, -0.3323,  0.0080,  0.0153,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([-116.9626, -126.5375, -129.9658, -138.2994], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-116.9626, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.5261)\n",
      "Q(target_action):  tensor(-118.4887, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.9711, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0176,  1.3679, -0.2201, -0.3323,  0.0080,  0.0153,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([-117.5038, -126.6683, -130.1265, -138.2312], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-117.5038, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0198,  1.3598, -0.2201, -0.3590,  0.0087,  0.0153,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([-116.9399, -126.0519, -128.9315, -137.4537], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-116.9399, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.5210)\n",
      "Q(target_action):  tensor(-118.4609, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.9572, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0198,  1.3598, -0.2201, -0.3590,  0.0087,  0.0153,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([-117.4268, -126.1696, -129.0762, -137.3934], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-117.4268, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0220,  1.3511, -0.2201, -0.3856,  0.0095,  0.0153,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([-116.8541, -125.5447, -127.8720, -136.6087], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-116.8541, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.5077)\n",
      "Q(target_action):  tensor(-118.3618, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.9349, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0220,  1.3511, -0.2201, -0.3856,  0.0095,  0.0153,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([-117.2922, -125.6508, -128.0025, -136.5554], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-117.2922, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0242,  1.3419, -0.2201, -0.4123,  0.0103,  0.0153,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([-116.7106, -125.0173, -126.7891, -135.7632], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-116.7106, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.4875)\n",
      "Q(target_action):  tensor(-118.1981, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.9060, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0242,  1.3419, -0.2201, -0.4123,  0.0103,  0.0153,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([-117.1046, -125.1128, -126.9066, -135.7160], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-117.1046, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0264,  1.3320, -0.2201, -0.4390,  0.0110,  0.0153,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([-116.5142, -124.4707, -125.6839, -134.9162], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-116.5142, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.4618)\n",
      "Q(target_action):  tensor(-117.9760, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.8714, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0264,  1.3320, -0.2201, -0.4390,  0.0110,  0.0153,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([-116.8685, -124.5567, -125.7897, -134.8745], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-116.8685, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0286,  1.3215, -0.2201, -0.4657,  0.0118,  0.0153,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([-116.2694, -123.9060, -124.5578, -134.0670], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-116.2694, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.4313)\n",
      "Q(target_action):  tensor(-117.7007, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.8323, grad_fn=<SubBackward0>)\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State:  tensor([-0.0286,  1.3215, -0.2201, -0.4657,  0.0118,  0.0153,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([-116.5879, -123.9833, -124.6530, -134.0302], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-116.5879, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0308,  1.3104, -0.2201, -0.4923,  0.0126,  0.0153,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([-115.9801, -123.3239, -123.4117, -133.2149], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-115.9801, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.3971)\n",
      "Q(target_action):  tensor(-117.3772, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.7893, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0308,  1.3104, -0.2201, -0.4923,  0.0126,  0.0153,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([-116.2663, -123.3935, -123.4974, -133.1824], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-116.2663, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0330,  1.2987, -0.2201, -0.5190,  0.0133,  0.0153,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([-115.6499, -122.7254, -122.2467, -132.3591], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-115.6499, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.3595)\n",
      "Q(target_action):  tensor(-117.0094, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.7431, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0330,  1.2987, -0.2201, -0.5190,  0.0133,  0.0153,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([-115.9070, -122.7880, -122.3238, -132.3304], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-115.9070, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0352,  1.2865, -0.2201, -0.5457,  0.0141,  0.0153,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([-115.2820, -122.1112, -121.0637, -131.4991], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-115.2820, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.3193)\n",
      "Q(target_action):  tensor(-116.6013, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.6943, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0352,  1.2865, -0.2201, -0.5457,  0.0141,  0.0153,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([-115.5130, -122.1675, -121.1330, -131.4737], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-115.5130, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0374,  1.2736, -0.2201, -0.5723,  0.0149,  0.0153,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([-114.8794, -121.4820, -119.8635, -130.6343], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-114.8794, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.2767)\n",
      "Q(target_action):  tensor(-116.1562, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.6431, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0374,  1.2736, -0.2201, -0.5723,  0.0149,  0.0153,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([-115.0869, -121.5325, -119.9258, -130.6119], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-115.0869, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0396,  1.2601, -0.2201, -0.5990,  0.0156,  0.0153,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([-114.4448, -120.8383, -118.6468, -129.7643], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-114.4448, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.2323)\n",
      "Q(target_action):  tensor(-115.6770, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.5901, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0396,  1.2601, -0.2201, -0.5990,  0.0156,  0.0153,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([-114.6311, -120.8837, -118.7029, -129.7445], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-114.6311, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0417,  1.2460, -0.2201, -0.6257,  0.0164,  0.0153,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([-113.9805, -120.1807, -117.4144, -128.8886], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-113.9805, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.1861)\n",
      "Q(target_action):  tensor(-115.1665, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.5355, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0417,  1.2460, -0.2201, -0.6257,  0.0164,  0.0153,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([-114.1477, -120.2215, -117.4647, -128.8712], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-114.1477, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0439,  1.2314, -0.2201, -0.6523,  0.0171,  0.0153,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([-113.4886, -119.5098, -116.1668, -128.0070], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-113.4886, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.1385)\n",
      "Q(target_action):  tensor(-114.6271, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.4794, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0439,  1.2314, -0.2201, -0.6523,  0.0171,  0.0153,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([-113.6387, -119.5463, -116.2120, -127.9916], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-113.6387, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0461,  1.2161, -0.2202, -0.6790,  0.0179,  0.0153,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([-112.9712, -118.8259, -114.9046, -127.1190], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-112.9712, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.0897)\n",
      "Q(target_action):  tensor(-114.0609, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.4222, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0461,  1.2161, -0.2202, -0.6790,  0.0179,  0.0153,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([-113.1058, -118.8587, -114.9451, -127.1054], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-113.1058, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0483,  1.2002, -0.2202, -0.7057,  0.0187,  0.0153,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([-112.4300, -118.1294, -113.6282, -126.2245], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-112.4300, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.0399)\n",
      "Q(target_action):  tensor(-113.4698, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.3640, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0483,  1.2002, -0.2202, -0.7057,  0.0187,  0.0153,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([-112.5507, -118.1589, -113.6646, -126.2124], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-112.5507, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0505,  1.1837, -0.2202, -0.7323,  0.0194,  0.0153,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([-111.8665, -117.4209, -112.3382, -125.3230], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-111.8665, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-0.9891)\n",
      "Q(target_action):  tensor(-112.8556, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.3049, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0505,  1.1837, -0.2202, -0.7323,  0.0194,  0.0153,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([-111.9748, -117.4473, -112.3708, -125.3124], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-111.9748, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0527,  1.1667, -0.2202, -0.7590,  0.0202,  0.0153,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-111.2822, -116.7005, -111.0349, -124.4145], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-111.0349, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-0.9376)\n",
      "Q(target_action):  tensor(-111.9725, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(0.0023, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0527,  1.1667, -0.2202, -0.7590,  0.0202,  0.0153,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-111.3791, -116.7241, -111.0642, -124.4050], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-111.0642, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0547,  1.1505, -0.2048, -0.7197,  0.0218,  0.0321,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([-111.8359, -117.2406, -112.5596, -125.0873], grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q(next_state):  tensor(-111.8359, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(5.3481)\n",
      "Q(target_action):  tensor(-106.4879, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(4.5764, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0547,  1.1505, -0.2048, -0.7197,  0.0218,  0.0321,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([-112.1656, -117.2453, -112.3602, -125.0603], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-112.1656, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0568,  1.1336, -0.2048, -0.7473,  0.0234,  0.0321,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-111.4606, -116.4872, -110.9940, -124.1465], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-110.9940, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.1513)\n",
      "Q(target_action):  tensor(-112.1453, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(0.0203, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0568,  1.1336, -0.2048, -0.7473,  0.0234,  0.0321,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-111.7553, -116.4914, -110.8162, -124.1223], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-110.8162, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0590,  1.1178, -0.2191, -0.7062,  0.0244,  0.0205,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-112.4274, -117.1822, -112.2609, -125.0547], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-112.2609, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(4.7187)\n",
      "Q(target_action):  tensor(-107.5422, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(3.2740, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0590,  1.1178, -0.2191, -0.7062,  0.0244,  0.0205,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-112.6930, -117.1852, -112.0997, -125.0319], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-112.0997, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0611,  1.1022, -0.2130, -0.6904,  0.0258,  0.0279,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-112.7747, -117.2838, -112.5874, -125.1906], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-112.5874, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(2.7901)\n",
      "Q(target_action):  tensor(-109.7972, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(2.3025, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0611,  1.1022, -0.2130, -0.6904,  0.0258,  0.0279,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-113.0132, -117.2861, -112.4417, -125.1694], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-112.4417, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0632,  1.0867, -0.2134, -0.6914,  0.0273,  0.0286,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-112.8210, -117.0796, -112.2047, -124.9491], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-112.2047, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(0.9977)\n",
      "Q(target_action):  tensor(-111.2070, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(1.2347, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0632,  1.0867, -0.2134, -0.6914,  0.0273,  0.0286,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-113.0353, -117.0815, -112.0735, -124.9298], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-112.0735, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0655,  1.0719, -0.2288, -0.6541,  0.0281,  0.0161,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-113.6565, -117.7151, -113.3676, -125.7843], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-113.3676, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(4.1284)\n",
      "Q(target_action):  tensor(-109.2392, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(2.8343, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0655,  1.0719, -0.2288, -0.6541,  0.0281,  0.0161,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-113.8497, -117.7164, -113.2489, -125.7663], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-113.2489, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0677,  1.0581, -0.2276, -0.6148,  0.0291,  0.0199,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([-114.4190, -118.3282, -114.7161, -126.5708], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-114.4190, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(4.7037)\n",
      "Q(target_action):  tensor(-109.7153, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(3.5336, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0677,  1.0581, -0.2276, -0.6148,  0.0291,  0.0199,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([-114.7856, -118.3217, -114.4398, -126.5464], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-114.7856, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0700,  1.0437, -0.2276, -0.6415,  0.0301,  0.0199,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-114.1217, -117.6147, -113.1527, -125.6899], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-113.1527, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.1812)\n",
      "Q(target_action):  tensor(-114.3340, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(0.4517, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0700,  1.0437, -0.2276, -0.6415,  0.0301,  0.0199,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-114.4258, -117.6095, -112.9261, -125.6687], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-112.9261, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0722,  1.0297, -0.2246, -0.6197,  0.0313,  0.0250,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-114.6572, -117.8646, -113.6759, -126.0200], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-113.6759, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(3.1073)\n",
      "Q(target_action):  tensor(-110.5686, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(2.3576, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0722,  1.0297, -0.2246, -0.6197,  0.0313,  0.0250,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-114.9309, -117.8598, -113.4715, -126.0005], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-113.4715, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0743,  1.0164, -0.2117, -0.5912,  0.0333,  0.0400,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-115.2361, -118.2149, -114.5556, -126.4630], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-114.5556, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(3.9308)\n",
      "Q(target_action):  tensor(-110.6248, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(2.8468, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0743,  1.0164, -0.2117, -0.5912,  0.0333,  0.0400,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-115.4822, -118.2102, -114.3712, -126.4450], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-114.3712, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0763,  1.0036, -0.1976, -0.5713,  0.0361,  0.0560,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-115.6282, -118.3993, -115.1156, -126.6933], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-115.1156, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(3.0331)\n",
      "Q(target_action):  tensor(-112.0825, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(2.2887, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0763,  1.0036, -0.1976, -0.5713,  0.0361,  0.0560,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-115.8494, -118.3949, -114.9493, -126.6768], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-114.9493, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0783,  0.9911, -0.2003, -0.5544,  0.0389,  0.0556,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-116.0494, -118.6066, -115.5012, -126.9698], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-115.5012, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(2.1556)\n",
      "Q(target_action):  tensor(-113.3456, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(1.6038, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0783,  0.9911, -0.2003, -0.5544,  0.0389,  0.0556,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Q_current):  tensor([-116.2487, -118.6026, -115.3514, -126.9549], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-115.3514, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0804,  0.9794, -0.2127, -0.5226,  0.0412,  0.0469,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-116.7946, -119.1621, -116.4837, -127.6929], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-116.4837, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(3.1418)\n",
      "Q(target_action):  tensor(-113.3420, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(2.0094, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0804,  0.9794, -0.2127, -0.5226,  0.0412,  0.0469,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-116.9745, -119.1585, -116.3488, -127.6793], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-116.3488, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0824,  0.9678, -0.2039, -0.5131,  0.0441,  0.0576,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-116.9739, -119.1831, -116.6596, -127.7189], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-116.6596, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(1.7553)\n",
      "Q(target_action):  tensor(-114.9043, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(1.4445, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0824,  0.9678, -0.2039, -0.5131,  0.0441,  0.0576,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-117.1357, -119.1797, -116.5381, -127.7066], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-116.5381, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0845,  0.9564, -0.2139, -0.5053,  0.0466,  0.0501,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-117.2226, -119.2569, -116.6953, -127.8323], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-116.6953, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(0.9073)\n",
      "Q(target_action):  tensor(-115.7880, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(0.7500, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0845,  0.9564, -0.2139, -0.5053,  0.0466,  0.0501,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-117.3683, -119.2539, -116.5859, -127.8212], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-116.5859, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0867,  0.9454, -0.2275, -0.4910,  0.0486,  0.0397,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-117.5995, -119.4771, -116.9959, -128.1328], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-116.9959, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(1.3352)\n",
      "Q(target_action):  tensor(-115.6607, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(0.9252, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0867,  0.9454, -0.2275, -0.4910,  0.0486,  0.0397,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-117.7309, -119.4744, -116.8973, -128.1228], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-116.8973, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0891,  0.9348, -0.2417, -0.4718,  0.0501,  0.0291,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-118.0618, -119.8025, -117.5123, -128.5648], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-117.5123, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(1.6881)\n",
      "Q(target_action):  tensor(-115.8242, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(1.0730, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0891,  0.9348, -0.2417, -0.4718,  0.0501,  0.0291,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-118.1804, -119.8001, -117.4235, -128.5558], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-117.4235, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0917,  0.9241, -0.2537, -0.4756,  0.0510,  0.0192,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-118.0652, -119.6587, -117.0975, -128.4077], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-117.0975, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-0.2385)\n",
      "Q(target_action):  tensor(-117.3360, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(0.0875, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0917,  0.9241, -0.2537, -0.4756,  0.0510,  0.0192,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-118.1720, -119.6565, -117.0176, -128.3997], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-117.0176, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0943,  0.9138, -0.2653, -0.4572,  0.0516,  0.0113,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-118.4701, -119.9559, -117.6101, -128.8016], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-117.6101, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(1.6766)\n",
      "Q(target_action):  tensor(-115.9335, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(1.0841, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0943,  0.9138, -0.2653, -0.4572,  0.0516,  0.0113,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-118.5665, -119.9540, -117.5382, -128.7943], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-117.5382, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.0972,  0.9039, -0.2846, -0.4400,  0.0514, -0.0041,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-118.8944, -120.2678, -118.0516, -129.2222], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-118.0516, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(1.1355)\n",
      "Q(target_action):  tensor(-116.9162, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(0.6220, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.0972,  0.9039, -0.2846, -0.4400,  0.0514, -0.0041,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-118.9814, -120.2661, -117.9869, -129.2157], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-117.9869, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.1001,  0.8942, -0.2959, -0.4330,  0.0508, -0.0126,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-119.0688, -120.3419, -118.1153, -129.3367], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-118.1153, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(0.6550)\n",
      "Q(target_action):  tensor(-117.4603, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(0.5265, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.1001,  0.8942, -0.2959, -0.4330,  0.0508, -0.0126,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-119.1472, -120.3404, -118.0571, -129.3308], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-118.0571, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.1032,  0.8850, -0.3052, -0.4089,  0.0499, -0.0179,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-119.5471, -120.7528, -118.9078, -129.8691], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-118.9078, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(2.0865)\n",
      "Q(target_action):  tensor(-116.8213, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(1.2358, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.1032,  0.8850, -0.3052, -0.4089,  0.0499, -0.0179,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-119.6178, -120.7514, -118.8554, -129.8638], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-118.8554, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.1063,  0.8757, -0.3129, -0.4121,  0.0487, -0.0236,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-119.4960, -120.6130, -118.5844, -129.7115], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-118.5844, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-0.0138)\n",
      "Q(target_action):  tensor(-118.5983, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(0.2572, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.1063,  0.8757, -0.3129, -0.4121,  0.0487, -0.0236,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-119.5597, -120.6118, -118.5373, -129.7068], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-118.5373, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.1096,  0.8668, -0.3239, -0.3964,  0.0471, -0.0313,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-119.8173, -120.8685, -119.0381, -130.0525], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-119.0381, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(1.2586)\n",
      "Q(target_action):  tensor(-117.7795, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(0.7578, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.1096,  0.8668, -0.3239, -0.3964,  0.0471, -0.0313,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-119.8748, -120.8675, -118.9957, -130.0483], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-118.9957, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.1129,  0.8579, -0.3290, -0.3943,  0.0454, -0.0343,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-119.8385, -120.8253, -118.9562, -130.0132], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-118.9562, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(0.5378)\n",
      "Q(target_action):  tensor(-118.4184, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(0.5773, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.1129,  0.8579, -0.3290, -0.3943,  0.0454, -0.0343,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-119.8902, -120.8244, -118.9180, -130.0094], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-118.9180, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.1163,  0.8497, -0.3410, -0.3646,  0.0433, -0.0422,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-120.4263, -121.3733, -120.0024, -130.7214], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-120.0024, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(2.1136)\n",
      "Q(target_action):  tensor(-117.8888, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(1.0292, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.1163,  0.8497, -0.3410, -0.3646,  0.0433, -0.0422,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-120.4731, -121.3726, -119.9681, -130.7180], grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q(current_action):  tensor(-119.9681, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.1200,  0.8423, -0.3607, -0.3277,  0.0404, -0.0573,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([-120.7318, -121.8434, -121.7508, -131.1252], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-120.7318, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(1.8515)\n",
      "Q(target_action):  tensor(-118.8804, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(1.0878, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.1200,  0.8423, -0.3607, -0.3277,  0.0404, -0.0573,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([-120.8430, -121.8465, -121.6653, -131.1285], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-120.8430, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.1236,  0.8343, -0.3607, -0.3544,  0.0376, -0.0573,  0.0000,  0.0000])\n",
      "Next action:   3\n",
      "All Q_next:  tensor([-120.5092, -121.3706, -120.2361, -130.6593], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-130.6593, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-0.8081)\n",
      "Q(target_action):  tensor(-131.4675, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-10.6245, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.1236,  0.8343, -0.3607, -0.3544,  0.0376, -0.0573,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  3\n",
      "All Q_current):  tensor([-121.1643, -121.3540, -120.1241, -130.0063], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-130.0063, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.1272,  0.8258, -0.3525, -0.3809,  0.0330, -0.0902,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-121.0413, -120.9895, -118.8925, -129.8701], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-118.8925, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-0.1118)\n",
      "Q(target_action):  tensor(-119.0043, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(11.0020, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.1272,  0.8258, -0.3525, -0.3809,  0.0330, -0.0902,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-121.5852, -120.9544, -119.2639, -128.6418], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-119.2639, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.1308,  0.8173, -0.3547, -0.3768,  0.0285, -0.0908,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-121.5611, -120.9298, -119.3111, -128.6328], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-119.3111, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(1.0888)\n",
      "Q(target_action):  tensor(-118.2223, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(1.0416, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.1308,  0.8173, -0.3547, -0.3768,  0.0285, -0.0908,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-122.0498, -120.8982, -119.6450, -127.5284], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-119.6450, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.1344,  0.8089, -0.3551, -0.3717,  0.0240, -0.0899,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-122.0378, -120.8894, -119.7442, -127.5382], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-119.7442, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(1.2521)\n",
      "Q(target_action):  tensor(-118.4921, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(1.1529, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.1344,  0.8089, -0.3551, -0.3717,  0.0240, -0.0899,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-122.4769, -120.8609, -120.0444, -126.5450], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-120.0444, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.1381,  0.8005, -0.3705, -0.3752,  0.0188, -0.1040,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-122.2942, -120.6911, -119.8111, -126.3279], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-119.8111, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-0.3246)\n",
      "Q(target_action):  tensor(-120.1357, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.0913, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.1381,  0.8005, -0.3705, -0.3752,  0.0188, -0.1040,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-122.6746, -120.6582, -120.0933, -125.4208], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-120.0933, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.1419,  0.7922, -0.3717, -0.3663,  0.0136, -0.1043,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-122.6481, -120.6801, -120.4347, -125.4312], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-120.4347, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(1.5052)\n",
      "Q(target_action):  tensor(-118.9295, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(1.1638, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.1419,  0.7922, -0.3717, -0.3663,  0.0136, -0.1043,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-122.9896, -120.6505, -120.6883, -124.6158], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-120.6883, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.1458,  0.7846, -0.3849, -0.3367,  0.0078, -0.1159,  0.0000,  0.0000])\n",
      "Next action:   1\n",
      "All Q_next:  tensor([-123.1473, -120.9908, -122.1254, -124.8989], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-120.9908, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(2.0071)\n",
      "Q(target_action):  tensor(-118.9837, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(1.7045, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.1458,  0.7846, -0.3849, -0.3367,  0.0078, -0.1159,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  1\n",
      "All Q_current):  tensor([-123.4590, -121.0509, -122.2725, -124.1668], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-121.0509, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.1498,  0.7765, -0.3933, -0.3631,  0.0037, -0.0821,  0.0000,  0.0000])\n",
      "Next action:   1\n",
      "All Q_next:  tensor([-122.6025, -120.2783, -120.8340, -123.0668], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-120.2783, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.2820)\n",
      "Q(target_action):  tensor(-121.5603, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.5094, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.1498,  0.7765, -0.3933, -0.3631,  0.0037, -0.0821,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  1\n",
      "All Q_current):  tensor([-122.8809, -120.3322, -120.9655, -122.4117], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-120.3322, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.1538,  0.7677, -0.4018, -0.3896,  0.0013, -0.0482,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-122.0132, -119.5550, -119.5320, -121.3043], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-119.5320, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.4481)\n",
      "Q(target_action):  tensor(-120.9801, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.6479, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.1538,  0.7677, -0.4018, -0.3896,  0.0013, -0.0482,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-122.2638, -119.6357, -119.6192, -120.7187], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-119.6192, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.1578,  0.7593, -0.3989, -0.3742, -0.0010, -0.0454,  0.0000,  0.0000])\n",
      "Next action:   1\n",
      "All Q_next:  tensor([-122.3327, -119.7812, -120.3127, -120.8626], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-119.7812, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(1.7479)\n",
      "Q(target_action):  tensor(-118.0333, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(1.5859, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.1578,  0.7593, -0.3989, -0.3742, -0.0010, -0.0454,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  1\n",
      "All Q_current):  tensor([-122.5628, -119.9327, -120.3172, -120.3367], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-119.9327, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.1619,  0.7503, -0.4105, -0.4011, -0.0009,  0.0014,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-121.5033, -119.0566, -118.8929, -119.0040], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-118.8929, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.9238)\n",
      "Q(target_action):  tensor(-120.8166, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.8839, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.1619,  0.7503, -0.4105, -0.4011, -0.0009,  0.0014,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-121.7111, -119.2358, -118.8558, -118.5346], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-118.8558, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.1661,  0.7416, -0.4144, -0.3853, -0.0010, -0.0024,  0.0000,  0.0000])\n",
      "Next action:   3\n",
      "All Q_next:  tensor([-121.7616, -119.3898, -119.6272, -118.6573], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-118.6573, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(1.2534)\n",
      "Q(target_action):  tensor(-117.4039, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(1.4519, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.1661,  0.7416, -0.4144, -0.3853, -0.0010, -0.0024,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  3\n",
      "All Q_current):  tensor([-121.9559, -119.5528, -119.5278, -118.3189], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-118.3189, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.1701,  0.7323, -0.4025, -0.4116, -0.0035, -0.0502,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-122.2605, -119.4237, -118.1669, -118.6517], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-118.1669, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-0.4467)\n",
      "Q(target_action):  tensor(-118.6136, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error (Q-target_action - Q_current_action):  tensor(-0.2947, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.1701,  0.7323, -0.4025, -0.4116, -0.0035, -0.0502,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-122.4382, -119.5718, -118.0641, -118.3630], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-118.0641, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.1743,  0.7236, -0.4108, -0.3880, -0.0065, -0.0585,  0.0000,  0.0000])\n",
      "Next action:   3\n",
      "All Q_next:  tensor([-122.5501, -119.8281, -119.2071, -118.5822], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-118.5822, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(1.2203)\n",
      "Q(target_action):  tensor(-117.3618, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(0.7023, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.1743,  0.7236, -0.4108, -0.3880, -0.0065, -0.0585,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  3\n",
      "All Q_current):  tensor([-122.7135, -119.9620, -119.0826, -118.3630], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-118.3630, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.1783,  0.7143, -0.4017, -0.4147, -0.0112, -0.0949,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-122.8461, -119.7247, -117.6581, -118.4852], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-117.6581, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-0.9253)\n",
      "Q(target_action):  tensor(-118.5834, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.2204, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.1783,  0.7143, -0.4017, -0.4147, -0.0112, -0.0949,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-122.9951, -119.8462, -117.5360, -118.3002], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-117.5360, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.1826,  0.7052, -0.4173, -0.4053, -0.0167, -0.1107,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-122.9269, -119.8627, -117.9638, -118.2723], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-117.9638, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-0.5159)\n",
      "Q(target_action):  tensor(-118.4798, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.9437, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.1826,  0.7052, -0.4173, -0.4053, -0.0167, -0.1107,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-123.0609, -119.9719, -117.8540, -118.1059], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-117.8540, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.1869,  0.6964, -0.4259, -0.3909, -0.0227, -0.1200,  0.0000,  0.0000])\n",
      "Next action:   3\n",
      "All Q_next:  tensor([-123.0809, -120.0783, -118.5010, -118.1948], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-118.1948, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(0.2087)\n",
      "Q(target_action):  tensor(-117.9861, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.1321, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.1869,  0.6964, -0.4259, -0.3909, -0.0227, -0.1200,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  3\n",
      "All Q_current):  tensor([-123.2008, -120.1764, -118.4082, -118.0375], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-118.0375, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.1911,  0.6870, -0.4143, -0.4181, -0.0311, -0.1665,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-123.4720, -119.9982, -116.9200, -118.3223], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-116.9200, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.1180)\n",
      "Q(target_action):  tensor(-118.0381, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.0006, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.1911,  0.6870, -0.4143, -0.4181, -0.0311, -0.1665,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-123.5806, -120.0870, -116.8365, -118.1801], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-116.8365, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.1954,  0.6778, -0.4175, -0.4093, -0.0396, -0.1712,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-123.5569, -120.0965, -117.1394, -118.2059], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-117.1394, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(0.0099)\n",
      "Q(target_action):  tensor(-117.1296, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.2931, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.1954,  0.6778, -0.4175, -0.4093, -0.0396, -0.1712,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-123.6547, -120.1765, -117.0643, -118.0780], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-117.0643, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.1998,  0.6695, -0.4298, -0.3680, -0.0490, -0.1871,  0.0000,  0.0000])\n",
      "Next action:   3\n",
      "All Q_next:  tensor([-123.9776, -120.7046, -119.0457, -118.5978], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-118.5978, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(1.3158)\n",
      "Q(target_action):  tensor(-117.2820, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.2177, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.1998,  0.6695, -0.4298, -0.3680, -0.0490, -0.1871,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  3\n",
      "All Q_current):  tensor([-124.0645, -120.7763, -118.9881, -118.4700], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-118.4700, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2041,  0.6606, -0.4215, -0.3951, -0.0600, -0.2204,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-124.2050, -120.5470, -117.5539, -118.5866], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-117.5539, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.5972)\n",
      "Q(target_action):  tensor(-119.1511, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.6811, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2041,  0.6606, -0.4215, -0.3951, -0.0600, -0.2204,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-124.2883, -120.6140, -117.4728, -118.5124], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-117.4728, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2083,  0.6527, -0.4091, -0.3524, -0.0707, -0.2144,  0.0000,  0.0000])\n",
      "Next action:   3\n",
      "All Q_next:  tensor([-124.6889, -121.1400, -119.3088, -119.1342], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-119.1342, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(3.0359)\n",
      "Q(target_action):  tensor(-116.0982, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(1.3746, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2083,  0.6527, -0.4091, -0.3524, -0.0707, -0.2144,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  3\n",
      "All Q_current):  tensor([-124.7725, -121.2037, -119.1754, -119.1498], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-119.1498, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2124,  0.6442, -0.4008, -0.3790, -0.0831, -0.2474,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-124.9584, -121.0216, -117.8178, -119.3131], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-117.8178, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.7529)\n",
      "Q(target_action):  tensor(-119.5707, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.4209, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2124,  0.6442, -0.4008, -0.3790, -0.0831, -0.2474,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-125.0353, -121.0790, -117.6768, -119.3512], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-117.6768, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2165,  0.6360, -0.3948, -0.3626, -0.0954, -0.2468,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-125.1086, -121.1632, -118.1737, -119.5212], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-118.1737, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(0.6692)\n",
      "Q(target_action):  tensor(-117.5046, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(0.1722, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2165,  0.6360, -0.3948, -0.3626, -0.0954, -0.2468,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-125.1778, -121.2148, -118.0469, -119.5556], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-118.0469, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2206,  0.6287, -0.3943, -0.3279, -0.1082, -0.2549,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-125.5102, -121.6480, -119.5584, -120.0713], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-119.5584, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(1.3042)\n",
      "Q(target_action):  tensor(-118.2543, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.2074, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2206,  0.6287, -0.3943, -0.3279, -0.1082, -0.2549,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-125.5735, -121.6955, -119.4455, -120.1032], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-119.4455, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2245,  0.6220, -0.3811, -0.2954, -0.1208, -0.2515,  0.0000,  0.0000])\n",
      "Next action:   3\n",
      "All Q_next:  tensor([-125.9773, -122.1514, -120.8452, -120.6890], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-120.6890, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(1.9983)\n",
      "Q(target_action):  tensor(-118.6907, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(0.7548, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2245,  0.6220, -0.3811, -0.2954, -0.1208, -0.2515,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Q_current):  tensor([-126.0381, -122.1951, -120.7084, -120.7627], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-120.7627, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2284,  0.6148, -0.3711, -0.3229, -0.1354, -0.2920,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-126.2337, -121.9853, -119.2609, -120.9511], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-119.2609, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.9142)\n",
      "Q(target_action):  tensor(-121.1752, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.4125, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2284,  0.6148, -0.3711, -0.3229, -0.1354, -0.2920,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-126.2821, -122.0208, -119.1257, -121.0339], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-119.1257, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2323,  0.6078, -0.3777, -0.3114, -0.1506, -0.3058,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-126.4499, -122.1621, -119.4952, -121.2731], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-119.4952, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.0745)\n",
      "Q(target_action):  tensor(-120.5697, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-1.4439, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2323,  0.6078, -0.3777, -0.3114, -0.1506, -0.3058,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-126.5027, -122.1990, -119.3653, -121.3567], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-119.3653, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2362,  0.6017, -0.3785, -0.2762, -0.1666, -0.3201,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-126.8653, -122.6304, -120.8175, -121.9146], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-120.8175, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(0.6304)\n",
      "Q(target_action):  tensor(-120.1871, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.8218, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2362,  0.6017, -0.3785, -0.2762, -0.1666, -0.3201,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-126.9123, -122.6628, -120.6992, -121.9894], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-120.6992, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2399,  0.5962, -0.3512, -0.2462, -0.1820, -0.3076,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-127.4064, -123.1250, -121.8869, -122.6719], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-121.8869, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(2.5093)\n",
      "Q(target_action):  tensor(-119.3776, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(1.3216, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2399,  0.5962, -0.3512, -0.2462, -0.1820, -0.3076,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-127.4486, -123.1541, -121.7803, -122.7392], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-121.7803, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2435,  0.5910, -0.3481, -0.2329, -0.1978, -0.3152,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-127.6442, -123.3163, -122.2284, -123.0259], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-122.2284, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-0.5305)\n",
      "Q(target_action):  tensor(-122.7589, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.9786, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2435,  0.5910, -0.3481, -0.2329, -0.1978, -0.3152,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-127.6823, -123.3425, -122.1322, -123.0866], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-122.1322, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2472,  0.5858, -0.3510, -0.2328, -0.2141, -0.3257,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-127.6849, -123.2755, -121.9352, -123.1105], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-121.9352, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.8327)\n",
      "Q(target_action):  tensor(-123.7680, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-1.6357, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2472,  0.5858, -0.3510, -0.2328, -0.2141, -0.3257,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-127.7191, -123.2991, -121.8487, -123.1651], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-121.8487, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2508,  0.5811, -0.3385, -0.2084, -0.2305, -0.3296,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-128.1361, -123.6829, -122.8211, -123.7409], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-122.8211, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(0.7134)\n",
      "Q(target_action):  tensor(-122.1076, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.2590, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2508,  0.5811, -0.3385, -0.2084, -0.2305, -0.3296,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-128.1670, -123.7042, -122.7430, -123.7903], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-122.7430, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2540,  0.5772, -0.3081, -0.1788, -0.2465, -0.3196,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-128.6725, -124.1737, -123.9744, -124.4963], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-123.9744, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(2.4653)\n",
      "Q(target_action):  tensor(-121.5091, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(1.2339, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2540,  0.5772, -0.3081, -0.1788, -0.2465, -0.3196,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-128.6956, -124.1904, -123.9082, -124.5361], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-123.9082, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2570,  0.5741, -0.2862, -0.1391, -0.2627, -0.3226,  0.0000,  0.0000])\n",
      "Next action:   1\n",
      "All Q_next:  tensor([-129.4377, -124.8995, -125.6094, -125.5273], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-124.8995, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(2.0440)\n",
      "Q(target_action):  tensor(-122.8555, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(1.0527, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2570,  0.5741, -0.2862, -0.1391, -0.2627, -0.3226,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  1\n",
      "All Q_current):  tensor([-129.4613, -124.9700, -125.5012, -125.5634], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-124.9700, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2601,  0.5705, -0.2973, -0.1639, -0.2765, -0.2762,  0.0000,  0.0000])\n",
      "Next action:   1\n",
      "All Q_next:  tensor([-128.4053, -124.0897, -124.1096, -124.2768], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-124.0897, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-3.3289)\n",
      "Q(target_action):  tensor(-127.4186, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-2.4486, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2601,  0.5705, -0.2973, -0.1639, -0.2765, -0.2762,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  1\n",
      "All Q_current):  tensor([-128.4265, -124.1537, -124.0129, -124.3091], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-124.1537, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2633,  0.5663, -0.3064, -0.1890, -0.2884, -0.2380,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-127.4471, -123.3171, -122.6565, -123.1195], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-122.6565, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-3.0279)\n",
      "Q(target_action):  tensor(-125.6844, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-1.5307, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2633,  0.5663, -0.3064, -0.1890, -0.2884, -0.2380,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-127.4695, -123.4535, -122.5011, -123.1481], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-122.5011, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2661,  0.5626, -0.2716, -0.1674, -0.2996, -0.2256,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-127.7576, -123.7435, -123.5298, -123.6089], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-123.5298, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(2.8864)\n",
      "Q(target_action):  tensor(-120.6434, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(1.8577, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2661,  0.5626, -0.2716, -0.1674, -0.2996, -0.2256,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-127.7778, -123.8663, -123.3897, -123.6347], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-123.3897, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2689,  0.5592, -0.2671, -0.1529, -0.3116, -0.2392,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-128.1006, -124.1334, -123.9666, -124.0683], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-123.9666, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-0.1871)\n",
      "Q(target_action):  tensor(-124.1537, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.7640, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2689,  0.5592, -0.2671, -0.1529, -0.3116, -0.2392,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-128.1189, -124.2443, -123.8402, -124.0917], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-123.8402, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2713,  0.5566, -0.2289, -0.1165, -0.3232, -0.2320,  0.0000,  0.0000])\n",
      "Next action:   1\n",
      "All Q_next:  tensor([-128.7683, -124.8769, -125.5351, -124.9988], grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q(next_state):  tensor(-124.8769, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(3.7636)\n",
      "Q(target_action):  tensor(-121.1133, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(2.7269, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2713,  0.5566, -0.2289, -0.1165, -0.3232, -0.2320,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  1\n",
      "All Q_current):  tensor([-128.7926, -125.1189, -125.2972, -125.0208], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-125.1189, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2738,  0.5535, -0.2381, -0.1406, -0.3327, -0.1910,  0.0000,  0.0000])\n",
      "Next action:   3\n",
      "All Q_next:  tensor([-127.8309, -124.3091, -124.0021, -123.8456], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-123.8456, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-2.7851)\n",
      "Q(target_action):  tensor(-126.6306, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-1.5117, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2738,  0.5535, -0.2381, -0.1406, -0.3327, -0.1910,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  3\n",
      "All Q_current):  tensor([-127.8520, -124.6032, -123.7887, -123.7761], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-123.7761, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2762,  0.5497, -0.2300, -0.1692, -0.3441, -0.2261,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-127.5003, -124.0941, -122.6665, -123.4030], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-122.6665, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.8300)\n",
      "Q(target_action):  tensor(-124.4965, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.7204, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2762,  0.5497, -0.2300, -0.1692, -0.3441, -0.2261,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-127.5212, -124.3589, -122.4430, -123.3830], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-122.4430, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2785,  0.5460, -0.2213, -0.1689, -0.3556, -0.2313,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-127.4793, -124.2803, -122.4220, -123.3842], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-122.4220, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-0.5158)\n",
      "Q(target_action):  tensor(-122.9377, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.4948, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2785,  0.5460, -0.2213, -0.1689, -0.3556, -0.2313,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-127.4981, -124.5188, -122.2209, -123.3662], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-122.2209, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2804,  0.5428, -0.1821, -0.1454, -0.3667, -0.2211,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-127.8540, -124.8623, -123.3664, -123.9200], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-123.3664, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(3.3321)\n",
      "Q(target_action):  tensor(-120.0342, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(2.1866, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2804,  0.5428, -0.1821, -0.1454, -0.3667, -0.2211,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-127.8712, -125.0774, -123.1852, -123.9040], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-123.1852, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2822,  0.5396, -0.1673, -0.1457, -0.3778, -0.2215,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-127.7907, -124.9688, -123.1788, -123.8710], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-123.1788, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-0.0852)\n",
      "Q(target_action):  tensor(-123.2640, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.0788, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2822,  0.5396, -0.1673, -0.1457, -0.3778, -0.2215,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-127.8062, -125.1624, -123.0160, -123.8566], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-123.0160, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2840,  0.5363, -0.1646, -0.1467, -0.3894, -0.2328,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-127.7815, -125.0859, -122.9102, -123.8653], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-122.9102, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.1329)\n",
      "Q(target_action):  tensor(-124.0431, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-1.0271, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2840,  0.5363, -0.1646, -0.1467, -0.3894, -0.2328,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-127.7955, -125.2602, -122.7636, -123.8523], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-122.7636, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2855,  0.5331, -0.1459, -0.1450, -0.4010, -0.2317,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-127.7460, -125.1824, -122.8635, -123.8680], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-122.8635, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(0.2372)\n",
      "Q(target_action):  tensor(-122.6263, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(0.1373, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2855,  0.5331, -0.1459, -0.1450, -0.4010, -0.2317,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-127.7586, -125.3393, -122.7317, -123.8564], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-122.7317, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2867,  0.5304, -0.1031, -0.1240, -0.4120, -0.2205,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-128.0594, -125.6241, -123.7907, -124.3519], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-123.7907, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(3.2266)\n",
      "Q(target_action):  tensor(-120.5640, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(2.1677, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2867,  0.5304, -0.1031, -0.1240, -0.4120, -0.2205,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-128.0710, -125.7657, -123.6722, -124.3416], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-123.6722, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2874,  0.5279, -0.0655, -0.1166, -0.4224, -0.2070,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-128.0680, -125.7553, -124.1218, -124.4546], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-124.1218, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(1.6010)\n",
      "Q(target_action):  tensor(-122.5208, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(1.1514, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2874,  0.5279, -0.0655, -0.1166, -0.4224, -0.2070,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-128.0787, -125.8828, -124.0154, -124.4456], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-124.0154, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2878,  0.5258, -0.0228, -0.0921, -0.4323, -0.1997,  0.0000,  0.0000])\n",
      "Next action:   3\n",
      "All Q_next:  tensor([-128.4960, -126.2721, -125.2404, -125.0806], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-125.0806, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(2.7541)\n",
      "Q(target_action):  tensor(-122.3266, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(1.6889, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2878,  0.5258, -0.0228, -0.0921, -0.4323, -0.1997,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  3\n",
      "All Q_current):  tensor([-128.5120, -126.3894, -125.0693, -125.1746], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-125.1746, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2880,  0.5232, -0.0122, -0.1224, -0.4448, -0.2498,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-128.2397, -125.8971, -123.9113, -124.9135], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-123.9113, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-3.8744)\n",
      "Q(target_action):  tensor(-127.7857, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-2.6111, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2880,  0.5232, -0.0122, -0.1224, -0.4448, -0.2498,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-128.2631, -126.0055, -123.6417, -125.1543], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-123.6417, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2879,  0.5207,  0.0229, -0.1141, -0.4569, -0.2409,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-128.2750, -125.9552, -124.0537, -125.3266], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-124.0537, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-0.6204)\n",
      "Q(target_action):  tensor(-124.6741, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error (Q-target_action - Q_current_action):  tensor(-1.0323, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2879,  0.5207,  0.0229, -0.1141, -0.4569, -0.2409,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-128.2960, -126.0521, -123.8105, -125.5440], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-123.8105, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2875,  0.5184,  0.0518, -0.1038, -0.4689, -0.2395,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-128.3804, -126.0026, -124.2215, -125.8461], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-124.2215, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.2372)\n",
      "Q(target_action):  tensor(-125.4588, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-1.6483, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2875,  0.5184,  0.0518, -0.1038, -0.4689, -0.2395,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-128.3995, -126.0900, -124.0024, -126.0422], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-124.0024, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2867,  0.5165,  0.0869, -0.0913, -0.4806, -0.2349,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-128.2801, -125.8087, -124.7264, -126.3305], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-124.7264, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-2.2717)\n",
      "Q(target_action):  tensor(-126.9981, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-2.9958, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2867,  0.5165,  0.0869, -0.0913, -0.4806, -0.2349,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-128.3007, -125.8906, -124.5260, -126.5079], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-124.5260, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2855,  0.5147,  0.1278, -0.0834, -0.4917, -0.2230,  0.0000,  0.0000])\n",
      "Next action:   1\n",
      "All Q_next:  tensor([-127.5539, -124.9445, -125.4935, -126.4923], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-124.9445, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-3.8518)\n",
      "Q(target_action):  tensor(-128.7963, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-4.2704, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2855,  0.5147,  0.1278, -0.0834, -0.4917, -0.2230,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  1\n",
      "All Q_current):  tensor([-127.5439, -124.7861, -125.5122, -126.6418], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-124.7861, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2844,  0.5123,  0.1173, -0.1061, -0.5003, -0.1718,  0.0000,  0.0000])\n",
      "Next action:   1\n",
      "All Q_next:  tensor([-126.4949, -123.9292, -124.2387, -125.3450], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-123.9292, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.1906)\n",
      "Q(target_action):  tensor(-125.1198, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.3336, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2844,  0.5123,  0.1173, -0.1061, -0.5003, -0.1718,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  1\n",
      "All Q_current):  tensor([-126.4860, -123.7878, -124.2554, -125.4786], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-123.7878, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2834,  0.5095,  0.1075, -0.1286, -0.5065, -0.1224,  0.0000,  0.0000])\n",
      "Next action:   1\n",
      "All Q_next:  tensor([-125.4882, -122.9820, -122.9998, -124.2217], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-122.9820, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.2896)\n",
      "Q(target_action):  tensor(-124.2716, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.4838, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2834,  0.5095,  0.1075, -0.1286, -0.5065, -0.1224,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  1\n",
      "All Q_current):  tensor([-125.4803, -122.8558, -123.0147, -124.3410], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-122.8558, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2824,  0.5061,  0.1010, -0.1527, -0.5110, -0.0903,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-124.6743, -122.1675, -121.7103, -123.2836], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-121.7103, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.6851)\n",
      "Q(target_action):  tensor(-123.3954, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.5395, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2824,  0.5061,  0.1010, -0.1527, -0.5110, -0.0903,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-124.6707, -122.0828, -121.6996, -123.3914], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-121.6996, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2813,  0.5027,  0.1152, -0.1543, -0.5158, -0.0961,  0.0000,  0.0000])\n",
      "Next action:   1\n",
      "All Q_next:  tensor([-124.4189, -121.7200, -121.7679, -123.3606], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-121.7200, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.3718)\n",
      "Q(target_action):  tensor(-123.0918, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-1.3922, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2813,  0.5027,  0.1152, -0.1543, -0.5158, -0.0961,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  1\n",
      "All Q_current):  tensor([-124.4069, -121.5720, -121.8201, -123.4548], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-121.5720, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2803,  0.4987,  0.1077, -0.1775, -0.5186, -0.0571,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-123.5501, -120.8700, -120.5481, -122.3307], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-120.5481, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.4270)\n",
      "Q(target_action):  tensor(-121.9751, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.4031, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2803,  0.4987,  0.1077, -0.1775, -0.5186, -0.0571,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-123.5419, -120.7583, -120.5772, -122.4157], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-120.5772, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2789,  0.4947,  0.1358, -0.1757, -0.5213, -0.0540,  0.0000,  0.0000])\n",
      "Next action:   1\n",
      "All Q_next:  tensor([-123.0786, -120.1565, -121.0406, -122.3842], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-120.1565, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.6085)\n",
      "Q(target_action):  tensor(-121.7650, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-1.1878, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2789,  0.4947,  0.1358, -0.1757, -0.5213, -0.0540,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  1\n",
      "All Q_current):  tensor([-123.0636, -119.9964, -121.1185, -122.4581], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-119.9964, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2777,  0.4903,  0.1274, -0.1986, -0.5218, -0.0102,  0.0000,  0.0000])\n",
      "Next action:   1\n",
      "All Q_next:  tensor([-122.1871, -119.3054, -119.8567, -121.2900], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-119.3054, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.0118)\n",
      "Q(target_action):  tensor(-120.3172, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.3208, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2777,  0.4903,  0.1274, -0.1986, -0.5218, -0.0102,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  1\n",
      "All Q_current):  tensor([-122.1738, -119.1622, -119.9264, -121.3560], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-119.1622, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2765,  0.4853,  0.1172, -0.2209, -0.5198,  0.0413,  0.0000,  0.0000])\n",
      "Next action:   1\n",
      "All Q_next:  tensor([-121.2548, -118.4755, -118.6781, -120.1114], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-118.4755, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-0.7485)\n",
      "Q(target_action):  tensor(-119.2240, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.0618, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2765,  0.4853,  0.1172, -0.2209, -0.5198,  0.0413,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  1\n",
      "All Q_current):  tensor([-121.2430, -118.3474, -118.7403, -120.1704], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-118.3474, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2754,  0.4798,  0.1067, -0.2437, -0.5151,  0.0927,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-120.3616, -117.7082, -117.4697, -118.9342], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-117.4697, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-0.6238)\n",
      "Q(target_action):  tensor(-118.0934, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(0.2540, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2754,  0.4798,  0.1067, -0.2437, -0.5151,  0.0927,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-120.3495, -117.5812, -117.5358, -118.9865], grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q(current_action):  tensor(-117.5358, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2739,  0.4747,  0.1466, -0.2253, -0.5104,  0.0941,  0.0000,  0.0000])\n",
      "Next action:   1\n",
      "All Q_next:  tensor([-120.0904, -117.1547, -119.0045, -119.4182], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-117.1547, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(0.4093)\n",
      "Q(target_action):  tensor(-116.7454, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(0.7904, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2739,  0.4747,  0.1466, -0.2253, -0.5104,  0.0941,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  1\n",
      "All Q_current):  tensor([-120.0841, -117.0791, -119.0311, -119.4664], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-117.0791, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2724,  0.4690,  0.1370, -0.2484, -0.5034,  0.1406,  0.0000,  0.0000])\n",
      "Next action:   1\n",
      "All Q_next:  tensor([-119.2845, -116.5061, -117.7495, -118.3004], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-116.5061, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-0.2600)\n",
      "Q(target_action):  tensor(-116.7661, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(0.3130, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2724,  0.4690,  0.1370, -0.2484, -0.5034,  0.1406,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  1\n",
      "All Q_current):  tensor([-119.2789, -116.4382, -117.7732, -118.3435], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-116.4382, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2711,  0.4628,  0.1292, -0.2725, -0.4945,  0.1775,  0.0000,  0.0000])\n",
      "Next action:   1\n",
      "All Q_next:  tensor([-118.5982, -115.9452, -116.4650, -117.2936], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-115.9452, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-0.3237)\n",
      "Q(target_action):  tensor(-116.2689, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(0.1694, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2711,  0.4628,  0.1292, -0.2725, -0.4945,  0.1775,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  1\n",
      "All Q_current):  tensor([-118.5932, -115.8844, -116.4861, -117.3323], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-115.8844, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2697,  0.4561,  0.1212, -0.2965, -0.4838,  0.2155,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-117.9258, -115.4163, -115.1793, -116.2788], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-115.1793, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-0.1760)\n",
      "Q(target_action):  tensor(-115.3553, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(0.5291, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2697,  0.4561,  0.1212, -0.2965, -0.4838,  0.2155,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-117.9182, -115.3367, -115.2196, -116.3125], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-115.2196, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2679,  0.4498,  0.1754, -0.2744, -0.4723,  0.2290,  0.0000,  0.0000])\n",
      "Next action:   1\n",
      "All Q_next:  tensor([-117.4717, -114.7230, -117.1146, -116.7409], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-114.7230, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(0.9428)\n",
      "Q(target_action):  tensor(-113.7802, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(1.4394, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2679,  0.4498,  0.1754, -0.2744, -0.4723,  0.2290,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  1\n",
      "All Q_current):  tensor([-117.4738, -114.7191, -117.0927, -116.7738], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-114.7191, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2661,  0.4430,  0.1671, -0.2979, -0.4588,  0.2694,  0.0000,  0.0000])\n",
      "Next action:   1\n",
      "All Q_next:  tensor([-116.8901, -114.3417, -115.8772, -115.7960], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-114.3417, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(0.4011)\n",
      "Q(target_action):  tensor(-113.9407, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(0.7785, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2661,  0.4430,  0.1671, -0.2979, -0.4588,  0.2694,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  1\n",
      "All Q_current):  tensor([-116.8917, -114.3380, -115.8573, -115.8253], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-114.3380, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2644,  0.4357,  0.1577, -0.3214, -0.4431,  0.3139,  0.0000,  0.0000])\n",
      "Next action:   1\n",
      "All Q_next:  tensor([-117.2154, -114.7568, -115.5051, -115.8402], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-114.7568, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(0.6124)\n",
      "Q(target_action):  tensor(-114.1445, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(0.1935, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2644,  0.4357,  0.1577, -0.3214, -0.4431,  0.3139,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  1\n",
      "All Q_current):  tensor([-117.2169, -114.7536, -115.4871, -115.8666], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-114.7536, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2627,  0.4278,  0.1470, -0.3450, -0.4250,  0.3625,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-117.6389, -115.2838, -115.2236, -115.9597], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-115.2236, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(0.8392)\n",
      "Q(target_action):  tensor(-114.3844, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(0.3692, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2627,  0.4278,  0.1470, -0.3450, -0.4250,  0.3625,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-117.6378, -115.2633, -115.2222, -115.9827], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-115.2222, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2608,  0.4202,  0.1759, -0.3316, -0.4068,  0.3641,  0.0000,  0.0000])\n",
      "Next action:   1\n",
      "All Q_next:  tensor([-117.6505, -115.1969, -116.3153, -116.3931], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-115.1969, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(2.2282)\n",
      "Q(target_action):  tensor(-112.9687, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(2.2535, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2608,  0.4202,  0.1759, -0.3316, -0.4068,  0.3641,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  1\n",
      "All Q_current):  tensor([-117.6646, -115.2857, -116.2234, -116.4194], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-115.2857, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2590,  0.4121,  0.1663, -0.3558, -0.3865,  0.4069,  0.0000,  0.0000])\n",
      "Next action:   1\n",
      "All Q_next:  tensor([-118.0268, -115.7573, -115.8059, -116.4259], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-115.7573, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(1.0598)\n",
      "Q(target_action):  tensor(-114.6975, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(0.5882, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2590,  0.4121,  0.1663, -0.3558, -0.3865,  0.4069,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  1\n",
      "All Q_current):  tensor([-118.0394, -115.8379, -115.7229, -116.4495], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-115.8379, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2572,  0.4034,  0.1556, -0.3799, -0.3638,  0.4539,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-118.4980, -116.4191, -115.3955, -116.5340], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-115.3955, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(1.2848)\n",
      "Q(target_action):  tensor(-114.1106, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(1.7272, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2572,  0.4034,  0.1556, -0.3799, -0.3638,  0.4539,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-118.4979, -116.4085, -115.3908, -116.5512], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-115.3908, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2551,  0.3951,  0.1909, -0.3661, -0.3405,  0.4649,  0.0000,  0.0000])\n",
      "Next action:   1\n",
      "All Q_next:  tensor([-118.6056, -116.4231, -116.8370, -117.1439], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-116.4231, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(2.6114)\n",
      "Q(target_action):  tensor(-113.8118, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(1.5791, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2551,  0.3951,  0.1909, -0.3661, -0.3405,  0.4649,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  1\n",
      "All Q_current):  tensor([-118.6164, -116.4905, -116.7686, -117.1635], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-116.4905, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2530,  0.3862,  0.1795, -0.3900, -0.3148,  0.5152,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-119.1672, -117.1646, -116.5196, -117.3333], grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q(next_state):  tensor(-116.5196, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(1.7606)\n",
      "Q(target_action):  tensor(-114.7590, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(1.7315, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2530,  0.3862,  0.1795, -0.3900, -0.3148,  0.5152,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-119.1653, -117.1411, -116.5292, -117.3466], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-116.5292, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2505,  0.3779,  0.2216, -0.3613, -0.2883,  0.5291,  0.0000,  0.0000])\n",
      "Next action:   1\n",
      "All Q_next:  tensor([-119.5434, -117.4098, -118.9612, -118.4275], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-117.4098, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(3.7086)\n",
      "Q(target_action):  tensor(-113.7012, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(2.8280, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2505,  0.3779,  0.2216, -0.3613, -0.2883,  0.5291,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  1\n",
      "All Q_current):  tensor([-119.5625, -117.5295, -118.8540, -118.4476], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-117.5295, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2482,  0.3691,  0.2107, -0.3854, -0.2595,  0.5767,  0.0000,  0.0000])\n",
      "Next action:   1\n",
      "All Q_next:  tensor([-120.0928, -118.1235, -118.4771, -118.6307], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-118.1235, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(2.1843)\n",
      "Q(target_action):  tensor(-115.9392, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(1.5903, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2482,  0.3691,  0.2107, -0.3854, -0.2595,  0.5767,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  1\n",
      "All Q_current):  tensor([-120.1095, -118.2310, -118.3792, -118.6491], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-118.2310, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2459,  0.3598,  0.1995, -0.4097, -0.2282,  0.6249,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-120.6686, -118.8370, -117.9766, -118.8633], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-117.9766, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(2.3562)\n",
      "Q(target_action):  tensor(-115.6205, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(2.6106, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2459,  0.3598,  0.1995, -0.4097, -0.2282,  0.6249,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-120.6650, -118.8035, -117.9989, -118.8724], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-117.9989, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2434,  0.3509,  0.2183, -0.3902, -0.1968,  0.6275,  0.0000,  0.0000])\n",
      "Next action:   1\n",
      "All Q_next:  tensor([-121.1372, -119.2429, -119.3171, -119.6093], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-119.2429, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(4.5588)\n",
      "Q(target_action):  tensor(-114.6841, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(3.3148, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2434,  0.3509,  0.2183, -0.3902, -0.1968,  0.6275,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  1\n",
      "All Q_current):  tensor([-121.1583, -119.3809, -119.1963, -119.6276], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-119.3809, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2409,  0.3415,  0.2104, -0.4155, -0.1638,  0.6605,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-121.5110, -119.7821, -118.4233, -119.5909], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-118.4233, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(2.3243)\n",
      "Q(target_action):  tensor(-116.0989, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(3.2820, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2409,  0.3415,  0.2104, -0.4155, -0.1638,  0.6605,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-121.5059, -119.7397, -118.4551, -119.5975], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-118.4551, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2383,  0.3327,  0.2330, -0.3858, -0.1304,  0.6687,  0.0000,  0.0000])\n",
      "Next action:   1\n",
      "All Q_next:  tensor([-122.2160, -120.4007, -120.5264, -120.7337], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-120.4007, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(5.4155)\n",
      "Q(target_action):  tensor(-114.9852, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(3.4698, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2383,  0.3327,  0.2330, -0.3858, -0.1304,  0.6687,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  1\n",
      "All Q_current):  tensor([-122.2377, -120.5415, -120.4061, -120.7509], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-120.5415, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2357,  0.3234,  0.2244, -0.4124, -0.0953,  0.7027,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-122.6217, -120.9782, -119.5839, -120.7174], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-119.5839, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(2.5163)\n",
      "Q(target_action):  tensor(-117.0676, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(3.4739, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2357,  0.3234,  0.2244, -0.4124, -0.0953,  0.7027,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-122.6150, -120.9256, -119.6267, -120.7218], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-119.6267, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2331,  0.3148,  0.2225, -0.3783, -0.0606,  0.6923,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-123.5499, -121.8563, -121.1034, -121.7891], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-121.1034, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(7.0621)\n",
      "Q(target_action):  tensor(-114.0413, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(5.5854, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2331,  0.3148,  0.2225, -0.3783, -0.0606,  0.6923,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-123.5439, -121.8087, -121.1440, -121.7930], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-121.1440, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2304,  0.3066,  0.2292, -0.3646, -0.0259,  0.6953,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-124.1403, -122.4018, -121.9885, -122.4458], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-121.9885, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(4.8136)\n",
      "Q(target_action):  tensor(-117.1748, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(3.9692, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2304,  0.3066,  0.2292, -0.3646, -0.0259,  0.6953,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-124.1350, -122.3590, -122.0263, -122.4495], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-122.0263, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2279,  0.2988,  0.2232, -0.3484,  0.0085,  0.6877,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-124.8342, -123.0770, -122.5604, -123.0572], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-122.5604, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(3.9155)\n",
      "Q(target_action):  tensor(-118.6450, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(3.3813, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2279,  0.2988,  0.2232, -0.3484,  0.0085,  0.6877,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-124.8295, -123.0383, -122.5953, -123.0605], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-122.5953, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2255,  0.2919,  0.2060, -0.3057,  0.0421,  0.6718,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-126.0806, -124.3039, -124.1888, -124.3558], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-124.1888, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(1.5453)\n",
      "Q(target_action):  tensor(-122.6435, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.0482, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2255,  0.2919,  0.2060, -0.3057,  0.0421,  0.6718,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-126.0764, -124.2689, -124.2206, -124.3589], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-124.2206, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2230,  0.2859,  0.2152, -0.2702,  0.0763,  0.6845,  0.0000,  0.0000])\n",
      "Next action:   1\n",
      "All Q_next:  tensor([-127.1838, -125.3454, -126.5449, -125.8208], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-125.3454, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-0.7725)\n",
      "Q(target_action):  tensor(-126.1179, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-1.8973, grad_fn=<SubBackward0>)\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State:  tensor([-0.2230,  0.2859,  0.2152, -0.2702,  0.0763,  0.6845,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  1\n",
      "All Q_current):  tensor([-127.1641, -125.2075, -126.6616, -125.8162], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-125.2075, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2206,  0.2792,  0.2040, -0.2974,  0.1128,  0.7292,  0.0000,  0.0000])\n",
      "Next action:   1\n",
      "All Q_next:  tensor([-127.7621, -125.8607, -126.0683, -125.9913], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-125.8607, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-4.5269)\n",
      "Q(target_action):  tensor(-130.3877, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-5.1802, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2206,  0.2792,  0.2040, -0.2974,  0.1128,  0.7292,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  1\n",
      "All Q_current):  tensor([-127.7441, -125.7334, -126.1738, -125.9871], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-125.7334, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2182,  0.2720,  0.1927, -0.3248,  0.1515,  0.7749,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-128.2263, -126.2630, -125.7193, -126.1365], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-125.7193, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-4.8958)\n",
      "Q(target_action):  tensor(-130.6151, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-4.8816, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2182,  0.2720,  0.1927, -0.3248,  0.1515,  0.7749,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-128.2437, -126.4176, -125.5888, -126.1487], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-125.5888, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2161,  0.2651,  0.1782, -0.3115,  0.1900,  0.7697,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-128.7302, -126.9198, -126.2558, -126.6659], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-126.2558, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.6001)\n",
      "Q(target_action):  tensor(-127.8559, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-2.2672, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2161,  0.2651,  0.1782, -0.3115,  0.1900,  0.7697,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-128.7460, -127.0597, -126.1375, -126.6771], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-126.1375, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2139,  0.2585,  0.1750, -0.2993,  0.2289,  0.7773,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-129.3985, -127.7181, -127.0800, -127.3937], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-127.0800, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-2.3257)\n",
      "Q(target_action):  tensor(-129.4057, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-3.2682, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2139,  0.2585,  0.1750, -0.2993,  0.2289,  0.7773,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-129.4128, -127.8447, -126.9719, -127.4039], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-126.9719, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2119,  0.2521,  0.1645, -0.2912,  0.2678,  0.7788,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-129.8771, -128.3247, -127.5683, -127.8862], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-127.5683, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-2.3424)\n",
      "Q(target_action):  tensor(-129.9107, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-2.9388, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2119,  0.2521,  0.1645, -0.2912,  0.2678,  0.7788,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-129.8901, -128.4390, -127.4701, -127.8954], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-127.4701, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2100,  0.2465,  0.1497, -0.2540,  0.3073,  0.7894,  0.0000,  0.0000])\n",
      "Next action:   3\n",
      "All Q_next:  tensor([-131.1295, -129.7572, -129.7476, -129.3096], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-129.3096, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(0.2612)\n",
      "Q(target_action):  tensor(-129.0484, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-1.5783, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2100,  0.2465,  0.1497, -0.2540,  0.3073,  0.7894,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  3\n",
      "All Q_current):  tensor([-131.1295, -129.8557, -129.7362, -129.2101], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-129.2101, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2081,  0.2404,  0.1613, -0.2780,  0.3442,  0.7388,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-130.0540, -128.7163, -127.7259, -127.9838], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-127.7259, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-5.7922)\n",
      "Q(target_action):  tensor(-133.5182, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-4.3081, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2081,  0.2404,  0.1613, -0.2780,  0.3442,  0.7388,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-130.0814, -128.8167, -127.5096, -128.1815], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-127.5096, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2065,  0.2352,  0.1271, -0.2432,  0.3811,  0.7377,  0.0000,  0.0000])\n",
      "Next action:   3\n",
      "All Q_next:  tensor([-130.3316, -129.5044, -130.0033, -128.5404], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-128.5404, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(1.2163)\n",
      "Q(target_action):  tensor(-127.3241, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(0.1855, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2065,  0.2352,  0.1271, -0.2432,  0.3811,  0.7377,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  3\n",
      "All Q_current):  tensor([-130.3622, -129.5982, -129.7946, -128.7361], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-128.7361, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2048,  0.2293,  0.1377, -0.2675,  0.4157,  0.6913,  0.0000,  0.0000])\n",
      "Next action:   3\n",
      "All Q_next:  tensor([-129.4177, -128.5512, -127.7645, -127.6373], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-127.6373, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-5.5839)\n",
      "Q(target_action):  tensor(-133.2213, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-4.4851, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2048,  0.2293,  0.1377, -0.2675,  0.4157,  0.6913,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  3\n",
      "All Q_current):  tensor([-129.4459, -128.6359, -127.5789, -127.8153], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-127.8153, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2030,  0.2230,  0.1464, -0.2911,  0.4482,  0.6500,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-128.5412, -127.6424, -125.6698, -126.7593], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-125.6698, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-5.1973)\n",
      "Q(target_action):  tensor(-130.8671, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-3.0518, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2030,  0.2230,  0.1464, -0.2911,  0.4482,  0.6500,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-128.5918, -127.7306, -125.3559, -127.1216], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-125.3559, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2015,  0.2174,  0.1223, -0.2584,  0.4815,  0.6663,  0.0000,  0.0000])\n",
      "Next action:   3\n",
      "All Q_next:  tensor([-129.0483, -128.5990, -128.0789, -127.7204], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-127.7204, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(0.8871)\n",
      "Q(target_action):  tensor(-126.8334, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-1.4774, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2015,  0.2174,  0.1223, -0.2584,  0.4815,  0.6663,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  3\n",
      "All Q_current):  tensor([-129.0819, -128.6728, -127.8665, -127.9498], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-127.9498, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.1999,  0.2113,  0.1327, -0.2819,  0.5124,  0.6183,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-128.1086, -127.5889, -125.7950, -126.8188], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-125.7950, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-5.1390)\n",
      "Q(target_action):  tensor(-130.9340, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-2.9842, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.1999,  0.2113,  0.1327, -0.2819,  0.5124,  0.6183,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-128.1639, -127.6675, -125.4617, -127.2218], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-125.4617, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.1988,  0.2055,  0.0818, -0.2696,  0.5426,  0.6031,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-127.5442, -127.5145, -126.4917, -126.5319], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-126.4917, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(0.1688)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q(target_action):  tensor(-126.3229, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.8612, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.1988,  0.2055,  0.0818, -0.2696,  0.5426,  0.6031,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-127.5942, -127.5853, -126.1921, -126.8944], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-126.1921, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.1982,  0.1997,  0.0373, -0.2671,  0.5720,  0.5880,  0.0000,  0.0000])\n",
      "Next action:   3\n",
      "All Q_next:  tensor([-126.8866, -127.2611, -126.6319, -126.0740], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-126.0740, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.5852)\n",
      "Q(target_action):  tensor(-127.6591, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-1.4670, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.1982,  0.1997,  0.0373, -0.2671,  0.5720,  0.5880,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  3\n",
      "All Q_current):  tensor([-126.9186, -127.3181, -126.4337, -126.3026], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-126.3026, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.1975,  0.1934,  0.0465, -0.2896,  0.5990,  0.5399,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-125.9385, -126.2300, -124.3886, -125.1595], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-124.3886, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-4.5916)\n",
      "Q(target_action):  tensor(-128.9802, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-2.6776, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.1975,  0.1934,  0.0465, -0.2896,  0.5990,  0.5399,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-125.9900, -126.2928, -124.0865, -125.5372], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-124.0865, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.1972,  0.1875,  0.0016, -0.2736,  0.6259,  0.5389,  0.0000,  0.0000])\n",
      "Next action:   3\n",
      "All Q_next:  tensor([-125.6909, -126.4405, -125.5544, -125.2189], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-125.2189, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-0.5957)\n",
      "Q(target_action):  tensor(-125.8146, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-1.7281, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.1972,  0.1875,  0.0016, -0.2736,  0.6259,  0.5389,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  3\n",
      "All Q_current):  tensor([-125.7223, -126.4891, -125.3632, -125.4472], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-125.4472, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.1969,  0.1810,  0.0094, -0.2959,  0.6507,  0.4947,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-124.7741, -125.4401, -123.3913, -124.3324], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-123.3913, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-4.2964)\n",
      "Q(target_action):  tensor(-127.6877, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-2.2405, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.1969,  0.1810,  0.0094, -0.2959,  0.6507,  0.4947,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-124.8214, -125.4938, -123.1176, -124.6797], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-123.1176, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.1973,  0.1749, -0.0532, -0.2843,  0.6747,  0.4802,  0.0000,  0.0000])\n",
      "Next action:   3\n",
      "All Q_next:  tensor([-124.0572, -125.2540, -124.1255, -123.8087], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-123.8087, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.6274)\n",
      "Q(target_action):  tensor(-125.4361, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-2.3185, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.1973,  0.1749, -0.0532, -0.2843,  0.6747,  0.4802,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  3\n",
      "All Q_current):  tensor([-124.0791, -125.2912, -123.9865, -123.9726], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-123.9726, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.1976,  0.1682, -0.0467, -0.3069,  0.6967,  0.4407,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-123.1686, -124.2851, -122.0822, -122.8923], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-122.0822, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-3.9391)\n",
      "Q(target_action):  tensor(-126.0212, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-2.0487, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.1976,  0.1682, -0.0467, -0.3069,  0.6967,  0.4407,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-123.2061, -124.3279, -121.8656, -123.1675], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-121.8656, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.1981,  0.1614, -0.0724, -0.3118,  0.7188,  0.4420,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-122.7820, -124.1312, -122.0383, -122.6491], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-122.0383, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-3.0776)\n",
      "Q(target_action):  tensor(-125.1159, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-3.2503, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.1981,  0.1614, -0.0724, -0.3118,  0.7188,  0.4420,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-122.8157, -124.1696, -121.8432, -122.8965], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-121.8432, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.1991,  0.1547, -0.1166, -0.3065,  0.7408,  0.4403,  0.0000,  0.0000])\n",
      "Next action:   3\n",
      "All Q_next:  tensor([-122.3197, -124.0598, -122.6361, -122.3165], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-122.3165, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-2.9486)\n",
      "Q(target_action):  tensor(-125.2651, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-3.4218, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.1991,  0.1547, -0.1166, -0.3065,  0.7408,  0.4403,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  3\n",
      "All Q_current):  tensor([-122.3181, -124.0769, -122.6154, -122.3233], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-122.3233, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2001,  0.1475, -0.1100, -0.3296,  0.7609,  0.4026,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-121.4140, -123.0728, -120.7025, -121.2447], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-120.7025, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-3.6367)\n",
      "Q(target_action):  tensor(-124.3392, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-2.0159, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2001,  0.1475, -0.1100, -0.3296,  0.7609,  0.4026,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-121.4310, -123.0980, -120.5943, -121.3760], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-120.5943, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2014,  0.1399, -0.1485, -0.3460,  0.7802,  0.3858,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-120.4072, -122.3156, -119.7866, -120.1290], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-119.7866, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-4.8001)\n",
      "Q(target_action):  tensor(-124.5867, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-3.9924, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2014,  0.1399, -0.1485, -0.3460,  0.7802,  0.3858,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-120.4219, -122.3378, -119.6904, -120.2460], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-119.6904, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2029,  0.1323, -0.1674, -0.3473,  0.8002,  0.3987,  0.0000,  0.0000])\n",
      "Next action:   3\n",
      "All Q_next:  tensor([-120.2797, -122.3989, -120.2318, -120.0620], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-120.0620, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-2.9063)\n",
      "Q(target_action):  tensor(-122.9683, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-3.2779, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2029,  0.1323, -0.1674, -0.3473,  0.8002,  0.3987,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  3\n",
      "All Q_current):  tensor([-120.2621, -122.4020, -120.2897, -119.9663], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-119.9663, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2044,  0.1242, -0.1619, -0.3705,  0.8184,  0.3644,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-119.3737, -121.4191, -118.4214, -118.9032], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-118.4214, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-3.4232)\n",
      "Q(target_action):  tensor(-121.8446, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-1.8782, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2044,  0.1242, -0.1619, -0.3705,  0.8184,  0.3644,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-119.3754, -121.4313, -118.3913, -118.9317], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-118.3913, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2063,  0.1162, -0.1962, -0.3651,  0.8372,  0.3758,  0.0000,  0.0000])\n",
      "Next action:   3\n",
      "All Q_next:  tensor([-119.1441, -121.5277, -119.3584, -118.6582], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-118.6582, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-2.9443)\n",
      "Q(target_action):  tensor(-121.6024, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-3.2111, grad_fn=<SubBackward0>)\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State:  tensor([-0.2063,  0.1162, -0.1962, -0.3651,  0.8372,  0.3758,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  3\n",
      "All Q_current):  tensor([-119.1147, -121.5219, -119.4711, -118.4893], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-118.4893, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2081,  0.1076, -0.1905, -0.3875,  0.8540,  0.3366,  0.0000,  0.0000])\n",
      "Next action:   3\n",
      "All Q_next:  tensor([-118.1654, -120.4722, -117.5182, -117.3653], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-117.3653, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-3.1978)\n",
      "Q(target_action):  tensor(-120.5630, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-2.0737, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2081,  0.1076, -0.1905, -0.3875,  0.8540,  0.3366,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  3\n",
      "All Q_current):  tensor([-118.1395, -120.4672, -117.6186, -117.2163], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-117.2163, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2098,  0.0986, -0.1840, -0.4095,  0.8686,  0.2926,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-117.1265, -119.3387, -115.5405, -116.0289], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-115.5405, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-2.9590)\n",
      "Q(target_action):  tensor(-118.4994, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-1.2831, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2098,  0.0986, -0.1840, -0.4095,  0.8686,  0.2926,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-117.1150, -119.3405, -115.5758, -115.9712], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-115.5758, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2118,  0.0895, -0.2080, -0.4120,  0.8839,  0.3044,  0.0000,  0.0000])\n",
      "Next action:   3\n",
      "All Q_next:  tensor([-116.8425, -119.2877, -115.9851, -115.6342], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-115.6342, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-2.8942)\n",
      "Q(target_action):  tensor(-118.5284, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-2.9526, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2118,  0.0895, -0.2080, -0.4120,  0.8839,  0.3044,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  3\n",
      "All Q_current):  tensor([-116.8049, -119.2747, -116.1413, -115.4111], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-115.4111, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2137,  0.0799, -0.2007, -0.4329,  0.8965,  0.2522,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-115.6937, -118.0330, -113.9014, -114.1265], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-113.9014, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-2.6794)\n",
      "Q(target_action):  tensor(-116.5807, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-1.1696, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2137,  0.0799, -0.2007, -0.4329,  0.8965,  0.2522,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-115.6707, -118.0269, -113.9919, -113.9950], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-113.9919, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2161,  0.0703, -0.2427, -0.4347,  0.9092,  0.2551,  0.0000,  0.0000])\n",
      "Next action:   3\n",
      "All Q_next:  tensor([-115.0690, -117.7521, -114.3316, -113.2753], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-113.2753, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-3.5447)\n",
      "Q(target_action):  tensor(-116.8199, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-2.8281, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2161,  0.0703, -0.2427, -0.4347,  0.9092,  0.2551,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  3\n",
      "All Q_current):  tensor([-115.0219, -117.7326, -114.5298, -112.9968], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-112.9968, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2184,  0.0601, -0.2372, -0.4573,  0.9201,  0.2174,  0.0000,  1.0000])\n",
      "Next action:   3\n",
      "All Q_next:  tensor([-129.4721, -132.8309, -131.2877, -127.7361], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-127.7361, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(7.2220)\n",
      "Q(target_action):  tensor(-120.5141, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-7.5173, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2184,  0.0601, -0.2372, -0.4573,  0.9201,  0.2174,  0.0000,  1.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  3\n",
      "All Q_current):  tensor([-129.4010, -132.7645, -131.3816, -127.3791], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-127.3791, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2207,  0.0532, -0.2360, -0.3114,  0.9190, -0.0253,  0.0000,  1.0000])\n",
      "Next action:   3\n",
      "All Q_next:  tensor([-133.6667, -136.4816, -137.2279, -132.7551], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-132.7551, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(12.4609)\n",
      "Q(target_action):  tensor(-120.2941, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(7.0849, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2207,  0.0532, -0.2360, -0.3114,  0.9190, -0.0253,  0.0000,  1.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  3\n",
      "All Q_current):  tensor([-133.6891, -136.5046, -137.4088, -132.5313], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-132.5313, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2233,  0.0465, -0.2447, -0.2849,  0.8991, -0.3973,  0.0000,  1.0000])\n",
      "Next action:   3\n",
      "All Q_next:  tensor([-137.2761, -139.8106, -140.8173, -136.2299], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-136.2299, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(3.3713)\n",
      "Q(target_action):  tensor(-132.8586, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.3273, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2233,  0.0465, -0.2447, -0.2849,  0.8991, -0.3973,  0.0000,  1.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  3\n",
      "All Q_current):  tensor([-137.2994, -139.8347, -140.9863, -136.0243], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-136.0243, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2260,  0.0402, -0.2474, -0.2605,  0.8607, -0.7700,  0.0000,  1.0000])\n",
      "Next action:   3\n",
      "All Q_next:  tensor([-142.0789, -143.3910, -143.7685, -141.5670], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-141.5670, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(5.3000)\n",
      "Q(target_action):  tensor(-136.2669, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.2426, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2260,  0.0402, -0.2474, -0.2605,  0.8607, -0.7700,  0.0000,  1.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  3\n",
      "All Q_current):  tensor([-142.1151, -143.4217, -143.9213, -141.3948], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-141.3948, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2289,  0.0341, -0.2465, -0.2410,  0.8031, -1.1507,  0.0000,  1.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([-147.3160, -147.4817, -148.1422, -147.4747], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-147.3160, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(6.9864)\n",
      "Q(target_action):  tensor(-140.3296, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(1.0653, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2289,  0.0341, -0.2465, -0.2410,  0.8031, -1.1507,  0.0000,  1.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([-147.4391, -147.5182, -148.2835, -147.2378], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-147.4391, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2319,  0.0281, -0.2498, -0.2343,  0.7351, -1.3612,  0.0000,  1.0000])\n",
      "Next action:   1\n",
      "All Q_next:  tensor([-150.4715, -149.7180, -149.8086, -150.7262], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-149.7180, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(6.8201)\n",
      "Q(target_action):  tensor(-142.8980, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(4.5412, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2319,  0.0281, -0.2498, -0.2343,  0.7351, -1.3612,  0.0000,  1.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  1\n",
      "All Q_current):  tensor([-150.1918, -150.0547, -149.9649, -150.4644], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-150.0547, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2349,  0.0215, -0.2566, -0.2657,  0.6696, -1.3104,  0.0000,  0.0000])\n",
      "Next action:   1\n",
      "All Q_next:  tensor([-130.1109, -129.5400, -133.3260, -131.3746], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-129.5400, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-6.4054)\n",
      "Q(target_action):  tensor(-135.9454, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(14.1093, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2349,  0.0215, -0.2566, -0.2657,  0.6696, -1.3104,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  1\n",
      "All Q_current):  tensor([-129.8638, -129.6554, -133.4331, -131.1472], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-129.6554, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2381,  0.0143, -0.2649, -0.2974,  0.6066, -1.2591,  0.0000,  0.0000])\n",
      "Next action:   1\n",
      "All Q_next:  tensor([-129.4622, -128.8646, -130.2427, -130.4790], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-128.8646, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(3.1153)\n",
      "Q(target_action):  tensor(-125.7493, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(3.9061, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2381,  0.0143, -0.2649, -0.2974,  0.6066, -1.2591,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Q_current):  tensor([-129.2393, -128.9631, -130.3362, -130.2746], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-128.9631, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2413,  0.0064, -0.2714, -0.3268,  0.5454, -1.2253,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-129.1005, -128.3957, -127.4591, -129.9191], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-127.4591, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(3.1528)\n",
      "Q(target_action):  tensor(-124.3064, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(4.6567, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2413,  0.0064, -0.2714, -0.3268,  0.5454, -1.2253,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-128.8032, -128.1743, -127.7854, -129.6561], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-127.7854, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-2.4495e-01, -1.1802e-03, -3.0862e-01, -3.1604e-01,  4.8381e-01,\n",
      "        -1.2310e+00,  0.0000e+00,  1.0000e+00])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-147.3158, -146.9386, -141.0807, -146.9856], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-141.0807, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(13.8102)\n",
      "Q(target_action):  tensor(-127.2704, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(0.5150, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-2.4495e-01, -1.1802e-03, -3.0862e-01, -3.1604e-01,  4.8381e-01,\n",
      "        -1.2310e+00,  0.0000e+00,  1.0000e+00])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-146.8435, -146.5540, -141.3302, -146.5504], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-141.3302, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2488, -0.0083, -0.3336, -0.2972,  0.4228, -1.2196,  0.0000,  1.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-146.3036, -146.1750, -141.3189, -145.9593], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-141.3189, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(4.8876)\n",
      "Q(target_action):  tensor(-136.4313, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(4.8989, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2488, -0.0083, -0.3336, -0.2972,  0.4228, -1.2196,  0.0000,  1.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-145.8794, -145.8298, -141.5409, -145.5681], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-141.5409, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2529, -0.0153, -0.3445, -0.2950,  0.3623, -1.2112,  0.0000,  0.0000])\n",
      "Next action:   1\n",
      "All Q_next:  tensor([-127.2116, -126.8265, -127.4667, -128.0183], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-126.8265, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-5.3500)\n",
      "Q(target_action):  tensor(-132.1765, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(9.3644, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2529, -0.0153, -0.3445, -0.2950,  0.3623, -1.2112,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  1\n",
      "All Q_current):  tensor([-127.1877, -127.2598, -127.1703, -127.9922], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-127.2598, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2569, -0.0229, -0.3524, -0.3242,  0.3036, -1.1743,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-126.9839, -126.6579, -124.3387, -127.5620], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-124.3387, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(2.8498)\n",
      "Q(target_action):  tensor(-121.4889, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(5.7709, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2569, -0.0229, -0.3524, -0.3242,  0.3036, -1.1743,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-126.8516, -126.6829, -124.3614, -127.4485], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-124.3614, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2613, -0.0302, -0.3812, -0.3136,  0.2443, -1.1844,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-126.5323, -126.4352, -124.3490, -127.1091], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-124.3490, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(3.6313)\n",
      "Q(target_action):  tensor(-120.7177, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(3.6438, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2613, -0.0302, -0.3812, -0.3136,  0.2443, -1.1844,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-126.4138, -126.4583, -124.3701, -127.0076], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-124.3701, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2659, -0.0369, -0.3952, -0.2920,  0.1854, -1.1798,  1.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-137.0021, -136.7212, -135.2432, -138.3458], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-135.2432, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(15.2866)\n",
      "Q(target_action):  tensor(-119.9566, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(4.4135, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2659, -0.0369, -0.3952, -0.2920,  0.1854, -1.1798,  1.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-136.9001, -136.7671, -135.3500, -138.2645], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-135.3500, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2703, -0.0427, -0.4033, -0.2539,  0.1465, -0.7746,  1.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-131.9660, -132.9175, -131.9610, -132.6162], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-131.9610, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(4.5426)\n",
      "Q(target_action):  tensor(-127.4184, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(7.9317, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2703, -0.0427, -0.4033, -0.2539,  0.1465, -0.7746,  1.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-131.8715, -132.9491, -132.0260, -132.5397], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-132.0260, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2748, -0.0476, -0.4271, -0.2151,  0.1269, -0.3913,  1.0000,  0.0000])\n",
      "Next action:   3\n",
      "All Q_next:  tensor([-126.5957, -129.0296, -129.5467, -126.5431], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-126.5431, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(0.9731)\n",
      "Q(target_action):  tensor(-125.5700, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(6.4560, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2748, -0.0476, -0.4271, -0.2151,  0.1269, -0.3913,  1.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  3\n",
      "All Q_current):  tensor([-126.7255, -129.1986, -129.2268, -127.0560], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-127.0560, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2789, -0.0525, -0.4132, -0.2184,  0.1246, -0.0467,  1.0000,  0.0000])\n",
      "Next action:   3\n",
      "All Q_next:  tensor([-122.3924, -125.8887, -125.9271, -121.8405], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-121.8405, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(0.7852)\n",
      "Q(target_action):  tensor(-121.0553, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(6.0008, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2789, -0.0525, -0.4132, -0.2184,  0.1246, -0.0467,  1.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  3\n",
      "All Q_current):  tensor([-122.4248, -125.9739, -125.6860, -122.2053], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-122.2053, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2828, -0.0575, -0.4019, -0.2227,  0.1400,  0.3090,  1.0000,  1.0000])\n",
      "Next action:   3\n",
      "All Q_next:  tensor([-130.8166, -136.1776, -137.6129, -130.1047], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-130.1047, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(8.7391)\n",
      "Q(target_action):  tensor(-121.3657, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(0.8396, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2828, -0.0575, -0.4019, -0.2227,  0.1400,  0.3090,  1.0000,  1.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  3\n",
      "All Q_current):  tensor([-130.7173, -136.1637, -137.3578, -130.3160], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-130.3160, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2865, -0.0621, -0.3891, -0.2031,  0.1541,  0.2791,  1.0000,  1.0000])\n",
      "Next action:   3\n",
      "All Q_next:  tensor([-131.0885, -136.5018, -137.8683, -130.7786], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-130.7786, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(0.1737)\n",
      "Q(target_action):  tensor(-130.6049, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.2889, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2865, -0.0621, -0.3891, -0.2031,  0.1541,  0.2791,  1.0000,  1.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  3\n",
      "All Q_current):  tensor([-130.9982, -136.4886, -137.6373, -130.9685], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-130.9685, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2902, -0.0662, -0.3782, -0.1839,  0.1665,  0.2497,  1.0000,  1.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([-131.8768, -137.3063, -138.7491, -132.0075], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-131.8768, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(0.1071)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|███████████████████████████████████▉| 1999/2000 [24:14<00:01,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q(target_action):  tensor(-131.7698, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.8012, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2902, -0.0662, -0.3782, -0.1839,  0.1665,  0.2497,  1.0000,  1.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([-131.7228, -137.2750, -138.5221, -132.2123], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-131.7228, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2938, -0.0699, -0.3768, -0.1660,  0.1798,  0.2652,  1.0000,  1.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([-132.2386, -137.7673, -139.2965, -132.8726], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-132.2386, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-0.8846)\n",
      "Q(target_action):  tensor(-133.1232, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-1.4004, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([-0.2938, -0.0699, -0.3768, -0.1660,  0.1798,  0.2652,  1.0000,  1.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([-132.0992, -137.7392, -139.0919, -133.0579], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-132.0992, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([-0.2974, -0.0728, -0.3779, -0.1356,  0.2015,  0.4228,  1.0000,  1.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([-134.8787, -140.1818, -143.5011, -136.8648], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-134.8787, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-100.)\n",
      "Q(target_action):  tensor(-100., grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(32.0992, grad_fn=<SubBackward0>)\n",
      " \n",
      "Iteration number:  1999\n",
      "State:  tensor([0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "State size:  torch.Size([8])\n",
      "Action:  3\n",
      "All Q_current):  tensor([-108.5627, -111.7751, -119.6287, -114.5420], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-114.5420, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.0157,  1.4175,  0.7998,  0.1323, -0.0198, -0.2161,  0.0000,  0.0000])\n",
      "Next action:   1\n",
      "All Q_next:  tensor([-118.1368, -115.7633, -140.5582, -133.7188], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-115.7633, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.7983)\n",
      "Q(target_action):  tensor(-117.5617, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-3.0197, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.0157,  1.4175,  0.7998,  0.1323, -0.0198, -0.2161,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  1\n",
      "All Q_current):  tensor([-116.3357, -115.1795, -139.9400, -133.5683], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-115.1795, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.0235,  1.4198,  0.7892,  0.1058, -0.0285, -0.1732,  0.0000,  0.0000])\n",
      "Next action:   1\n",
      "All Q_next:  tensor([-115.6940, -114.7800, -138.9875, -132.5694], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-114.7800, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(0.2978)\n",
      "Q(target_action):  tensor(-114.4822, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(0.6973, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.0235,  1.4198,  0.7892,  0.1058, -0.0285, -0.1732,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  1\n",
      "All Q_current):  tensor([-114.0884, -114.2571, -138.4337, -132.4319], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-114.2571, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.0312,  1.4216,  0.7814,  0.0790, -0.0356, -0.1421,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([-113.4773, -113.8612, -137.5259, -131.4813], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-113.4773, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(0.1484)\n",
      "Q(target_action):  tensor(-113.3289, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(0.9282, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.0312,  1.4216,  0.7814,  0.0790, -0.0356, -0.1421,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([-112.0848, -113.3484, -137.0275, -131.3542], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-112.0848, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.0390,  1.4228,  0.7815,  0.0524, -0.0427, -0.1421,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([-111.5310, -112.9264, -136.2358, -130.5420], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-111.5310, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-0.6262)\n",
      "Q(target_action):  tensor(-112.1572, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.0724, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.0390,  1.4228,  0.7815,  0.0524, -0.0427, -0.1421,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([-110.2899, -112.4690, -135.7913, -130.4283], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-110.2899, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.0467,  1.4234,  0.7815,  0.0257, -0.0498, -0.1420,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([-109.7395, -112.0477, -134.9990, -129.6138], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-109.7395, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-0.6608)\n",
      "Q(target_action):  tensor(-110.4003, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.1104, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.0467,  1.4234,  0.7815,  0.0257, -0.0498, -0.1420,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([-108.6328, -111.6398, -134.6025, -129.5122], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-108.6328, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 5.4476e-02,  1.4234e+00,  7.8150e-01, -9.9416e-04, -5.6914e-02,\n",
      "        -1.4202e-01,  0.0000e+00,  0.0000e+00])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([-108.0854, -111.2189, -133.8095, -128.6955], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-108.0854, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-0.6959)\n",
      "Q(target_action):  tensor(-108.7813, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.1485, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 5.4476e-02,  1.4234e+00,  7.8150e-01, -9.9416e-04, -5.6914e-02,\n",
      "        -1.4202e-01,  0.0000e+00,  0.0000e+00])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([-107.0982, -110.8551, -133.4558, -128.6048], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-107.0982, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.0622,  1.4228,  0.7815, -0.0277, -0.0640, -0.1420,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([-106.5532, -110.4344, -132.6617, -127.7858], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-106.5532, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-0.7310)\n",
      "Q(target_action):  tensor(-107.2842, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.1860, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.0622,  1.4228,  0.7815, -0.0277, -0.0640, -0.1420,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([-105.6723, -110.1099, -132.3462, -127.7050], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-105.6723, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.0700,  1.4215,  0.7815, -0.0543, -0.0711, -0.1420,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([-104.7851, -109.1959, -131.1924, -126.5893], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-104.7851, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-0.7661)\n",
      "Q(target_action):  tensor(-105.5512, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(0.1211, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.0700,  1.4215,  0.7815, -0.0543, -0.0711, -0.1420,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([-104.0019, -108.9076, -130.9119, -126.5172], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-104.0019, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.0777,  1.4197,  0.7816, -0.0810, -0.0782, -0.1420,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([-103.2145, -107.9922, -129.6073, -125.5356], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-103.2145, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-0.8007)\n",
      "Q(target_action):  tensor(-104.0153, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.0134, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.0777,  1.4197,  0.7816, -0.0810, -0.0782, -0.1420,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([-102.5249, -107.7383, -129.3517, -125.4799], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-102.5249, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.0855,  1.4173,  0.7816, -0.1077, -0.0853, -0.1419,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([-101.8931, -106.8905, -127.9338, -124.6853], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-101.8931, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-0.8347)\n",
      "Q(target_action):  tensor(-102.7278, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.2029, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.0855,  1.4173,  0.7816, -0.1077, -0.0853, -0.1419,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([-101.2795, -106.6642, -127.7059, -124.6352], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-101.2795, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.0932,  1.4143,  0.7816, -0.1344, -0.0924, -0.1419,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([-100.6515, -105.8166, -126.2858, -123.8383], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-100.6515, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-0.8675)\n",
      "Q(target_action):  tensor(-101.5191, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.2396, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.0932,  1.4143,  0.7816, -0.1344, -0.0924, -0.1419,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([-100.1055, -105.6148, -126.0826, -123.7935], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-100.1055, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.1009,  1.4107,  0.7816, -0.1610, -0.0995, -0.1419,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([ -99.4809, -104.7671, -124.6602, -122.9943], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-99.4809, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-0.8992)\n",
      "Q(target_action):  tensor(-100.3801, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.2745, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.1009,  1.4107,  0.7816, -0.1610, -0.0995, -0.1419,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([ -98.9950, -104.5873, -124.4790, -122.9541], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-98.9950, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.1087,  1.4065,  0.7816, -0.1877, -0.1066, -0.1419,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([ -98.3732, -103.7393, -123.0542, -122.1526], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-98.3732, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-0.9293)\n",
      "Q(target_action):  tensor(-99.3025, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.3075, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.1087,  1.4065,  0.7816, -0.1877, -0.1066, -0.1419,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([ -97.9408, -103.5791, -122.8927, -122.1167], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-97.9408, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.1164,  1.4017,  0.7817, -0.2144, -0.1137, -0.1418,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([ -97.3214, -102.7308, -121.4653, -121.3129], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-97.3214, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-0.9576)\n",
      "Q(target_action):  tensor(-98.2791, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.3382, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.1164,  1.4017,  0.7817, -0.2144, -0.1137, -0.1418,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([ -96.9367, -102.5880, -121.3213, -121.2808], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-96.9367, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.1242,  1.3963,  0.7817, -0.2411, -0.1208, -0.1418,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([ -96.3193, -101.7392, -119.8912, -120.4746], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-96.3193, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-0.9841)\n",
      "Q(target_action):  tensor(-97.3034, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.3667, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.1242,  1.3963,  0.7817, -0.2411, -0.1208, -0.1418,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([ -95.9769, -101.6120, -119.7629, -120.4460], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-95.9769, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.1319,  1.3902,  0.7817, -0.2677, -0.1279, -0.1418,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([ -95.3612, -100.7627, -118.3301, -119.6375], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-95.3612, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.0085)\n",
      "Q(target_action):  tensor(-96.3697, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.3928, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.1319,  1.3902,  0.7817, -0.2677, -0.1279, -0.1418,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([ -95.0566, -100.6494, -118.2157, -119.6120], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-95.0566, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.1397,  1.3836,  0.7817, -0.2944, -0.1349, -0.1418,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([ -94.4422,  -99.7993, -116.7800, -118.8011], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-94.4422, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.0306)\n",
      "Q(target_action):  tensor(-95.4728, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.4163, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.1397,  1.3836,  0.7817, -0.2944, -0.1349, -0.1418,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([ -94.1712,  -99.6984, -116.6781, -118.7784], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-94.1712, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.1474,  1.3764,  0.7817, -0.3211, -0.1420, -0.1418,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([ -93.5578,  -98.8476, -115.2394, -117.9651], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-93.5578, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.0505)\n",
      "Q(target_action):  tensor(-94.6084, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.4372, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.1474,  1.3764,  0.7817, -0.3211, -0.1420, -0.1418,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([ -93.3167,  -98.7578, -115.1487, -117.9449], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-93.3167, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.1552,  1.3686,  0.7818, -0.3477, -0.1491, -0.1417,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([ -92.7042,  -97.9060, -113.7068, -117.1293], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-92.7042, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.0680)\n",
      "Q(target_action):  tensor(-93.7722, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.4555, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.1552,  1.3686,  0.7818, -0.3477, -0.1491, -0.1417,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([ -92.4897,  -97.8261, -113.6261, -117.1114], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-92.4897, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.1629,  1.3602,  0.7818, -0.3744, -0.1562, -0.1417,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([ -91.8777,  -96.9734, -112.1811, -116.2933], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-91.8777, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.0831)\n",
      "Q(target_action):  tensor(-92.9608, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.4711, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.1629,  1.3602,  0.7818, -0.3744, -0.1562, -0.1417,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([ -91.6869,  -96.9023, -112.1092, -116.2774], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-91.6869, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.1707,  1.3512,  0.7818, -0.4011, -0.1633, -0.1417,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([ -91.0753,  -96.0485, -110.6610, -115.4569], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-91.0753, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.0958)\n",
      "Q(target_action):  tensor(-92.1710, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.4841, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.1707,  1.3512,  0.7818, -0.4011, -0.1633, -0.1417,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([ -90.9056,  -95.9853, -110.5970, -115.4429], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-90.9056, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.1784,  1.3416,  0.7818, -0.4278, -0.1704, -0.1417,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([ -90.2941,  -95.1305, -109.1455, -114.6199], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-90.2941, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.1060)\n",
      "Q(target_action):  tensor(-91.4001, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.4945, grad_fn=<SubBackward0>)\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State:  tensor([ 0.1784,  1.3416,  0.7818, -0.4278, -0.1704, -0.1417,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([ -90.1432,  -95.0742, -109.0886, -114.6075], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-90.1432, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.1862,  1.3314,  0.7818, -0.4544, -0.1774, -0.1416,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([ -89.5317,  -94.2182, -107.6338, -113.7822], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-89.5317, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.1139)\n",
      "Q(target_action):  tensor(-90.6456, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.5023, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.1862,  1.3314,  0.7818, -0.4544, -0.1774, -0.1416,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([ -89.3976,  -94.1682, -107.5832, -113.7712], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-89.3976, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.1939,  1.3206,  0.7819, -0.4811, -0.1845, -0.1416,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([ -88.7859,  -93.3110, -106.1249, -112.9434], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-88.7859, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.1193)\n",
      "Q(target_action):  tensor(-89.9052, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.5076, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.1939,  1.3206,  0.7819, -0.4811, -0.1845, -0.1416,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([ -88.6666,  -93.2666, -106.0799, -112.9337], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-88.6666, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.2016,  1.3092,  0.7815, -0.5082, -0.1920, -0.1496,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([ -88.1130,  -92.4280, -104.5765, -112.1804], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-88.1130, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.1549)\n",
      "Q(target_action):  tensor(-89.2678, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.6012, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.2016,  1.3092,  0.7815, -0.5082, -0.1920, -0.1496,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([ -88.0070,  -92.3886, -104.5366, -112.1720], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-88.0070, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.2094,  1.2971,  0.7816, -0.5354, -0.1991, -0.1416,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([ -87.3191,  -91.4843, -103.0636, -111.2373], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-87.3191, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.1581)\n",
      "Q(target_action):  tensor(-88.4772, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.4702, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.2094,  1.2971,  0.7816, -0.5354, -0.1991, -0.1416,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([ -87.2250,  -91.4492, -103.0281, -111.2299], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-87.2250, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.2171,  1.2845,  0.7816, -0.5621, -0.2062, -0.1416,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([ -86.6123,  -90.5885, -101.5592, -110.3950], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-86.6123, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.1209)\n",
      "Q(target_action):  tensor(-87.7332, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.5082, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.2171,  1.2845,  0.7816, -0.5621, -0.2062, -0.1416,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([ -86.5286,  -90.5574, -101.5277, -110.3884], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-86.5286, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.2249,  1.2713,  0.7816, -0.5887, -0.2133, -0.1416,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([ -85.9152,  -89.6952, -100.0552, -109.5510], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-85.9152, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.1179)\n",
      "Q(target_action):  tensor(-87.0331, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.5045, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.2249,  1.2713,  0.7816, -0.5887, -0.2133, -0.1416,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([ -85.8409,  -89.6676, -100.0272, -109.5453], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-85.8409, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.2326,  1.2575,  0.7816, -0.6154, -0.2203, -0.1416,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([ -85.2266,  -88.8040,  -98.5510, -108.7053], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-85.2266, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.1130)\n",
      "Q(target_action):  tensor(-86.3396, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.4988, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.2326,  1.2575,  0.7816, -0.6154, -0.2203, -0.1416,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([ -85.1606,  -88.7795,  -98.5262, -108.7003], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-85.1606, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.2404,  1.2430,  0.7816, -0.6421, -0.2274, -0.1416,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([ -84.5455,  -87.9144,  -97.0464, -107.8577], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-84.5455, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.1063)\n",
      "Q(target_action):  tensor(-85.6518, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.4912, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.2404,  1.2430,  0.7816, -0.6421, -0.2274, -0.1416,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([ -84.4868,  -87.8927,  -97.0244, -107.8534], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-84.4868, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.2481,  1.2280,  0.7816, -0.6688, -0.2345, -0.1416,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([ -83.8707,  -87.0261,  -95.5410, -107.0083], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-83.8707, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.0980)\n",
      "Q(target_action):  tensor(-84.9687, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.4819, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.2481,  1.2280,  0.7816, -0.6688, -0.2345, -0.1416,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([ -83.8186,  -87.0069,  -95.5214, -107.0045], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-83.8186, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.2559,  1.2124,  0.7816, -0.6954, -0.2416, -0.1416,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([ -83.2015,  -86.1388,  -94.0343, -106.1569], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-83.2015, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.0882)\n",
      "Q(target_action):  tensor(-84.2897, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.4711, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.2559,  1.2124,  0.7816, -0.6954, -0.2416, -0.1416,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([ -83.1552,  -86.1217,  -94.0170, -106.1536], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-83.1552, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.2636,  1.1962,  0.7816, -0.7221, -0.2486, -0.1416,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([ -82.6065,  -85.3409,  -92.6531, -105.3931], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-82.6065, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.0771)\n",
      "Q(target_action):  tensor(-83.6835, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.5283, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.2636,  1.1962,  0.7816, -0.7221, -0.2486, -0.1416,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([ -82.5654,  -85.3259,  -92.6380, -105.3904], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-82.5654, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.2714,  1.1794,  0.7816, -0.7488, -0.2557, -0.1416,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([ -82.0703,  -84.6135,  -91.3707, -104.6982], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-82.0703, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.0647)\n",
      "Q(target_action):  tensor(-83.1350, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.5696, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.2714,  1.1794,  0.7816, -0.7488, -0.2557, -0.1416,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([ -82.0338,  -84.6003,  -91.3573, -104.6959], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-82.0338, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.2791,  1.1619,  0.7816, -0.7754, -0.2628, -0.1416,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([ -81.5381,  -83.8872,  -90.0876, -104.0021], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-81.5381, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.0513)\n",
      "Q(target_action):  tensor(-82.5894, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.5557, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.2791,  1.1619,  0.7816, -0.7754, -0.2628, -0.1416,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Q_current):  tensor([ -81.5056,  -83.8755,  -90.0757, -104.0001], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-81.5056, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.2868,  1.1439,  0.7816, -0.8021, -0.2699, -0.1416,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([ -81.0094,  -83.1617,  -88.8035, -103.3046], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-81.0094, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.0370)\n",
      "Q(target_action):  tensor(-82.0463, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.5408, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.2868,  1.1439,  0.7816, -0.8021, -0.2699, -0.1416,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([ -80.9804,  -83.1513,  -88.7930, -103.3029], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-80.9804, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.2946,  1.1253,  0.7816, -0.8288, -0.2770, -0.1416,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([ -80.4835,  -82.4368,  -87.5182, -102.6058], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-80.4835, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.0219)\n",
      "Q(target_action):  tensor(-81.5054, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.5251, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.2946,  1.1253,  0.7816, -0.8288, -0.2770, -0.1416,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([ -80.4576,  -82.4276,  -87.5089, -102.6044], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-80.4576, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.3023,  1.1061,  0.7816, -0.8555, -0.2840, -0.1416,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([ -79.9601,  -81.7123,  -86.2316, -101.9055], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-79.9601, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.0063)\n",
      "Q(target_action):  tensor(-80.9664, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.5088, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.3023,  1.1061,  0.7816, -0.8555, -0.2840, -0.1416,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([ -79.9370,  -81.7042,  -86.2234, -101.9043], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-79.9370, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.3101,  1.0863,  0.7816, -0.8821, -0.2911, -0.1416,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([ -79.5061,  -81.0821,  -84.9510, -101.2411], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-79.5061, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-0.9904)\n",
      "Q(target_action):  tensor(-80.4964, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.5594, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.3101,  1.0863,  0.7816, -0.8821, -0.2911, -0.1416,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([ -79.4732,  -81.0589,  -84.9434, -101.2320], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-79.4732, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.3178,  1.0658,  0.7816, -0.9088, -0.2982, -0.1416,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([ -79.3348,  -80.8455,  -83.7014, -100.7300], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-79.3348, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-0.9742)\n",
      "Q(target_action):  tensor(-80.3090, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.8358, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.3178,  1.0658,  0.7816, -0.9088, -0.2982, -0.1416,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([ -79.3054,  -80.8248,  -83.6946, -100.7219], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-79.3054, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.3256,  1.0448,  0.7816, -0.9355, -0.3053, -0.1416,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([ -79.1683,  -80.6136,  -82.4503, -100.2195], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-79.1683, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-0.9580)\n",
      "Q(target_action):  tensor(-80.1264, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.8210, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.3256,  1.0448,  0.7816, -0.9355, -0.3053, -0.1416,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([ -79.1419,  -80.5952,  -82.4443, -100.2123], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-79.1419, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.3333,  1.0232,  0.7816, -0.9621, -0.3124, -0.1416,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([-79.0061, -80.3862, -81.1976, -99.7093], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-79.0061, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-0.9420)\n",
      "Q(target_action):  tensor(-79.9482, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.8063, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.3333,  1.0232,  0.7816, -0.9621, -0.3124, -0.1416,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([-78.9824, -80.3698, -81.1923, -99.7029], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-78.9824, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.3411,  1.0010,  0.7816, -0.9888, -0.3194, -0.1416,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([-78.8479, -80.1630, -79.9432, -99.1995], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-78.8479, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-0.9265)\n",
      "Q(target_action):  tensor(-79.7744, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.7920, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.3411,  1.0010,  0.7816, -0.9888, -0.3194, -0.1416,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([-78.8264, -80.1483, -79.9385, -99.1938], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-78.8264, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.3488,  0.9782,  0.7816, -1.0155, -0.3265, -0.1416,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-78.6931, -79.9438, -78.6870, -98.6898], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-78.6870, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-0.9116)\n",
      "Q(target_action):  tensor(-79.5986, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.7722, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.3488,  0.9782,  0.7816, -1.0155, -0.3265, -0.1416,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-78.7186, -79.9521, -78.6522, -98.6993], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-78.6522, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.3567,  0.9553,  0.7964, -1.0198, -0.3334, -0.1384,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([-77.8783, -78.8745, -78.3749, -98.0222], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-77.8783, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-0.3638)\n",
      "Q(target_action):  tensor(-78.2421, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(0.4101, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.3567,  0.9553,  0.7964, -1.0198, -0.3334, -0.1384,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([-77.9249, -78.8937, -78.3275, -98.0388], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-77.9249, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.3646,  0.9318,  0.7964, -1.0465, -0.3404, -0.1384,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-77.7955, -78.6945, -77.0726, -97.5356], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-77.0726, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-0.8899)\n",
      "Q(target_action):  tensor(-77.9624, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.0376, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.3646,  0.9318,  0.7964, -1.0465, -0.3404, -0.1384,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-77.8398, -78.7128, -77.0286, -97.5513], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-77.0286, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.3726,  0.9085,  0.8072, -1.0335, -0.3477, -0.1477,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([-77.2783, -77.9451, -77.4448, -97.3514], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-77.2783, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(1.1868)\n",
      "Q(target_action):  tensor(-76.0915, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(0.9371, grad_fn=<SubBackward0>)\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State:  tensor([ 0.3726,  0.9085,  0.8072, -1.0335, -0.3477, -0.1477,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([-77.3721, -77.9881, -77.3688, -97.3837], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-77.3721, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.3806,  0.8847,  0.8072, -1.0602, -0.3551, -0.1477,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-77.2298, -77.7719, -76.1094, -96.8697], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-76.1094, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-0.9648)\n",
      "Q(target_action):  tensor(-77.0742, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(0.2979, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.3806,  0.8847,  0.8072, -1.0602, -0.3551, -0.1477,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-77.2972, -77.8022, -76.0526, -96.8929], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-76.0526, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.3887,  0.8609,  0.8171, -1.0618, -0.3627, -0.1515,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-76.7318, -77.0452, -75.8660, -96.4650], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-75.8660, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(0.0743)\n",
      "Q(target_action):  tensor(-75.7917, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(0.2609, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.3887,  0.8609,  0.8171, -1.0618, -0.3627, -0.1515,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-76.7925, -77.0725, -75.8150, -96.4859], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-75.8150, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.3971,  0.8369,  0.8391, -1.0676, -0.3698, -0.1423,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-75.5775, -75.5424, -75.5019, -95.5164], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-75.5019, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-0.9983)\n",
      "Q(target_action):  tensor(-76.5002, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.6851, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.3971,  0.8369,  0.8391, -1.0676, -0.3698, -0.1423,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-75.6319, -75.5670, -75.4562, -95.5352], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-75.4562, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.4055,  0.8129,  0.8551, -1.0677, -0.3769, -0.1417,  0.0000,  0.0000])\n",
      "Next action:   1\n",
      "All Q_next:  tensor([-74.7624, -74.4413, -75.3586, -94.9123], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-74.4413, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-0.2243)\n",
      "Q(target_action):  tensor(-74.6657, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(0.7905, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.4055,  0.8129,  0.8551, -1.0677, -0.3769, -0.1417,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  1\n",
      "All Q_current):  tensor([-74.8410, -74.5188, -75.2940, -94.9496], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-74.5188, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.4139,  0.7884,  0.8448, -1.0920, -0.3817, -0.0967,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-74.4109, -74.2267, -73.8248, -93.9161], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-73.8248, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(0.0174)\n",
      "Q(target_action):  tensor(-73.8073, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(0.7114, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.4139,  0.7884,  0.8448, -1.0920, -0.3817, -0.0967,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-74.4545, -74.2464, -73.7877, -93.9311], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-73.7877, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.4226,  0.7645,  0.8679, -1.0633, -0.3870, -0.1060,  0.0000,  0.0000])\n",
      "Next action:   1\n",
      "All Q_next:  tensor([-73.6862, -73.2573, -74.9512, -93.7980], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-73.2573, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(1.6750)\n",
      "Q(target_action):  tensor(-71.5823, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(2.2054, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.4226,  0.7645,  0.8679, -1.0633, -0.3870, -0.1060,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  1\n",
      "All Q_current):  tensor([-73.7227, -73.3144, -74.8489, -93.8133], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-73.3144, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.4311,  0.7400,  0.8600, -1.0878, -0.3906, -0.0707,  0.0000,  0.0000])\n",
      "Next action:   1\n",
      "All Q_next:  tensor([-73.1357, -72.7286, -73.4166, -92.7813], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-72.7286, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-0.0990)\n",
      "Q(target_action):  tensor(-72.8276, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(0.4868, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.4311,  0.7400,  0.8600, -1.0878, -0.3906, -0.0707,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  1\n",
      "All Q_current):  tensor([-73.2039, -72.8280, -73.3282, -92.8163], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-72.8280, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.4396,  0.7150,  0.8492, -1.1120, -0.3918, -0.0237,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-72.8557, -72.6556, -71.8667, -91.8288], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-71.8667, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(0.3124)\n",
      "Q(target_action):  tensor(-71.5544, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(1.2736, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.4396,  0.7150,  0.8492, -1.1120, -0.3918, -0.0237,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-72.8690, -72.6573, -71.8231, -91.8271], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-71.8231, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.4482,  0.6904,  0.8597, -1.0939, -0.3937, -0.0392,  0.0000,  0.0000])\n",
      "Next action:   1\n",
      "All Q_next:  tensor([-72.7523, -72.3929, -72.7029, -92.1587], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-72.3929, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(1.9171)\n",
      "Q(target_action):  tensor(-70.4757, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(1.3473, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.4482,  0.6904,  0.8597, -1.0939, -0.3937, -0.0392,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  1\n",
      "All Q_current):  tensor([-72.8246, -72.4974, -72.6372, -92.2045], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-72.4974, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 4.5671e-01,  6.6525e-01,  8.5121e-01, -1.1179e+00, -3.9372e-01,\n",
      "        -8.7743e-05,  0.0000e+00,  0.0000e+00])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-72.8888, -72.7390, -71.6118, -91.7575], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-71.6118, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(0.2106)\n",
      "Q(target_action):  tensor(-71.4012, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(1.0962, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 4.5671e-01,  6.6525e-01,  8.5121e-01, -1.1179e+00, -3.9372e-01,\n",
      "        -8.7743e-05,  0.0000e+00,  0.0000e+00])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-72.9126, -72.7578, -71.5825, -91.7698], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-71.5825, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.4654,  0.6402,  0.8661, -1.1130, -0.3939, -0.0043,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-72.7049, -72.3725, -72.1272, -91.9285], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-72.1272, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(0.7015)\n",
      "Q(target_action):  tensor(-71.4257, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(0.1568, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.4654,  0.6402,  0.8661, -1.1130, -0.3939, -0.0043,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-72.7265, -72.3893, -72.1010, -91.9400], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-72.1010, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.4745,  0.6157,  0.9123, -1.0902, -0.3935,  0.0094,  0.0000,  0.0000])\n",
      "Next action:   1\n",
      "All Q_next:  tensor([-73.0458, -72.6677, -74.2551, -92.9429], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-72.6677, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(0.0392)\n",
      "Q(target_action):  tensor(-72.6285, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.5276, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.4745,  0.6157,  0.9123, -1.0902, -0.3935,  0.0094,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  1\n",
      "All Q_current):  tensor([-73.0616, -72.6680, -74.2463, -92.9500], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-72.6680, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.4836,  0.5906,  0.9051, -1.1146, -0.3913,  0.0426,  0.0000,  0.0000])\n",
      "Next action:   1\n",
      "All Q_next:  tensor([-72.8566, -72.3801, -73.2888, -92.5343], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-72.3801, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(0.1580)\n",
      "Q(target_action):  tensor(-72.2221, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(0.4460, grad_fn=<SubBackward0>)\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State:  tensor([ 0.4836,  0.5906,  0.9051, -1.1146, -0.3913,  0.0426,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  1\n",
      "All Q_current):  tensor([-72.8706, -72.3802, -73.2808, -92.5404], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-72.3802, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.4925,  0.5650,  0.8938, -1.1383, -0.3867,  0.0930,  0.0000,  0.0000])\n",
      "Next action:   1\n",
      "All Q_next:  tensor([-72.5681, -72.0677, -72.2626, -91.9565], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-72.0677, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(0.6657)\n",
      "Q(target_action):  tensor(-71.4020, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(0.9782, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.4925,  0.5650,  0.8938, -1.1383, -0.3867,  0.0930,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  1\n",
      "All Q_current):  tensor([-72.5804, -72.0676, -72.2551, -91.9615], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-72.0676, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.5014,  0.5388,  0.8828, -1.1623, -0.3796,  0.1415,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-72.3069, -71.7800, -71.2297, -91.4080], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-71.2297, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(0.8022)\n",
      "Q(target_action):  tensor(-70.4275, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(1.6401, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.5014,  0.5388,  0.8828, -1.1623, -0.3796,  0.1415,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-72.3078, -71.7345, -71.2701, -91.4030], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-71.2701, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.5108,  0.5130,  0.9259, -1.1441, -0.3718,  0.1561,  0.0000,  0.0000])\n",
      "Next action:   1\n",
      "All Q_next:  tensor([-72.8961, -72.3892, -73.1809, -92.4759], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-72.3892, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(0.4612)\n",
      "Q(target_action):  tensor(-71.9280, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.6579, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.5108,  0.5130,  0.9259, -1.1441, -0.3718,  0.1561,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  1\n",
      "All Q_current):  tensor([-72.8922, -72.3298, -73.2363, -92.4671], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-72.3298, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.5200,  0.4866,  0.9150, -1.1680, -0.3616,  0.2046,  0.0000,  0.0000])\n",
      "Next action:   1\n",
      "All Q_next:  tensor([-72.6355, -72.0637, -72.2049, -91.9251], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-72.0637, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(0.9760)\n",
      "Q(target_action):  tensor(-71.0877, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(1.2421, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.5200,  0.4866,  0.9150, -1.1680, -0.3616,  0.2046,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  1\n",
      "All Q_current):  tensor([-72.6318, -72.0100, -72.2544, -91.9169], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-72.0100, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.5292,  0.4597,  0.9067, -1.1931, -0.3496,  0.2396,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-72.4743, -71.7880, -71.2313, -91.5170], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-71.2313, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(0.8031)\n",
      "Q(target_action):  tensor(-70.4282, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(1.5818, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.5292,  0.4597,  0.9067, -1.1931, -0.3496,  0.2396,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-72.4615, -71.6959, -71.3219, -91.5009], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-71.3219, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.5386,  0.4328,  0.9299, -1.1954, -0.3372,  0.2485,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-72.8323, -72.0169, -71.9376, -92.0784], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-71.9376, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(0.3618)\n",
      "Q(target_action):  tensor(-71.5758, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-0.2539, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.5386,  0.4328,  0.9299, -1.1954, -0.3372,  0.2485,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-72.8205, -71.9335, -72.0195, -92.0637], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-72.0195, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.5482,  0.4065,  0.9508, -1.1638, -0.3252,  0.2397,  0.0000,  0.0000])\n",
      "Next action:   1\n",
      "All Q_next:  tensor([-73.7988, -73.0195, -74.0142, -93.5269], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-73.0195, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(2.9036)\n",
      "Q(target_action):  tensor(-70.1159, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(1.9036, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.5482,  0.4065,  0.9508, -1.1638, -0.3252,  0.2397,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  1\n",
      "All Q_current):  tensor([-73.7996, -72.9975, -74.0315, -93.5238], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-72.9975, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.5578,  0.3797,  0.9436, -1.1892, -0.3117,  0.2704,  0.0000,  0.0000])\n",
      "Next action:   1\n",
      "All Q_next:  tensor([-73.6755, -72.7910, -73.0096, -93.1695], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-72.7910, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(0.5745)\n",
      "Q(target_action):  tensor(-72.2165, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(0.7810, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.5578,  0.3797,  0.9436, -1.1892, -0.3117,  0.2704,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  1\n",
      "All Q_current):  tensor([-73.6764, -72.7714, -73.0252, -93.1669], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-72.7714, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.5673,  0.3523,  0.9327, -1.2136, -0.2958,  0.3170,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-73.4672, -72.5502, -71.9649, -92.6658], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-71.9649, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(0.9986)\n",
      "Q(target_action):  tensor(-70.9663, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(1.8051, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.5673,  0.3523,  0.9327, -1.2136, -0.2958,  0.3170,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-73.4573, -72.4812, -72.0335, -92.6539], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-72.0335, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.5769,  0.3255,  0.9462, -1.1889, -0.2804,  0.3077,  0.0000,  0.0000])\n",
      "Next action:   1\n",
      "All Q_next:  tensor([-74.3622, -73.4627, -73.5712, -93.9375], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-73.4627, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(2.8918)\n",
      "Q(target_action):  tensor(-70.5709, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(1.4626, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.5769,  0.3255,  0.9462, -1.1889, -0.2804,  0.3077,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  1\n",
      "All Q_current):  tensor([-74.3621, -73.4426, -73.5889, -93.9345], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-73.4426, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.5864,  0.2981,  0.9353, -1.2132, -0.2627,  0.3544,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-74.1628, -73.2346, -72.5267, -93.4402], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-72.5267, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(0.9502)\n",
      "Q(target_action):  tensor(-71.5765, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(1.8661, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.5864,  0.2981,  0.9353, -1.2132, -0.2627,  0.3544,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-74.1519, -73.1627, -72.6001, -93.4281], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-72.6001, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.5963,  0.2714,  0.9668, -1.1868, -0.2445,  0.3638,  0.0000,  0.0000])\n",
      "Next action:   1\n",
      "All Q_next:  tensor([-74.9537, -74.0987, -74.5793, -94.6890], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-74.0987, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(1.9075)\n",
      "Q(target_action):  tensor(-72.1912, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(0.4089, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.5963,  0.2714,  0.9668, -1.1868, -0.2445,  0.3638,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Q_current):  tensor([-74.9464, -74.0462, -74.6336, -94.6804], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-74.0462, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.6061,  0.2440,  0.9594, -1.2125, -0.2248,  0.3940,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-74.8569, -73.8788, -73.5812, -94.3476], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-73.5812, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(0.5732)\n",
      "Q(target_action):  tensor(-73.0079, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(1.0382, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.6061,  0.2440,  0.9594, -1.2125, -0.2248,  0.3940,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-74.8444, -73.8013, -73.6632, -94.3348], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-73.6632, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.6161,  0.2175,  0.9884, -1.1757, -0.2048,  0.4008,  0.0000,  0.0000])\n",
      "Next action:   1\n",
      "All Q_next:  tensor([-75.8200, -74.9755, -76.0238, -95.8317], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-74.9755, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(2.7183)\n",
      "Q(target_action):  tensor(-72.2572, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(1.4060, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.6161,  0.2175,  0.9884, -1.1757, -0.2048,  0.4008,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  1\n",
      "All Q_current):  tensor([-75.8179, -74.9479, -76.0543, -95.8284], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-74.9479, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.6262,  0.1904,  0.9801, -1.2012, -0.1830,  0.4351,  0.0000,  0.0000])\n",
      "Next action:   1\n",
      "All Q_next:  tensor([-75.7062, -74.7775, -74.9784, -95.4524], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-74.7775, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(0.6059)\n",
      "Q(target_action):  tensor(-74.1716, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(0.7763, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.6262,  0.1904,  0.9801, -1.2012, -0.1830,  0.4351,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  1\n",
      "All Q_current):  tensor([-75.7047, -74.7531, -75.0063, -95.4500], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-74.7531, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.6361,  0.1627,  0.9712, -1.2270, -0.1595,  0.4715,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-75.5908, -74.5890, -73.9036, -95.0580], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-73.9036, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(0.6635)\n",
      "Q(target_action):  tensor(-73.2401, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(1.5129, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.6361,  0.1627,  0.9712, -1.2270, -0.1595,  0.4715,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-75.5807, -74.5219, -73.9782, -95.0483], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-73.9782, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.6460,  0.1356,  0.9688, -1.2050, -0.1366,  0.4580,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-76.5163, -75.5300, -75.0393, -96.2297], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-75.0393, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(3.5042)\n",
      "Q(target_action):  tensor(-71.5351, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(2.4431, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.6460,  0.1356,  0.9688, -1.2050, -0.1366,  0.4580,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-76.5072, -75.4689, -75.1074, -96.2208], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-75.1074, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.6560,  0.1089,  0.9763, -1.1839, -0.1138,  0.4556,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-77.3390, -76.3956, -76.3371, -97.3291], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-76.3371, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(2.6552)\n",
      "Q(target_action):  tensor(-73.6819, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(1.4255, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.6560,  0.1089,  0.9763, -1.1839, -0.1138,  0.4556,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-77.3312, -76.3407, -76.3998, -97.3216], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-76.3998, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.6662,  0.0822,  0.9920, -1.1845, -0.0904,  0.4665,  0.0000,  0.0000])\n",
      "Next action:   1\n",
      "All Q_next:  tensor([-77.7537, -76.7554, -76.8876, -97.8831], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-76.7554, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(0.3586)\n",
      "Q(target_action):  tensor(-76.3968, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(0.0031, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.6662,  0.0822,  0.9920, -1.1845, -0.0904,  0.4665,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  1\n",
      "All Q_current):  tensor([-77.7468, -76.7062, -76.9447, -97.8767], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-76.7062, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.6763,  0.0549,  0.9830, -1.2109, -0.0653,  0.5026,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-77.7115, -76.6370, -75.9392, -97.5779], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-75.9392, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(0.2949)\n",
      "Q(target_action):  tensor(-75.6443, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(1.0618, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.6763,  0.0549,  0.9830, -1.2109, -0.0653,  0.5026,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-77.6954, -76.5543, -76.0214, -97.5621], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-76.0214, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.6865,  0.0279,  0.9980, -1.2018, -0.0396,  0.5137,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-78.2214, -77.1273, -76.8167, -98.2743], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-76.8167, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(1.1621)\n",
      "Q(target_action):  tensor(-75.6546, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(0.3669, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.6865,  0.0279,  0.9980, -1.2018, -0.0396,  0.5137,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([-78.2071, -77.0530, -76.8918, -98.2603], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-76.8918, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.6967,  0.0015,  0.9930, -1.1691, -0.0144,  0.5054,  0.0000,  0.0000])\n",
      "Next action:   1\n",
      "All Q_next:  tensor([-79.2182, -78.2199, -78.2747, -99.5463], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-78.2199, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(4.0888)\n",
      "Q(target_action):  tensor(-74.1311, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(2.7606, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.6967,  0.0015,  0.9930, -1.1691, -0.0144,  0.5054,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  1\n",
      "All Q_current):  tensor([-79.2393, -78.2637, -78.2721, -99.5699], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-78.2637, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.7068, -0.0254,  0.9846, -1.1961,  0.0126,  0.5393,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([ -80.0129,  -79.0163,  -78.2091, -100.3364], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-78.2091, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-2.4399)\n",
      "Q(target_action):  tensor(-80.6490, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-2.3853, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.7068, -0.0254,  0.9846, -1.1961,  0.0126,  0.5393,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([ -80.0417,  -79.1283,  -78.1103, -100.3638], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-78.1103, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.7170, -0.0518,  0.9963, -1.1746,  0.0402,  0.5515,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([ -80.8757,  -80.0354,  -79.3870, -101.5023], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-79.3870, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-3.3227)\n",
      "Q(target_action):  tensor(-82.7097, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-4.5994, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.7170, -0.0518,  0.9963, -1.1746,  0.0402,  0.5515,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([ -80.9006,  -80.1354,  -79.2936, -101.5257], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-79.2936, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.7273, -0.0774,  1.0004, -1.1392,  0.0681,  0.5591,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([ -82.1906,  -81.6391,  -81.3523, -103.2429], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-81.3523, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.9347)\n",
      "Q(target_action):  tensor(-83.2870, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-3.9934, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.7273, -0.0774,  1.0004, -1.1392,  0.0681,  0.5591,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([ -82.2035,  -81.7177,  -81.2499, -103.2516], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-81.2499, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.7375, -0.1023,  0.9855, -1.1106,  0.0956,  0.5502,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([ -83.1936,  -82.8626,  -82.2846, -104.4222], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-82.2846, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.2365)\n",
      "Q(target_action):  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 2000/2000 [24:15<00:00,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-83.5211, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-2.2712, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.7375, -0.1023,  0.9855, -1.1106,  0.0956,  0.5502,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([ -83.2054,  -82.9346,  -82.1914, -104.4304], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-82.1914, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.7476, -0.1263,  0.9817, -1.0676,  0.1235,  0.5562,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([ -84.6341,  -84.6352,  -84.4381, -106.3003], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-84.4381, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-1.0005)\n",
      "Q(target_action):  tensor(-85.4387, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-3.2473, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.7476, -0.1263,  0.9817, -1.0676,  0.1235,  0.5562,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([ -84.6320,  -84.6877,  -84.3465, -106.2934], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-84.3465, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.7576, -0.1496,  0.9801, -1.0364,  0.1517,  0.5649,  0.0000,  0.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([ -85.6335,  -85.8949,  -86.0407, -107.6257], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-85.6335, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-2.1404)\n",
      "Q(target_action):  tensor(-87.7739, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-3.4275, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.7576, -0.1496,  0.9801, -1.0364,  0.1517,  0.5649,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([ -85.4903,  -85.9669,  -86.1287, -107.6098], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-85.4903, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.7677, -0.1734,  0.9801, -1.0632,  0.1799,  0.5649,  0.0000,  0.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([ -84.9575,  -85.2584,  -84.8010, -106.8210], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-84.8010, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-6.2599)\n",
      "Q(target_action):  tensor(-91.0609, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-5.5706, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.7677, -0.1734,  0.9801, -1.0632,  0.1799,  0.5649,  0.0000,  0.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([ -85.0512,  -85.2788,  -84.5974, -106.8158], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-84.5974, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.7776, -0.1965,  0.9620, -1.0290,  0.2081,  0.5635,  0.0000,  1.0000])\n",
      "Next action:   0\n",
      "All Q_next:  tensor([ -95.5088,  -99.3433,  -97.8425, -118.0886], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-95.5088, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(9.1182)\n",
      "Q(target_action):  tensor(-86.3906, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-1.7932, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.7776, -0.1965,  0.9620, -1.0290,  0.2081,  0.5635,  0.0000,  1.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  0\n",
      "All Q_current):  tensor([ -95.4728,  -99.3196,  -97.7347, -118.0474], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-95.4728, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.7876, -0.2191,  0.9890, -1.0074,  0.2192,  0.2210,  0.0000,  1.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([ -94.8613,  -97.9171,  -93.8214, -116.7301], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-93.8214, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-2.9590)\n",
      "Q(target_action):  tensor(-96.7804, grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-1.3076, grad_fn=<SubBackward0>)\n",
      " \n",
      "State:  tensor([ 0.7876, -0.2191,  0.9890, -1.0074,  0.2192,  0.2210,  0.0000,  1.0000])\n",
      "State size:  torch.Size([8])\n",
      "Action:  2\n",
      "All Q_current):  tensor([ -94.9094,  -97.9179,  -93.6282, -116.6969], grad_fn=<AddBackward0>)\n",
      "Q(current_action):  tensor(-93.6282, grad_fn=<SelectBackward0>)\n",
      "Next state:  tensor([ 0.7967, -0.2383,  0.9793, -0.5561,  0.1593, -3.5746,  1.0000,  1.0000])\n",
      "Next action:   2\n",
      "All Q_next:  tensor([-158.4766, -157.8551, -143.7248, -195.7384], grad_fn=<AddBackward0>)\n",
      "Q(next_state):  tensor(-143.7248, grad_fn=<SelectBackward0>)\n",
      "Reward tensor(-100.)\n",
      "Q(target_action):  tensor(-100., grad_fn=<AddBackward0>)\n",
      "Error (Q-target_action - Q_current_action):  tensor(-6.3718, grad_fn=<SubBackward0>)\n",
      " \n",
      "----------------End Algorithm--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1932\\2042601975.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[0mreward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'plots/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'_result'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%Y%m%d%H%M%S\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseeds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseeds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseeds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Use seed as column labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m     \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreward\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"------------------------End Environment-------------------\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Algorithm: Deep SARSA\" )\n",
    "for i in envs:\n",
    "    print(\"ENVIRONMENT:-----------\", i)\n",
    "    env = gym.make(i)\n",
    "    res=[]\n",
    "\n",
    "    for j in seeds:\n",
    "        print(\"Seed = \", j)\n",
    "        rewards = []\n",
    "        aver_reward = []\n",
    "        aver = deque(maxlen = 100)\n",
    "        state_size = env.observation_space.shape[0]\n",
    "        action_size=env.action_space.n\n",
    "        agent = Agent(state_size, action_size, j)\n",
    "        epsilon = epsilon_start\n",
    "\n",
    "        for i_episode in tqdm(range(1, n_episodes+1)):\n",
    "            if i_episode < 3 or i_episode > 1998:\n",
    "                print(\"Iteration number: \", i_episode-1)\n",
    "            state = env.reset()\n",
    "            score = 0\n",
    "            action = agent.act(state, epsilon)\n",
    "            for t in range(max_t):\n",
    "\n",
    "                step_result = env.step(action)\n",
    "                next_state, reward, done, _ = step_result[:4]\n",
    "                next_action = agent.act(next_state, epsilon)\n",
    "                agent.step(state,action,reward,next_state,next_action, done, state_size)\n",
    "                state = next_state\n",
    "                action = next_action\n",
    "                score = score + reward\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            aver.append(score)\n",
    "            aver_reward.append(np.mean(aver))\n",
    "            rewards.append(score)\n",
    "            epsilon = max(epsilon_end, epsilon_decay*epsilon) # decrease epsilon\n",
    "\n",
    "        # Save the model\n",
    "        reward = \"model/\"+ \"Seed\" + str(j)  + i + \"_\" + \"_\" + str(n_episodes) + \"_\" + str(datetime.now().strftime(\"%Y%m%d%H%M%S\"))\n",
    "        torch.save(agent.qnetwork_local.state_dict(), reward + '.pt')\n",
    "\n",
    "        # Append the average reward to the results\n",
    "        res.append(aver_reward)\n",
    "        print(\"----------------End Algorithm--------------------\")\n",
    "\n",
    "    fig = plt.figure()\n",
    "\n",
    "    reward = 'plots/' + i + '_result' + str(datetime.now().strftime(\"%Y%m%d%H%M%S\"))\n",
    "    df = pd.DataFrame({str(seeds[0]): res[0], str(seeds[1]): res[1], str(seeds[2]): res[2]})  # Use seed as column labels\n",
    "    df.to_csv(reward + '.csv')\n",
    "    print(\"------------------------End Environment-------------------\")\n",
    "\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "\n",
    "    # Plot rewards for each seed\n",
    "    for seed in seeds:\n",
    "        plt.plot(df[str(seed)], label='Seed ' + str(seed))\n",
    "\n",
    "    plt.title('Learning Curve ' + i)\n",
    "\n",
    "    # Insert the legends in the plot\n",
    "    fig.legend(loc='lower right')\n",
    "    fig.savefig(reward + '.png', dpi=100)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e6fdff",
   "metadata": {
    "id": "71e6fdff"
   },
   "source": [
    "## 7. Demonstration with learned policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d68355",
   "metadata": {
    "id": "59d68355"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Function to simulate a model in an environment\n",
    "def simulate_model(env, agent, n_episodes=100, max_t=1000):\n",
    "    scores = []\n",
    "\n",
    "    for i_episode in tqdm(range(1, n_episodes+1)):\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        epsilon = 0\n",
    "\n",
    "        for t in range(max_t):\n",
    "            env.render()  # Move rendering here\n",
    "            action = agent.act(state, epsilon = 0)\n",
    "            step_result = env.step(action)\n",
    "            next_state, reward, done, _ = step_result[:4]\n",
    "            next_action = agent.act(next_state, epsilon = 0)\n",
    "            agent.step(state,action,reward,next_state,next_action, done, state_size)\n",
    "            state = next_state\n",
    "            action = next_action\n",
    "            score = score + reward\n",
    "            if done:\n",
    "                break\n",
    "        print(score)\n",
    "        scores.append(score)\n",
    "\n",
    "    # Close the environment\n",
    "    env.close()\n",
    "\n",
    "    # Print average score\n",
    "    print(\"Average score:\", np.mean(scores))\n",
    "\n",
    "\n",
    "env_name = 'LunarLander-v2'\n",
    "model_path = 'model/Seed37_LunarLander-v2__DeepSARSA_4000_20240407085617.pt'\n",
    "env = gym.make(env_name,  render_mode = 'human')\n",
    "\n",
    "# Get environment parameters\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "# Initialize the agent\n",
    "agent = Agent(state_size, action_size, seed=37)\n",
    "\n",
    "# Load the model weights\n",
    "agent.qnetwork_local.load_state_dict(torch.load(model_path))\n",
    "agent.qnetwork_local.eval()\n",
    "\n",
    "# Simulate the model in the environment\n",
    "simulate_model(env, agent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d59aa6",
   "metadata": {
    "id": "b7d59aa6"
   },
   "source": [
    "## 8. Plot the shaded graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68c6775",
   "metadata": {
    "id": "d68c6775"
   },
   "outputs": [],
   "source": [
    "# Step 1: Read Data from CSV\n",
    "df = pd.read_csv('plots/PriorityDDQN_MountainCar.csv')\n",
    "\n",
    "# Step 2: Remove the first row (Episodes, Run 1, Run 2, Run 3) to keep only the rewards data\n",
    "# df = df.drop(0)\n",
    "\n",
    "# Step 3: Convert the remaining DataFrame to numeric values\n",
    "df = df.apply(pd.to_numeric)\n",
    "\n",
    "# Step 4: Select only the three late columns (Run 1, Run 2, Run 3)\n",
    "later_columns = df.iloc[:, 1:4]\n",
    "\n",
    "# Step 5: Calculate Mean and Standard Deviation for the three later columns\n",
    "mean_values = later_columns.mean(axis=1)\n",
    "std_values = later_columns.std(axis=1)\n",
    "print(\"Standard deviation: \", std_values)\n",
    "print(\"Mean: \", mean_values)\n",
    "\n",
    "# Step 6: Plot the Data\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(mean_values, label='Mean Sum of Rewards', color = \"red\")\n",
    "plt.fill_between(mean_values.index, mean_values - std_values, mean_values + std_values, alpha=0.5, label='Standard Deviation', color = \"orange\")\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Sum of Rewards')\n",
    "plt.title('Shaded Plot of Sum of Rewards with Mean and Standard Deviation for Priority Double Deep Q-Learning Algorithm, Mountain Car')\n",
    "plt.savefig('plots/shaded_plot_PriorityDDQN_MountainCar.png')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a49bd2a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7a49bd2a",
    "outputId": "e752b731-92a2-4ad7-8533-9c4114dfed8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ufal.pybox2d in /usr/local/lib/python3.10/dist-packages (2.3.10.3)\n"
     ]
    }
   ],
   "source": [
    "pip install ufal.pybox2d"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
