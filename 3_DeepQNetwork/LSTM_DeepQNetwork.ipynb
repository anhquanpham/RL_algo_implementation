{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2480ed9",
   "metadata": {},
   "source": [
    "# LSTM Deep Q Network for different environments "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc4a8ac",
   "metadata": {},
   "source": [
    "## 1. Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "606f2c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt #plotting library\n",
    "from matplotlib import animation #animated visualizations library\n",
    "from collections import namedtuple, deque \n",
    "#nametuple creates tuple subclasses with name fields, access elements by names instead of index\n",
    "#deque (double-ended queue) for adding and removing elements from both ends\n",
    "from tqdm import tqdm\n",
    "#add progress bars to Python code for easy monitoring progress of loops and tasks\n",
    "# %matplotlib inline \n",
    "import gym #environments for agents\n",
    "from datetime import datetime #manipulating dates and times\n",
    "import pandas as pd #work with structured data\n",
    "import torch #Pytorch supports tensor computations and neural networks\n",
    "import torch.nn as nn #Pytorch supports building neural networks\n",
    "import torch.nn.functional as F\n",
    "#common functions in neural network operations \n",
    "    # Activation functions (ReLU, sigmoid, tanh)\n",
    "    # Loss functions (cross_entropy, mse_loss)\n",
    "    #Utility functions for tensor manipulation (softmax, dropout, batch_norm, etc.)\n",
    "import torch.optim as optim #optimization algorithms for training neural networks\n",
    "import random #generate random numbers/selections\n",
    "from collections import namedtuple, deque \n",
    "import itertools \n",
    "# provides various functions for creating iterators and combining them for complex interators\n",
    "# includes cycle, chain, zip, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3593047",
   "metadata": {},
   "source": [
    "## 2. LSTM model for approximating Q-values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f571f047",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 64\n",
    "num_layers = 3\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_size, hidden_size, num_layers, action_size, seed):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "\n",
    "        self.hidden_dim = hidden_size\n",
    "        self.layer_dim = num_layers\n",
    "        \n",
    "        #LSTM model\n",
    "        self.lstm = nn.LSTM(input_size=state_size, hidden_size = 64, num_layers = 3, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, action_size)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        # Reshape state to have 3 dimensions: batch_size, sequence_length, input_size\n",
    "        # Assuming state has shape (batch_size, input_size)\n",
    "        h0 = torch.zeros(self.layer_dim, state.size(0), self.hidden_dim).requires_grad_()\n",
    "        c0 = torch.zeros(self.layer_dim, state.size(0), self.hidden_dim).requires_grad_()\n",
    "        # Reshape lstm_out to remove sequence length dimension\n",
    "        lstm_out, (hn,cn) = self.lstm(state, (h0.detach(), c0.detach()))\n",
    "        \n",
    "        lstm_out = self.fc(lstm_out[:, -1, :]) \n",
    "        \n",
    "        return lstm_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba2eecb",
   "metadata": {},
   "source": [
    "## 3. Implement the replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d6df66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\") #Device Initialization\n",
    "\n",
    "class ReplayBuffer: #Fixed-size buffer to store experience tuples.\n",
    "    # Initialize a ReplayBuffer object.\n",
    "    def __init__(self, state_size, action_size, buffer_size, batch_size, priority):\n",
    "            #state_size(int): dimension of each state\n",
    "            #action_size (int): dimension of each action \n",
    "            #buffer_size (int): max size of buffer \n",
    "            #batch_size (int): size of 1 training batch\n",
    "            #seed (int): random seed\n",
    "        self.priority = priority        \n",
    "        #Pointer to keep track of position within the replay buffer where next experiene will be added\n",
    "        # ptr = 0, new experience added -> increment to ptr = 1\n",
    "        self. ptr = 0\n",
    "        #Check if buffer have been filled\n",
    "        self.n = 0 \n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.state_size = state_size\n",
    "        \n",
    "        state_size = env.observation_space.shape[0]\n",
    "        action_size = env.action_space.n\n",
    "\n",
    "        #create states tensor full of zeros with {buffer_size} rows & {state_size} columns\n",
    "        #store in device CPU, allocate RAM to it \n",
    "        self.states = torch.zeros((1,)+(buffer_size,)+(state_size,)).to(device)\n",
    "        #Similar to above \n",
    "        self.next_states = torch.zeros((1,)+(buffer_size,)+(state_size,)).to(device)\n",
    "        #Only 1 action taken per experience tuple stored\n",
    "        # => action size = buffer size rows but 1 column only needed to store\n",
    "        self.actions = torch.zeros(1, buffer_size, dtype=torch.long).to(device) \n",
    "        # Reward same with action\n",
    "        self.rewards = torch.zeros(1, buffer_size, dtype=torch.float).to(device) \n",
    "        \n",
    "        # Flag to indicate transition/end between an episode ('done' flag)\n",
    "        # True/False floating values (0.0 False, 1.0 True)\n",
    "        self.dones = torch.zeros(1, buffer_size, dtype=torch.float).to(device)\n",
    "        \n",
    "        # Error in case implement prioritize replay\n",
    "        self.error = np.zeros((1, buffer_size), dtype=float)\n",
    "        \n",
    "        # Priority\n",
    "        self.priority = priority\n",
    "    \n",
    "  # Add new experience to buffer\n",
    "    def add(self, state, action, reward, next_state, done, state_size):\n",
    "        \n",
    "        state_size = env.observation_space.shape[0]\n",
    "\n",
    "        # Convert state and next_state to appropriate types\n",
    "        state = torch.tensor(state[0], dtype=torch.float32).reshape(1, 1,state_size)\n",
    "        state = state.to(device)\n",
    "            \n",
    "\n",
    "        next_state = torch.tensor(next_state[0], dtype=torch.float32).reshape(1, 1, state_size)\n",
    "        next_state = next_state.to(device)\n",
    "            \n",
    "        # Convert action, reward, and done flag to tensors\n",
    "        #action = torch.as_tensor(action, dtype=torch.float32).to(device)\n",
    "        reward = torch.as_tensor(reward, dtype=torch.float32).to(device)\n",
    "        done = torch.as_tensor(done, dtype=torch.bool).to(device)\n",
    "        \n",
    "        # Store the data in the replay buffer\n",
    "        self.states[0][self.ptr] = state\n",
    "        self.next_states[0][self.ptr] = next_state\n",
    "        self.actions[0][self.ptr] = action\n",
    "        self.rewards[0][self.ptr] = reward\n",
    "        self.dones[0][self.ptr] = done\n",
    "        \n",
    "        \n",
    "       \n",
    "        # Increment the pointer\n",
    "        self.ptr = (self.ptr + 1) % self.buffer_size\n",
    "\n",
    "        # Reset pointer and flag when buffer is filled\n",
    "        if self.ptr == 0:\n",
    "            self.n = self.buffer_size\n",
    "        \n",
    "    #Sample a batch of experience from memory\n",
    "    def sample(self, get_all=False):\n",
    "        n = len(self) # Length of the Buffer\n",
    "        state_size = env.observation_space.shape[0]\n",
    "        action_size = env.action_space.n\n",
    "        # Return all experience stored in buffer, no sampling\n",
    "        if get_all:\n",
    "            return self.states[:n], self.actions[:n], self.rewards[:n], self.next_states[:n], self.dones[:n]\n",
    "        \n",
    "        # else, do sampling: \n",
    "        else:\n",
    "            if self.priority:     \n",
    "            #enable prioritized experience replay\n",
    "            #experience are sampled based on priorities probability distribution p = self.error\n",
    "                idx = np.random.choice(n, self.batch_size, replace=False, p=self.error)\n",
    "            else: #uniform sampling\n",
    "                idx = np.random.choice(n, self.batch_size, replace=False)\n",
    "            #Replace = False to only sample once for each element\n",
    "            \n",
    "            states = torch.empty(1, self.batch_size, state_size)\n",
    "            next_states = torch.empty(1, self.batch_size, state_size)\n",
    "            actions = torch.empty(1, self.batch_size)\n",
    "            rewards = torch.empty(1, self.batch_size)\n",
    "            dones = torch.empty(1, self.batch_size)\n",
    "            #Retrieve sampled experiences \n",
    "            states[0] = self.states[0][idx]\n",
    "            next_states[0] = self.next_states[0][idx]\n",
    "            actions[0] = self.actions[0][idx]\n",
    "            rewards[0] = self.rewards[0][idx]\n",
    "            dones[0] = self.dones[0][idx]\n",
    "            \n",
    "            return (states, actions, rewards, next_states, dones), idx\n",
    "    \n",
    "    # Update the error associated with experiences in replay buffer\n",
    "    def update_error(self, error, idx=None): #specify index number, if not specify, all are updated\n",
    "        error = torch.abs(error.detach()) #absolute value of errors, detach to prevent gradient computation to be attached to error tensor\n",
    "        error = error / error.sum() #Normalize, ensure all errors add up to 1\n",
    "        if idx is not None: #index are specified, then only update specified indices\n",
    "            self.error[0][idx] = error.cpu().numpy()\n",
    "        else: # not specify index, all are updated\n",
    "            self.error[0][:len(self)] = error.cpu().numpy()\n",
    "    \n",
    "    # \n",
    "    def __len__(self): \n",
    "        if self.n == 0:\n",
    "            return self.ptr\n",
    "        else:\n",
    "            return self.n #when buffer is filled self.n stored that size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e6742c",
   "metadata": {},
   "source": [
    "## 4. Set the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b7e89a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = 100000 #Replay buffer size\n",
    "# batch_size = 3 #Minibatch size\n",
    "gamma = 0.99 #Discount factor\n",
    "tau = 1 #Soft update target parameters, tau = 1 = copy completely\n",
    "alpha = 0.0005 #learning rate\n",
    "update_every = 4 #How often to update the network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4817f2b6",
   "metadata": {},
   "source": [
    "## 5. Agent learning implementation¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0d1af1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(): #Learning by interacting with the environment\n",
    "    def __init__(self, state_size, action_size, hidden_size, num_layers, batch_size, seed, ddqn, priority):\n",
    "        self.state_size = state_size #Dimension of each state\n",
    "        self.action_size = action_size #Dimension of action\n",
    "        self.seed = random.seed(seed) #Choose the random seed\n",
    "        self.ddqn = ddqn #Store whether agent uses DDQN\n",
    "        self.priority = priority #Whether uses experience replay\n",
    "        self.batch_size = batch_size\n",
    "        #Q-Network\n",
    "        self.qnetwork_local = QNetwork(state_size, hidden_size, num_layers, action_size, seed).to(device)\n",
    "        self.qnetwork_target = QNetwork(state_size, hidden_size, num_layers, action_size, seed).to(device)\n",
    "        \n",
    "        #Initializes optimizer for updating the weights of local Q-Network\n",
    "        self.optimizer = torch.optim.SGD(self.qnetwork_local.parameters(), lr=alpha)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(state_size, (action_size,), buffer_size, batch_size, priority)\n",
    "        # Initialize timestep (to keep track timesteps for updating target)\n",
    "        self.t_step = 0\n",
    "        \n",
    "    def step(self, state, action, reward, next_state, done, step_size):\n",
    "        # Convert states to torch tensors and add batch dimension\n",
    "\n",
    "        # Save experience in replay memory\n",
    "        self.memory.add(state, action, reward, next_state, done, step_size)\n",
    "\n",
    "        self.t_step += 1  # Increment the timestep counter\n",
    "\n",
    "        # Update the Q-network after each step in the DQN case\n",
    "        if not self.ddqn and len(self.memory) > self.batch_size:\n",
    "            experiences, idx = self.memory.sample()\n",
    "            error = self.learn(experiences)\n",
    "            self.memory.update_error(error, idx)\n",
    "\n",
    "        # In the DDQN case, update the Q-network after a certain number of steps\n",
    "        if self.ddqn and (self.t_step % update_every) == 0 and len(self.memory) > self.batch_size:\n",
    "            experiences, idx = self.memory.sample()\n",
    "            error = self.learn(experiences)\n",
    "            self.memory.update_error(error, idx)\n",
    "\n",
    "                \n",
    " \n",
    "    def act(self, state, epsilon):\n",
    "\n",
    "        state = torch.tensor(state[0], dtype=torch.float32)\n",
    "\n",
    "        \n",
    "        self.qnetwork_local.eval()  # Evaluation mode\n",
    "        with torch.no_grad():  # Disable gradient calculation when choosing action\n",
    "            state_input = state.view(1,1,-1) #treat this tensor as a single time step of a sequence for a batch size of 1\n",
    "            action_values = self.qnetwork_local(state_input)  # Pass preprocessed states into local Q-network\n",
    "        self.qnetwork_local.train()  # Set local Q network back to training mode after inference is complete\n",
    "\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() > epsilon:\n",
    "            # Return action with highest Q-value\n",
    "            action = np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            # Select a random action\n",
    "            action = random.choice(range(self.action_size))\n",
    "        action = min(max(action, 0), self.action_size - 1) # Ensure action is within valid range\n",
    "        \n",
    "        return action   \n",
    "            \n",
    "    #Update value parameters using batch of experience tuples\n",
    "    def learn(self, experiences):\n",
    "        #experiences (Tuple[torch.Variable]): tuple of (s, a, r, s', done) tuples \n",
    "        #gamma (float): discount factor\n",
    "        \n",
    "        \n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        ## compute and minimize the loss\n",
    "        self.optimizer.zero_grad() #Resets all gradients to zero, gradients from previous batches do not accumulate\n",
    "        if i_episode < 3 or i_episode > 1998:    \n",
    "            print(\"Current state: \", state)\n",
    "            print(\" \")\n",
    "            print(\"Experiences: \")\n",
    "            print(\"States: \", states)\n",
    "            print(\"Actions: \", actions)\n",
    "            print(\"Rewards: \", rewards)\n",
    "            print(\"Next states: \", next_states)\n",
    "            print(\" \")\n",
    "        #Same as explained above\n",
    "        if self.ddqn: #DDQN\n",
    "            old_val = self.qnetwork_local(states).gather(-1, actions)\n",
    "            if i_episode < 3 or i_episode > 1998:\n",
    "                print(\"Q_current: \", old_val)\n",
    "                print(\" \")\n",
    "            with torch.no_grad():\n",
    "                if i_episode < 3 or i_episode > 1998:\n",
    "                    print(\"All Q values for next_states in order to choose actions, estimate with local network: \")\n",
    "                    print(self.qnetwork_local(next_states))\n",
    "                    print(\" \")\n",
    "                next_actions = self.qnetwork_local(next_states).argmax(-1, keepdim=True)\n",
    "                if i_episode < 3 or i_episode > 1998:\n",
    "                    print(\"Actions, chose with local network = \", next_actions)\n",
    "                    print(\" \")\n",
    "                    print(\"All Q values for next_states, estimate with target network: \")\n",
    "                    print(self.qnetwork_target(next_states))\n",
    "                    print(\" \")\n",
    "                maxQ = self.qnetwork_target(next_states).gather(-1, next_actions)\n",
    "                if i_episode < 3 or i_episode > 1998:\n",
    "                    print(\"Chosen maximum Q - value from actions gathered above = \", maxQ)\n",
    "                    print(\" \")\n",
    "                target = rewards+gamma*maxQ*(1-dones)\n",
    "                if i_episode < 3 or i_episode > 1998:\n",
    "                    print(\"Q_target = \", target)\n",
    "                    print(\" \")\n",
    "        else: # Normal DQN\n",
    "            old_val = self.qnetwork_local(states)[0][action]\n",
    "            if i_episode < 3 or i_episode > 1998:\n",
    "                print(\"All Q_current: \",self.qnetwork_local(states))\n",
    "                print(\" \")\n",
    "\n",
    "                print(\"Q_current: \", old_val)\n",
    "                print(\" \")\n",
    "\n",
    "            with torch.no_grad():\n",
    "                if i_episode < 3 or i_episode > 1998:\n",
    "                    print(\"All Q values for next_states = \")\n",
    "                    print(self.qnetwork_target(next_states))\n",
    "                    print(\" \")\n",
    "                maxQ = self.qnetwork_target(next_states).max(-1, keepdim=True)[0]\n",
    "                if i_episode < 3 or i_episode > 1998:\n",
    "                    print(\"Chosen maximum Q - value= \", maxQ)\n",
    "                    print(\" \")\n",
    "                target = reward+gamma*maxQ*(1-done)\n",
    "                if i_episode < 3 or i_episode > 1998:\n",
    "                    print(\"Q_target = \", target)\n",
    "                    print(\" \")\n",
    "               \n",
    "        if i_episode < 3 or i_episode > 1998:\n",
    "            print(\"Error(Q_target - Q_current): \", target - old_val)\n",
    "            print(\" \")\n",
    "        \n",
    "            # mean-squared error for regression\n",
    "        loss = F.mse_loss(old_val, target) #Calculate loss of current and target Q-values\n",
    "        loss.backward() #Gradient of loss with respect to Q-network parameters are computed\n",
    "                        #and used to update the parameters\n",
    "        self.optimizer.step() #update the neural network\n",
    "                                #step method applies optimization to update parameters\n",
    "                                #steps:\n",
    "                                #1. Optimizer uses computed gradients to update parameters\n",
    "                                #2. Adam optimizer is applied to each parameter based on the gradients and learning rate\n",
    "                                #3. Gradients are cleared back to zero as first step for next training iteration\n",
    "\n",
    "        ##UPDATE THE TARGET NETWORK##\n",
    "        if not self.ddqn:  # If DQN, update after each episode\n",
    "            self.soft_update(self.qnetwork_local, self.qnetwork_target, tau) \n",
    "        else:  # If DDQN, update after every update_every episodes\n",
    "            if (self.t_step % update_every) == 0:\n",
    "                self.soft_update(self.qnetwork_local, self.qnetwork_target, tau)\n",
    "        \n",
    "        return old_val - target #temporal difference TD error between old Q and target Q to monitor training progress\n",
    "\n",
    "    \n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        #local_model: online model, actively being trained. Weights will be copied from here\n",
    "        #target_model: use to generate target Q-values during training. Weights will be copied to here\n",
    "        #tau: interpolation parameters determine rate at which parameters of target models are updated\n",
    "        #small tau slower update, big tau faster update, less stable\n",
    "        \n",
    "        #function iterates over parameters of both target model and local model using zip\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            #for each target param - local param pair, update target param by the formula\n",
    "            # target_param = tau*local_param + (1-tau)*target_local \n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "            \n",
    "    # Function to simulate a model in an environment\n",
    "    def simulate_model(env_name, model_path):\n",
    "        # Load the environment\n",
    "        env = gym.make(env_name)\n",
    "\n",
    "        # Get environment parameters\n",
    "        state_size = env.observation_space.shape[0]\n",
    "        action_size = env.action_space.n\n",
    "\n",
    "        # Initialize the agent\n",
    "        agent = Agent(state_size, action_size, seed) \n",
    "\n",
    "        # Load the model weights\n",
    "        agent.qnetwork_local.load_state_dict(torch.load(model_path))\n",
    "        agent.qnetwork_local.eval()\n",
    "\n",
    "        # Simulate the model in the environment\n",
    "        scores = []\n",
    "        n_episodes = 100  # Number of episodes for simulation\n",
    "        max_t = 1000  # Maximum number of timesteps per episode\n",
    "\n",
    "        for i_episode in tqdm(range(1, n_episodes+1)):\n",
    "            state = env.reset()\n",
    "            score = 0\n",
    "\n",
    "            for t in range(max_t):\n",
    "                action = agent.act(state, epsilon=0.)  # Greedy action selection, no exploration\n",
    "                state, reward, done, _ = env.step(action)\n",
    "                score += reward\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            scores.append(score)\n",
    "\n",
    "        # Close the environment\n",
    "        env.close()\n",
    "\n",
    "        # Print average score\n",
    "        print(\"Average score:\", np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c721e03",
   "metadata": {},
   "source": [
    "## 6. Training parameters and environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4bc87b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = ['LunarLander-v2'] #list of environments  ,'\n",
    "#list of implementing algos, each element consists of [DDQN is enabled?, Prioritized experience replay enabled?, type of algo]\n",
    "algos = [[False, False, 'DQN']] #, [True, False, 'DDQN'], [True, True, 'PriorityDDQN']]\n",
    "n_episodes = 2000 #number of training episodes\n",
    "max_t = 1000 #maximum number of timesteps\n",
    "epsilon_start = 0.99 #starting value of epsilon greedy\n",
    "epsilon_end = 0.01 #minimum value of epsilon\n",
    "epsilon_decay = 0.995 #rate at which epsilon decays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71578dd0",
   "metadata": {},
   "source": [
    "## 7. Training implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e088cb1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed = 1\n",
      "ENVIRONMENT:----------- LunarLander-v2\n",
      "Algorithm: DQN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                      | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number:  0\n",
      "Current state:  (array([-0.03171902,  1.3444399 , -0.63850516, -0.644411  ,  0.02830621,\n",
      "        0.1061321 ,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0317,  1.3444, -0.6385, -0.6444,  0.0283,  0.1061,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0254,  1.3589, -0.6289, -0.6175,  0.0230,  0.0678,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0128,  1.3861, -0.6482, -0.5639,  0.0147,  0.1453,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0191,  1.3728, -0.6365, -0.5909,  0.0196,  0.0983,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 1., 3., 3.]])\n",
      "Rewards:  tensor([[-0.9525, -1.7017, -0.1359, -0.2772]])\n",
      "Next states:  tensor([[[-0.0381,  1.3293, -0.6385, -0.6711,  0.0336,  0.1061,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0317,  1.3444, -0.6385, -0.6444,  0.0283,  0.1061,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0191,  1.3728, -0.6365, -0.5909,  0.0196,  0.0983,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0254,  1.3589, -0.6289, -0.6175,  0.0230,  0.0678,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1051,  0.0152,  0.0810, -0.0960]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1051, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1051,  0.0152,  0.0810, -0.0960]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0810]])\n",
      " \n",
      "Q_target =  tensor([[-0.8722]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.7671]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.03805065,  1.3293431 , -0.63852185, -0.6710813 ,  0.03361032,\n",
      "        0.10609183,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0254,  1.3589, -0.6289, -0.6175,  0.0230,  0.0678,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0191,  1.3728, -0.6365, -0.5909,  0.0196,  0.0983,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0381,  1.3293, -0.6385, -0.6711,  0.0336,  0.1061,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0064,  1.3988, -0.6491, -0.5383,  0.0074,  0.1470,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[1., 3., 3., 0.]])\n",
      "Rewards:  tensor([[-1.7017, -0.2772, -0.1573, -1.0547]])\n",
      "Next states:  tensor([[[-0.0317,  1.3444, -0.6385, -0.6444,  0.0283,  0.1061,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0254,  1.3589, -0.6289, -0.6175,  0.0230,  0.0678,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0443,  1.3137, -0.6298, -0.6972,  0.0372,  0.0709,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0128,  1.3861, -0.6482, -0.5639,  0.0147,  0.1453,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1061,  0.0152,  0.0810, -0.0960]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.0960, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1061,  0.0152,  0.0811, -0.0959]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0811]])\n",
      " \n",
      "Q_target =  tensor([[-0.0771]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.0189]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.04431276,  1.313658  , -0.62978405, -0.69719845,  0.03715281,\n",
      "        0.07085605,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0064,  1.3988, -0.6491, -0.5383,  0.0074,  0.1470,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0191,  1.3728, -0.6365, -0.5909,  0.0196,  0.0983,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0128,  1.3861, -0.6482, -0.5639,  0.0147,  0.1453,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0254,  1.3589, -0.6289, -0.6175,  0.0230,  0.0678,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 3., 3., 1.]])\n",
      "Rewards:  tensor([[-1.0547, -0.2772, -0.1359, -1.7017]])\n",
      "Next states:  tensor([[[-0.0128,  1.3861, -0.6482, -0.5639,  0.0147,  0.1453,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0254,  1.3589, -0.6289, -0.6175,  0.0230,  0.0678,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0191,  1.3728, -0.6365, -0.5909,  0.0196,  0.0983,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0317,  1.3444, -0.6385, -0.6444,  0.0283,  0.1061,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1062,  0.0151,  0.0810, -0.0960]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1062, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1062,  0.0151,  0.0810, -0.0960]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0810]])\n",
      " \n",
      "Q_target =  tensor([[-0.6657]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.5595]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.05057497,  1.297373  , -0.6297933 , -0.72386795,  0.04069551,\n",
      "        0.07086049,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0317,  1.3444, -0.6385, -0.6444,  0.0283,  0.1061,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0506,  1.2974, -0.6298, -0.7239,  0.0407,  0.0709,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0443,  1.3137, -0.6298, -0.6972,  0.0372,  0.0709,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0064,  1.3988, -0.6491, -0.5383,  0.0074,  0.1470,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 2., 0., 0.]])\n",
      "Rewards:  tensor([[-0.9525,  2.9091, -0.7459, -1.0547]])\n",
      "Next states:  tensor([[[-0.0381,  1.3293, -0.6385, -0.6711,  0.0336,  0.1061,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0569,  1.2819, -0.6393, -0.6882,  0.0440,  0.0651,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0506,  1.2974, -0.6298, -0.7239,  0.0407,  0.0709,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0128,  1.3861, -0.6482, -0.5639,  0.0147,  0.1453,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1068,  0.0153,  0.0811, -0.0959]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.0811, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1068,  0.0153,  0.0811, -0.0959]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0811]])\n",
      " \n",
      "Q_target =  tensor([[2.9894]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[2.9083]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.0569355 ,  1.2818904 , -0.6393319 , -0.6882101 ,  0.04395086,\n",
      "        0.0651129 ,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0317,  1.3444, -0.6385, -0.6444,  0.0283,  0.1061,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0569,  1.2819, -0.6393, -0.6882,  0.0440,  0.0651,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0381,  1.3293, -0.6385, -0.6711,  0.0336,  0.1061,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0254,  1.3589, -0.6289, -0.6175,  0.0230,  0.0678,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 1., 3., 1.]])\n",
      "Rewards:  tensor([[-0.9525, -1.7555, -0.1573, -1.7017]])\n",
      "Next states:  tensor([[[-0.0381,  1.3293, -0.6385, -0.6711,  0.0336,  0.1061,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0634,  1.2658, -0.6502, -0.7157,  0.0494,  0.1088,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0443,  1.3137, -0.6298, -0.6972,  0.0372,  0.0709,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0317,  1.3444, -0.6385, -0.6444,  0.0283,  0.1061,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1068,  0.0154,  0.0851, -0.0959]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.0154, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1068,  0.0154,  0.0851, -0.0958]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0851]])\n",
      " \n",
      "Q_target =  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\phama\\AppData\\Local\\Temp\\ipykernel_16512\\1174236218.py:138: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = F.mse_loss(old_val, target) #Calculate loss of current and target Q-values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.6713]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-1.6867]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.06338243,  1.2657909 , -0.6501922 , -0.7156982 ,  0.04939284,\n",
      "        0.10884927,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0569,  1.2819, -0.6393, -0.6882,  0.0440,  0.0651,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0128,  1.3861, -0.6482, -0.5639,  0.0147,  0.1453,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0443,  1.3137, -0.6298, -0.6972,  0.0372,  0.0709,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0381,  1.3293, -0.6385, -0.6711,  0.0336,  0.1061,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[1., 3., 0., 3.]])\n",
      "Rewards:  tensor([[-1.7555, -0.1359, -0.7459, -0.1573]])\n",
      "Next states:  tensor([[[-0.0634,  1.2658, -0.6502, -0.7157,  0.0494,  0.1088,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0191,  1.3728, -0.6365, -0.5909,  0.0196,  0.0983,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0506,  1.2974, -0.6298, -0.7239,  0.0407,  0.0709,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0443,  1.3137, -0.6298, -0.6972,  0.0372,  0.0709,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1068,  0.0131,  0.0850, -0.0958]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1068, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1068,  0.0132,  0.0850, -0.0958]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0850]])\n",
      " \n",
      "Q_target =  tensor([[-0.8175]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.7107]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.06982956,  1.249092  , -0.65020955, -0.7423638 ,  0.05483287,\n",
      "        0.10881089,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0443,  1.3137, -0.6298, -0.6972,  0.0372,  0.0709,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0634,  1.2658, -0.6502, -0.7157,  0.0494,  0.1088,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0128,  1.3861, -0.6482, -0.5639,  0.0147,  0.1453,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0381,  1.3293, -0.6385, -0.6711,  0.0336,  0.1061,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 3., 3.]])\n",
      "Rewards:  tensor([[-0.7459, -0.9017, -0.1359, -0.1573]])\n",
      "Next states:  tensor([[[-0.0506,  1.2974, -0.6298, -0.7239,  0.0407,  0.0709,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0698,  1.2491, -0.6502, -0.7424,  0.0548,  0.1088,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0191,  1.3728, -0.6365, -0.5909,  0.0196,  0.0983,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0443,  1.3137, -0.6298, -0.6972,  0.0372,  0.0709,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1078,  0.0131,  0.0850, -0.0958]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1078, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1077,  0.0132,  0.0850, -0.0958]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0850]])\n",
      " \n",
      "Q_target =  tensor([[-0.7934]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.6857]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.07627688,  1.2317935 , -0.6502248 , -0.76903355,  0.06027256,\n",
      "        0.10880345,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0317,  1.3444, -0.6385, -0.6444,  0.0283,  0.1061,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0634,  1.2658, -0.6502, -0.7157,  0.0494,  0.1088,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0128,  1.3861, -0.6482, -0.5639,  0.0147,  0.1453,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0254,  1.3589, -0.6289, -0.6175,  0.0230,  0.0678,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 3., 1.]])\n",
      "Rewards:  tensor([[-0.9525, -0.9017, -0.1359, -1.7017]])\n",
      "Next states:  tensor([[[-0.0381,  1.3293, -0.6385, -0.6711,  0.0336,  0.1061,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0698,  1.2491, -0.6502, -0.7424,  0.0548,  0.1088,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0191,  1.3728, -0.6365, -0.5909,  0.0196,  0.0983,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0317,  1.3444, -0.6385, -0.6444,  0.0283,  0.1061,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1087,  0.0131,  0.0850, -0.0958]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1087, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1087,  0.0131,  0.0850, -0.0958]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0850]])\n",
      " \n",
      "Q_target =  tensor([[-0.7676]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.6589]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.08272447,  1.2138953 , -0.6502402 , -0.7957046 ,  0.0657114 ,\n",
      "        0.10878666,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0506,  1.2974, -0.6298, -0.7239,  0.0407,  0.0709,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0827,  1.2139, -0.6502, -0.7957,  0.0657,  0.1088,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0443,  1.3137, -0.6298, -0.6972,  0.0372,  0.0709,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0191,  1.3728, -0.6365, -0.5909,  0.0196,  0.0983,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[2., 3., 0., 3.]])\n",
      "Rewards:  tensor([[ 2.9091,  0.0685, -0.7459, -0.2772]])\n",
      "Next states:  tensor([[[-0.0569,  1.2819, -0.6393, -0.6882,  0.0440,  0.0651,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0891,  1.1954, -0.6394, -0.8219,  0.0690,  0.0652,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0506,  1.2974, -0.6298, -0.7239,  0.0407,  0.0709,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0254,  1.3589, -0.6289, -0.6175,  0.0230,  0.0678,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1095,  0.0132,  0.0850, -0.0957]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.0957, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1094,  0.0133,  0.0851, -0.0957]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0851]])\n",
      " \n",
      "Q_target =  tensor([[0.1528]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.2485]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.08908577,  1.1954055 , -0.63941705, -0.8219151 ,  0.06897019,\n",
      "        0.06518148,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0634,  1.2658, -0.6502, -0.7157,  0.0494,  0.1088,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0698,  1.2491, -0.6502, -0.7424,  0.0548,  0.1088,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0891,  1.1954, -0.6394, -0.8219,  0.0690,  0.0652,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0569,  1.2819, -0.6393, -0.6882,  0.0440,  0.0651,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 3., 1.]])\n",
      "Rewards:  tensor([[-0.9017, -0.8776,  0.1767, -1.7555]])\n",
      "Next states:  tensor([[[-0.0698,  1.2491, -0.6502, -0.7424,  0.0548,  0.1088,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0763,  1.2318, -0.6502, -0.7690,  0.0603,  0.1088,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0954,  1.1763, -0.6297, -0.8484,  0.0703,  0.0261,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0634,  1.2658, -0.6502, -0.7157,  0.0494,  0.1088,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1094,  0.0133,  0.0851, -0.0953]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.0953, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1094,  0.0134,  0.0851, -0.0953]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0851]])\n",
      " \n",
      "Q_target =  tensor([[0.2610]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.3563]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.09536924,  1.1763183 , -0.629666  , -0.8483808 ,  0.07027295,\n",
      "        0.02605741,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0254,  1.3589, -0.6289, -0.6175,  0.0230,  0.0678,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0891,  1.1954, -0.6394, -0.8219,  0.0690,  0.0652,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0954,  1.1763, -0.6297, -0.8484,  0.0703,  0.0261,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0569,  1.2819, -0.6393, -0.6882,  0.0440,  0.0651,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[1., 3., 1., 1.]])\n",
      "Rewards:  tensor([[-1.7017,  0.1767, -1.0820, -1.7555]])\n",
      "Next states:  tensor([[[-0.0317,  1.3444, -0.6385, -0.6444,  0.0283,  0.1061,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0954,  1.1763, -0.6297, -0.8484,  0.0703,  0.0261,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1017,  1.1566, -0.6378, -0.8755,  0.0732,  0.0589,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0634,  1.2658, -0.6502, -0.7157,  0.0494,  0.1088,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1094,  0.0133,  0.0852, -0.0949]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.0133, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1094,  0.0134,  0.0852, -0.0948]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0852]])\n",
      " \n",
      "Q_target =  tensor([[-0.9977]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-1.0110]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.10171795,  1.1566234 , -0.63783294, -0.8754723 ,  0.0732192 ,\n",
      "        0.05893009,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0064,  1.3988, -0.6491, -0.5383,  0.0074,  0.1470,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0191,  1.3728, -0.6365, -0.5909,  0.0196,  0.0983,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0698,  1.2491, -0.6502, -0.7424,  0.0548,  0.1088,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0891,  1.1954, -0.6394, -0.8219,  0.0690,  0.0652,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 3., 0., 3.]])\n",
      "Rewards:  tensor([[-1.0547, -0.2772, -0.8776,  0.1767]])\n",
      "Next states:  tensor([[[-0.0128,  1.3861, -0.6482, -0.5639,  0.0147,  0.1453,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0254,  1.3589, -0.6289, -0.6175,  0.0230,  0.0678,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0763,  1.2318, -0.6502, -0.7690,  0.0603,  0.1088,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0954,  1.1763, -0.6297, -0.8484,  0.0703,  0.0261,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1096,  0.0117,  0.0850, -0.0949]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.0949, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1096,  0.0118,  0.0850, -0.0949]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0850]])\n",
      " \n",
      "Q_target =  tensor([[0.4083]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.5032]], grad_fn=<SubBackward0>)\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current state:  (array([-0.1079875 ,  1.1363484 , -0.6278875 , -0.90116096,  0.07414717,\n",
      "        0.01856121,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0827,  1.2139, -0.6502, -0.7957,  0.0657,  0.1088,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0891,  1.1954, -0.6394, -0.8219,  0.0690,  0.0652,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0954,  1.1763, -0.6297, -0.8484,  0.0703,  0.0261,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0191,  1.3728, -0.6365, -0.5909,  0.0196,  0.0983,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 3., 1., 3.]])\n",
      "Rewards:  tensor([[ 0.0685,  0.1767, -1.0820, -0.2772]])\n",
      "Next states:  tensor([[[-0.0891,  1.1954, -0.6394, -0.8219,  0.0690,  0.0652,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0954,  1.1763, -0.6297, -0.8484,  0.0703,  0.0261,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1017,  1.1566, -0.6378, -0.8755,  0.0732,  0.0589,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0254,  1.3589, -0.6289, -0.6175,  0.0230,  0.0678,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1094,  0.0121,  0.0851, -0.0941]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.0851, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1093,  0.0121,  0.0852, -0.0941]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0852]])\n",
      " \n",
      "Q_target =  tensor([[3.2324]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[3.1473]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.11419334,  1.1164414 , -0.62205076, -0.88482666,  0.07559726,\n",
      "        0.02900448,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0506,  1.2974, -0.6298, -0.7239,  0.0407,  0.0709,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0317,  1.3444, -0.6385, -0.6444,  0.0283,  0.1061,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0569,  1.2819, -0.6393, -0.6882,  0.0440,  0.0651,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0381,  1.3293, -0.6385, -0.6711,  0.0336,  0.1061,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[2., 0., 1., 3.]])\n",
      "Rewards:  tensor([[ 2.9091, -0.9525, -1.7555, -0.1573]])\n",
      "Next states:  tensor([[[-0.0569,  1.2819, -0.6393, -0.6882,  0.0440,  0.0651,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0381,  1.3293, -0.6385, -0.6711,  0.0336,  0.1061,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0634,  1.2658, -0.6502, -0.7157,  0.0494,  0.1088,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0443,  1.3137, -0.6298, -0.6972,  0.0372,  0.0709,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1095,  0.0119,  0.0893, -0.0942]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.0119, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1095,  0.0120,  0.0893, -0.0942]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0893]])\n",
      " \n",
      "Q_target =  tensor([[-1.0357]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-1.0476]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.12047396,  1.0959333 , -0.6314121 , -0.9116438 ,  0.07891984,\n",
      "        0.06645775,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0763,  1.2318, -0.6502, -0.7690,  0.0603,  0.1088,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0317,  1.3444, -0.6385, -0.6444,  0.0283,  0.1061,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0381,  1.3293, -0.6385, -0.6711,  0.0336,  0.1061,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0891,  1.1954, -0.6394, -0.8219,  0.0690,  0.0652,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 3., 3.]])\n",
      "Rewards:  tensor([[-0.8517, -0.9525, -0.1573,  0.1767]])\n",
      "Next states:  tensor([[[-0.0827,  1.2139, -0.6502, -0.7957,  0.0657,  0.1088,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0381,  1.3293, -0.6385, -0.6711,  0.0336,  0.1061,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0443,  1.3137, -0.6298, -0.6972,  0.0372,  0.0709,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0954,  1.1763, -0.6297, -0.8484,  0.0703,  0.0261,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1095,  0.0106,  0.0893, -0.0941]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.0893, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1094,  0.0106,  0.0893, -0.0941]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0893]])\n",
      " \n",
      "Q_target =  tensor([[4.6102]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[4.5209]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.12664966,  1.076193  , -0.62174857, -0.87757534,  0.08306564,\n",
      "        0.08292355,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0634,  1.2658, -0.6502, -0.7157,  0.0494,  0.1088,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0128,  1.3861, -0.6482, -0.5639,  0.0147,  0.1453,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0954,  1.1763, -0.6297, -0.8484,  0.0703,  0.0261,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0443,  1.3137, -0.6298, -0.6972,  0.0372,  0.0709,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 3., 1., 0.]])\n",
      "Rewards:  tensor([[-0.9017, -0.1359, -1.0820, -0.7459]])\n",
      "Next states:  tensor([[[-0.0698,  1.2491, -0.6502, -0.7424,  0.0548,  0.1088,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0191,  1.3728, -0.6365, -0.5909,  0.0196,  0.0983,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1017,  1.1566, -0.6378, -0.8755,  0.0732,  0.0589,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0506,  1.2974, -0.6298, -0.7239,  0.0407,  0.0709,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1095,  0.0107,  0.0955, -0.0941]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.0941, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1094,  0.0108,  0.0955, -0.0941]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0955]])\n",
      " \n",
      "Q_target =  tensor([[0.3047]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.3989]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.13273707,  1.0558634 , -0.6106659 , -0.9036465 ,  0.08497662,\n",
      "        0.038223  ,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.1327,  1.0559, -0.6107, -0.9036,  0.0850,  0.0382,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0763,  1.2318, -0.6502, -0.7690,  0.0603,  0.1088,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1142,  1.1164, -0.6221, -0.8848,  0.0756,  0.0290,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0634,  1.2658, -0.6502, -0.7157,  0.0494,  0.1088,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 1., 0.]])\n",
      "Rewards:  tensor([[-0.4136, -0.8517, -1.1241, -0.9017]])\n",
      "Next states:  tensor([[[-0.1388,  1.0349, -0.6107, -0.9303,  0.0869,  0.0382,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0827,  1.2139, -0.6502, -0.7957,  0.0657,  0.1088,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1205,  1.0959, -0.6314, -0.9116,  0.0789,  0.0665,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0698,  1.2491, -0.6502, -0.7424,  0.0548,  0.1088,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1092,  0.0111,  0.0957, -0.0935]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1092, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1092,  0.0111,  0.0957, -0.0934]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0957]])\n",
      " \n",
      "Q_target =  tensor([[-0.3189]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.2097]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.13882446,  1.0349338 , -0.61067027, -0.93031085,  0.08688858,\n",
      "        0.03824282,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0381,  1.3293, -0.6385, -0.6711,  0.0336,  0.1061,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0827,  1.2139, -0.6502, -0.7957,  0.0657,  0.1088,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0191,  1.3728, -0.6365, -0.5909,  0.0196,  0.0983,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1327,  1.0559, -0.6107, -0.9036,  0.0850,  0.0382,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 3., 3., 0.]])\n",
      "Rewards:  tensor([[-0.1573,  0.0685, -0.2772, -0.4136]])\n",
      "Next states:  tensor([[[-0.0443,  1.3137, -0.6298, -0.6972,  0.0372,  0.0709,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0891,  1.1954, -0.6394, -0.8219,  0.0690,  0.0652,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0254,  1.3589, -0.6289, -0.6175,  0.0230,  0.0678,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1388,  1.0349, -0.6107, -0.9303,  0.0869,  0.0382,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1097,  0.0108,  0.0955, -0.0936]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.0955, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1097,  0.0109,  0.0956, -0.0936]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.0956]])\n",
      " \n",
      "Q_target =  tensor([[3.9380]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[3.8425]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.14496508,  1.0147454 , -0.61610997, -0.89738816,  0.08891318,\n",
      "        0.0404959 ,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.1266,  1.0762, -0.6217, -0.8776,  0.0831,  0.0829,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0763,  1.2318, -0.6502, -0.7690,  0.0603,  0.1088,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0634,  1.2658, -0.6502, -0.7157,  0.0494,  0.1088,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1017,  1.1566, -0.6378, -0.8755,  0.0732,  0.0589,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 0., 0., 3.]])\n",
      "Rewards:  tensor([[ 0.2102, -0.8517, -0.9017,  0.3241]])\n",
      "Next states:  tensor([[[-0.1327,  1.0559, -0.6107, -0.9036,  0.0850,  0.0382,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0827,  1.2139, -0.6502, -0.7957,  0.0657,  0.1088,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0698,  1.2491, -0.6502, -0.7424,  0.0548,  0.1088,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1080,  1.1363, -0.6279, -0.9012,  0.0741,  0.0186,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1096,  0.0111,  0.1009, -0.0934]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.1009, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1095,  0.0112,  0.1009, -0.0934]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1009]])\n",
      " \n",
      "Q_target =  tensor([[3.0927]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[2.9919]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.1511611 ,  0.9950971 , -0.62169874, -0.87338036,  0.09099454,\n",
      "        0.0416308 ,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.1450,  1.0147, -0.6161, -0.8974,  0.0889,  0.0405,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0064,  1.3988, -0.6491, -0.5383,  0.0074,  0.1470,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1017,  1.1566, -0.6378, -0.8755,  0.0732,  0.0589,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1205,  1.0959, -0.6314, -0.9116,  0.0789,  0.0665,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[2., 0., 3., 2.]])\n",
      "Rewards:  tensor([[ 2.9928, -1.0547,  0.3241,  4.5218]])\n",
      "Next states:  tensor([[[-0.1512,  0.9951, -0.6217, -0.8734,  0.0910,  0.0416,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0128,  1.3861, -0.6482, -0.5639,  0.0147,  0.1453,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1080,  1.1363, -0.6279, -0.9012,  0.0741,  0.0186,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1266,  1.0762, -0.6217, -0.8776,  0.0831,  0.0829,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1096,  0.0112,  0.1050, -0.0935]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.0935, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1095,  0.0113,  0.1050, -0.0935]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1050]])\n",
      " \n",
      "Q_target =  tensor([[0.3707]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.4642]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.15727977,  0.9748568 , -0.6119871 , -0.89957815,  0.0911211 ,\n",
      "        0.00253141,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0381,  1.3293, -0.6385, -0.6711,  0.0336,  0.1061,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0827,  1.2139, -0.6502, -0.7957,  0.0657,  0.1088,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0254,  1.3589, -0.6289, -0.6175,  0.0230,  0.0678,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1080,  1.1363, -0.6279, -0.9012,  0.0741,  0.0186,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 3., 1., 2.]])\n",
      "Rewards:  tensor([[-0.1573,  0.0685, -1.7017,  3.1481]])\n",
      "Next states:  tensor([[[-0.0443,  1.3137, -0.6298, -0.6972,  0.0372,  0.0709,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0891,  1.1954, -0.6394, -0.8219,  0.0690,  0.0652,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0317,  1.3444, -0.6385, -0.6444,  0.0283,  0.1061,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1142,  1.1164, -0.6221, -0.8848,  0.0756,  0.0290,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1097,  0.0110,  0.1049, -0.0929]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.0110, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1097,  0.0111,  0.1049, -0.0929]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1049]])\n",
      " \n",
      "Q_target =  tensor([[-1.1669]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-1.1779]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.1634841 ,  0.95397866, -0.6227855 , -0.92806286,  0.09345821,\n",
      "        0.04674232,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0634,  1.2658, -0.6502, -0.7157,  0.0494,  0.1088,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1205,  1.0959, -0.6314, -0.9116,  0.0789,  0.0665,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1450,  1.0147, -0.6161, -0.8974,  0.0889,  0.0405,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0128,  1.3861, -0.6482, -0.5639,  0.0147,  0.1453,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 2., 2., 3.]])\n",
      "Rewards:  tensor([[-0.9017,  4.5218,  2.9928, -0.1359]])\n",
      "Next states:  tensor([[[-0.0698,  1.2491, -0.6502, -0.7424,  0.0548,  0.1088,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1266,  1.0762, -0.6217, -0.8776,  0.0831,  0.0829,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1512,  0.9951, -0.6217, -0.8734,  0.0910,  0.0416,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0191,  1.3728, -0.6365, -0.5909,  0.0196,  0.0983,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1095,  0.0097,  0.1050, -0.0928]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.0928, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1095,  0.0098,  0.1050, -0.0928]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1050]])\n",
      " \n",
      "Q_target =  tensor([[0.3776]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.4704]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.16961709,  0.9325208 , -0.61382306, -0.953713  ,  0.09397234,\n",
      "        0.01028248,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0698,  1.2491, -0.6502, -0.7424,  0.0548,  0.1088,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0569,  1.2819, -0.6393, -0.6882,  0.0440,  0.0651,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1635,  0.9540, -0.6228, -0.9281,  0.0935,  0.0467,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1266,  1.0762, -0.6217, -0.8776,  0.0831,  0.0829,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 1., 3., 3.]])\n",
      "Rewards:  tensor([[-0.8776, -1.7555,  0.2737,  0.2102]])\n",
      "Next states:  tensor([[[-0.0763,  1.2318, -0.6502, -0.7690,  0.0603,  0.1088,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0634,  1.2658, -0.6502, -0.7157,  0.0494,  0.1088,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1696,  0.9325, -0.6138, -0.9537,  0.0940,  0.0103,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1327,  1.0559, -0.6107, -0.9036,  0.0850,  0.0382,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1096,  0.0097,  0.1050, -0.0922]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.0922, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1095,  0.0098,  0.1051, -0.0921]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1051]])\n",
      " \n",
      "Q_target =  tensor([[0.7021]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.7943]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.17566052,  0.91047436, -0.60256493, -0.9797334 ,  0.09221895,\n",
      "       -0.0350683 ,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.1450,  1.0147, -0.6161, -0.8974,  0.0889,  0.0405,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0698,  1.2491, -0.6502, -0.7424,  0.0548,  0.1088,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0381,  1.3293, -0.6385, -0.6711,  0.0336,  0.1061,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0891,  1.1954, -0.6394, -0.8219,  0.0690,  0.0652,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[2., 0., 3., 3.]])\n",
      "Rewards:  tensor([[ 2.9928, -0.8776, -0.1573,  0.1767]])\n",
      "Next states:  tensor([[[-0.1512,  0.9951, -0.6217, -0.8734,  0.0910,  0.0416,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0763,  1.2318, -0.6502, -0.7690,  0.0603,  0.1088,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0443,  1.3137, -0.6298, -0.6972,  0.0372,  0.0709,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0954,  1.1763, -0.6297, -0.8484,  0.0703,  0.0261,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1096,  0.0096,  0.1049, -0.0911]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1096, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1096,  0.0097,  0.1050, -0.0911]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1050]])\n",
      " \n",
      "Q_target =  tensor([[0.1028]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.2125]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.18170385,  0.88782793, -0.6025649 , -1.0064003 ,  0.09046554,\n",
      "       -0.03506836,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.1327,  1.0559, -0.6107, -0.9036,  0.0850,  0.0382,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1017,  1.1566, -0.6378, -0.8755,  0.0732,  0.0589,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0381,  1.3293, -0.6385, -0.6711,  0.0336,  0.1061,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0827,  1.2139, -0.6502, -0.7957,  0.0657,  0.1088,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 3., 3., 3.]])\n",
      "Rewards:  tensor([[-0.4136,  0.3241, -0.1573,  0.0685]])\n",
      "Next states:  tensor([[[-0.1388,  1.0349, -0.6107, -0.9303,  0.0869,  0.0382,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1080,  1.1363, -0.6279, -0.9012,  0.0741,  0.0186,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0443,  1.3137, -0.6298, -0.6972,  0.0372,  0.0709,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0891,  1.1954, -0.6394, -0.8219,  0.0690,  0.0652,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1093,  0.0097,  0.1049, -0.0911]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.0911, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1092,  0.0097,  0.1050, -0.0911]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1050]])\n",
      " \n",
      "Q_target =  tensor([[0.7905]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.8816]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.18767309,  0.86458784, -0.59326965, -1.0326775 ,  0.08684496,\n",
      "       -0.07241165,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0064,  1.3988, -0.6491, -0.5383,  0.0074,  0.1470,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1205,  1.0959, -0.6314, -0.9116,  0.0789,  0.0665,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1080,  1.1363, -0.6279, -0.9012,  0.0741,  0.0186,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1327,  1.0559, -0.6107, -0.9036,  0.0850,  0.0382,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 2., 2., 0.]])\n",
      "Rewards:  tensor([[-1.0547,  4.5218,  3.1481, -0.4136]])\n",
      "Next states:  tensor([[[-0.0128,  1.3861, -0.6482, -0.5639,  0.0147,  0.1453,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1266,  1.0762, -0.6217, -0.8776,  0.0831,  0.0829,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1142,  1.1164, -0.6221, -0.8848,  0.0756,  0.0290,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1388,  1.0349, -0.6107, -0.9303,  0.0869,  0.0382,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1093,  0.0096,  0.1050, -0.0899]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.0096, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1093,  0.0097,  0.1050, -0.0899]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1050]])\n",
      " \n",
      "Q_target =  tensor([[-0.3609]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.3705]], grad_fn=<SubBackward0>)\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current state:  (array([-0.19371176,  0.84073234, -0.60198605, -1.060136  ,  0.08498938,\n",
      "       -0.03711165,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.1937,  0.8407, -0.6020, -1.0601,  0.0850, -0.0371,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1573,  0.9749, -0.6120, -0.8996,  0.0911,  0.0025,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0506,  1.2974, -0.6298, -0.7239,  0.0407,  0.0709,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1080,  1.1363, -0.6279, -0.9012,  0.0741,  0.0186,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[2., 1., 2., 2.]])\n",
      "Rewards:  tensor([[ 3.3563, -1.2708,  2.9091,  3.1481]])\n",
      "Next states:  tensor([[[-0.1997,  0.8172, -0.6010, -1.0451,  0.0834, -0.0311,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1635,  0.9540, -0.6228, -0.9281,  0.0935,  0.0467,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0569,  1.2819, -0.6393, -0.6882,  0.0440,  0.0651,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1142,  1.1164, -0.6221, -0.8848,  0.0756,  0.0290,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1091,  0.0095,  0.1052, -0.0898]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.1052, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1091,  0.0095,  0.1052, -0.0898]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1052]])\n",
      " \n",
      "Q_target =  tensor([[3.4605]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[3.3553]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.19973794,  0.8172156 , -0.6010495 , -1.0450987 ,  0.08343608,\n",
      "       -0.03106565,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.1017,  1.1566, -0.6378, -0.8755,  0.0732,  0.0589,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1142,  1.1164, -0.6221, -0.8848,  0.0756,  0.0290,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0506,  1.2974, -0.6298, -0.7239,  0.0407,  0.0709,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0763,  1.2318, -0.6502, -0.7690,  0.0603,  0.1088,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 1., 2., 0.]])\n",
      "Rewards:  tensor([[ 0.3241, -1.1241,  2.9091, -0.8517]])\n",
      "Next states:  tensor([[[-0.1080,  1.1363, -0.6279, -0.9012,  0.0741,  0.0186,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1205,  1.0959, -0.6314, -0.9116,  0.0789,  0.0665,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0569,  1.2819, -0.6393, -0.6882,  0.0440,  0.0651,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0827,  1.2139, -0.6502, -0.7957,  0.0657,  0.1088,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1093,  0.0093,  0.1095, -0.0898]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1093, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1093,  0.0093,  0.1096, -0.0898]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1096]])\n",
      " \n",
      "Q_target =  tensor([[0.1364]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.2457]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.20576406,  0.79309905, -0.6010495 , -1.0717655 ,  0.0818828 ,\n",
      "       -0.0310657 ,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.1573,  0.9749, -0.6120, -0.8996,  0.0911,  0.0025,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1205,  1.0959, -0.6314, -0.9116,  0.0789,  0.0665,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0827,  1.2139, -0.6502, -0.7957,  0.0657,  0.1088,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1450,  1.0147, -0.6161, -0.8974,  0.0889,  0.0405,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[1., 2., 3., 2.]])\n",
      "Rewards:  tensor([[-1.2708,  4.5218,  0.0685,  2.9928]])\n",
      "Next states:  tensor([[[-0.1635,  0.9540, -0.6228, -0.9281,  0.0935,  0.0467,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1266,  1.0762, -0.6217, -0.8776,  0.0831,  0.0829,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0891,  1.1954, -0.6394, -0.8219,  0.0690,  0.0652,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1512,  0.9951, -0.6217, -0.8734,  0.0910,  0.0416,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1088,  0.0095,  0.1097, -0.0898]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.0898, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1088,  0.0096,  0.1097, -0.0898]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1097]])\n",
      " \n",
      "Q_target =  tensor([[0.9774]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[1.0672]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.21169567,  0.7683895 , -0.58919114, -1.0979857 ,  0.07794893,\n",
      "       -0.07867758,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.1757,  0.9105, -0.6026, -0.9797,  0.0922, -0.0351,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2117,  0.7684, -0.5892, -1.0980,  0.0779, -0.0787,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0128,  1.3861, -0.6482, -0.5639,  0.0147,  0.1453,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1512,  0.9951, -0.6217, -0.8734,  0.0910,  0.0416,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 3., 3.]])\n",
      "Rewards:  tensor([[-0.0011,  0.3097, -0.1359,  0.2667]])\n",
      "Next states:  tensor([[[-0.1817,  0.8878, -0.6026, -1.0064,  0.0905, -0.0351,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2176,  0.7431, -0.5892, -1.1247,  0.0740, -0.0787,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0191,  1.3728, -0.6365, -0.5909,  0.0196,  0.0983,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1573,  0.9749, -0.6120, -0.8996,  0.0911,  0.0025,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1088,  0.0097,  0.1099, -0.0884]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1088, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1087,  0.0097,  0.1099, -0.0884]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1099]])\n",
      " \n",
      "Q_target =  tensor([[0.4186]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.5273]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.21762729,  0.7430803 , -0.589191  , -1.1246537 ,  0.07401506,\n",
      "       -0.07867743,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.1635,  0.9540, -0.6228, -0.9281,  0.0935,  0.0467,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0634,  1.2658, -0.6502, -0.7157,  0.0494,  0.1088,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1997,  0.8172, -0.6010, -1.0451,  0.0834, -0.0311,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0891,  1.1954, -0.6394, -0.8219,  0.0690,  0.0652,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 0., 0., 3.]])\n",
      "Rewards:  tensor([[ 0.2737, -0.9017,  0.0279,  0.1767]])\n",
      "Next states:  tensor([[[-0.1696,  0.9325, -0.6138, -0.9537,  0.0940,  0.0103,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0698,  1.2491, -0.6502, -0.7424,  0.0548,  0.1088,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2058,  0.7931, -0.6010, -1.0718,  0.0819, -0.0311,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0954,  1.1763, -0.6297, -0.8484,  0.0703,  0.0261,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1081,  0.0096,  0.1098, -0.0883]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.1098, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1080,  0.0096,  0.1098, -0.0883]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1098]])\n",
      " \n",
      "Q_target =  tensor([[3.0672]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[2.9574]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.22346935,  0.71784484, -0.5808041 , -1.121414  ,  0.0706476 ,\n",
      "       -0.06734882,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0317,  1.3444, -0.6385, -0.6444,  0.0283,  0.1061,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1080,  1.1363, -0.6279, -0.9012,  0.0741,  0.0186,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2117,  0.7684, -0.5892, -1.0980,  0.0779, -0.0787,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1757,  0.9105, -0.6026, -0.9797,  0.0922, -0.0351,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 2., 0., 0.]])\n",
      "Rewards:  tensor([[-9.5248e-01,  3.1481e+00,  3.0974e-01, -1.0662e-03]])\n",
      "Next states:  tensor([[[-0.0381,  1.3293, -0.6385, -0.6711,  0.0336,  0.1061,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1142,  1.1164, -0.6221, -0.8848,  0.0756,  0.0290,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2176,  0.7431, -0.5892, -1.1247,  0.0740, -0.0787,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1817,  0.8878, -0.6026, -1.0064,  0.0905, -0.0351,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1081,  0.0096,  0.1139, -0.0884]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.0884, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1080,  0.0097,  0.1140, -0.0883]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1140]])\n",
      " \n",
      "Q_target =  tensor([[1.1907]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[1.2790]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.22921772,  0.69203156, -0.5690299 , -1.1469907 ,  0.06489893,\n",
      "       -0.11497344,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.1388,  1.0349, -0.6107, -0.9303,  0.0869,  0.0382,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1205,  1.0959, -0.6314, -0.9116,  0.0789,  0.0665,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2176,  0.7431, -0.5892, -1.1247,  0.0740, -0.0787,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0569,  1.2819, -0.6393, -0.6882,  0.0440,  0.0651,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[2., 2., 2., 1.]])\n",
      "Rewards:  tensor([[ 3.8434,  4.5218,  2.9584, -1.7555]])\n",
      "Next states:  tensor([[[-0.1450,  1.0147, -0.6161, -0.8974,  0.0889,  0.0405,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1266,  1.0762, -0.6217, -0.8776,  0.0831,  0.0829,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2235,  0.7178, -0.5808, -1.1214,  0.0706, -0.0673,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0634,  1.2658, -0.6502, -0.7157,  0.0494,  0.1088,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1080,  0.0098,  0.1139, -0.0866]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.0098, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1080,  0.0098,  0.1139, -0.0866]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1139]])\n",
      " \n",
      "Q_target =  tensor([[-0.0843]])\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error(Q_target - Q_current):  tensor([[-0.0941]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.23504606,  0.6656163 , -0.5790481 , -1.173854  ,  0.0611571 ,\n",
      "       -0.07483658,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0317,  1.3444, -0.6385, -0.6444,  0.0283,  0.1061,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1817,  0.8878, -0.6026, -1.0064,  0.0905, -0.0351,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2176,  0.7431, -0.5892, -1.1247,  0.0740, -0.0787,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1757,  0.9105, -0.6026, -0.9797,  0.0922, -0.0351,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 3., 2., 0.]])\n",
      "Rewards:  tensor([[-9.5248e-01,  6.8657e-01,  2.9584e+00, -1.0662e-03]])\n",
      "Next states:  tensor([[[-0.0381,  1.3293, -0.6385, -0.6711,  0.0336,  0.1061,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1877,  0.8646, -0.5933, -1.0327,  0.0868, -0.0724,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2235,  0.7178, -0.5808, -1.1214,  0.0706, -0.0673,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1817,  0.8878, -0.6026, -1.0064,  0.0905, -0.0351,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1080,  0.0097,  0.1141, -0.0866]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1080, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1079,  0.0097,  0.1141, -0.0866]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1141]])\n",
      " \n",
      "Q_target =  tensor([[0.4282]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.5362]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.24087434,  0.63860124, -0.57904804, -1.2005218 ,  0.05741527,\n",
      "       -0.07483649,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0634,  1.2658, -0.6502, -0.7157,  0.0494,  0.1088,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0128,  1.3861, -0.6482, -0.5639,  0.0147,  0.1453,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2409,  0.6386, -0.5790, -1.2005,  0.0574, -0.0748,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1327,  1.0559, -0.6107, -0.9036,  0.0850,  0.0382,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 3., 1., 0.]])\n",
      "Rewards:  tensor([[-0.9017, -0.1359, -0.2496, -0.4136]])\n",
      "Next states:  tensor([[[-0.0698,  1.2491, -0.6502, -0.7424,  0.0548,  0.1088,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0191,  1.3728, -0.6365, -0.5909,  0.0196,  0.0983,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2468,  0.6110, -0.5874, -1.2274,  0.0553, -0.0413,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1388,  1.0349, -0.6107, -0.9303,  0.0869,  0.0382,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1075,  0.0093,  0.1138, -0.0867]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.0093, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1075,  0.0094,  0.1138, -0.0866]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1138]])\n",
      " \n",
      "Q_target =  tensor([[-0.1369]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.1462]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.24676938,  0.6109829 , -0.58741057, -1.2273964 ,  0.05534991,\n",
      "       -0.04130721,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.1266,  1.0762, -0.6217, -0.8776,  0.0831,  0.0829,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0254,  1.3589, -0.6289, -0.6175,  0.0230,  0.0678,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0698,  1.2491, -0.6502, -0.7424,  0.0548,  0.1088,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0128,  1.3861, -0.6482, -0.5639,  0.0147,  0.1453,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 1., 0., 3.]])\n",
      "Rewards:  tensor([[ 0.2102, -1.7017, -0.8776, -0.1359]])\n",
      "Next states:  tensor([[[-0.1327,  1.0559, -0.6107, -0.9036,  0.0850,  0.0382,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0317,  1.3444, -0.6385, -0.6444,  0.0283,  0.1061,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0763,  1.2318, -0.6502, -0.7690,  0.0603,  0.1088,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0191,  1.3728, -0.6365, -0.5909,  0.0196,  0.0983,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1077,  0.0089,  0.1135, -0.0868]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.1135, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1077,  0.0089,  0.1135, -0.0867]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1135]])\n",
      " \n",
      "Q_target =  tensor([[2.6922]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[2.5788]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.25262126,  0.58341867, -0.58341414, -1.2250141 ,  0.05359471,\n",
      "       -0.03510388,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.2117,  0.7684, -0.5892, -1.0980,  0.0779, -0.0787,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1017,  1.1566, -0.6378, -0.8755,  0.0732,  0.0589,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0506,  1.2974, -0.6298, -0.7239,  0.0407,  0.0709,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0763,  1.2318, -0.6502, -0.7690,  0.0603,  0.1088,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 3., 2., 0.]])\n",
      "Rewards:  tensor([[ 0.3097,  0.3241,  2.9091, -0.8517]])\n",
      "Next states:  tensor([[[-0.2176,  0.7431, -0.5892, -1.1247,  0.0740, -0.0787,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1080,  1.1363, -0.6279, -0.9012,  0.0741,  0.0186,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0569,  1.2819, -0.6393, -0.6882,  0.0440,  0.0651,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0827,  1.2139, -0.6502, -0.7957,  0.0657,  0.1088,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1074,  0.0094,  0.1173, -0.0866]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.1173, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1074,  0.0094,  0.1173, -0.0866]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1173]])\n",
      " \n",
      "Q_target =  tensor([[6.0129]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[5.8956]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.25838393,  0.5567363 , -0.57515824, -1.1858475 ,  0.05249526,\n",
      "       -0.02198891,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.1450,  1.0147, -0.6161, -0.8974,  0.0889,  0.0405,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1877,  0.8646, -0.5933, -1.0327,  0.0868, -0.0724,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2584,  0.5567, -0.5752, -1.1858,  0.0525, -0.0220,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1327,  1.0559, -0.6107, -0.9036,  0.0850,  0.0382,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[2., 1., 2., 0.]])\n",
      "Rewards:  tensor([[ 2.9928, -0.4649,  3.9762, -0.4136]])\n",
      "Next states:  tensor([[[-0.1512,  0.9951, -0.6217, -0.8734,  0.0910,  0.0416,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1937,  0.8407, -0.6020, -1.0601,  0.0850, -0.0371,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2642,  0.5307, -0.5835, -1.1589,  0.0512, -0.0260,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1388,  1.0349, -0.6107, -0.9303,  0.0869,  0.0382,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1070,  0.0101,  0.1258, -0.0865]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.1258, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1070,  0.0101,  0.1258, -0.0865]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1258]])\n",
      " \n",
      "Q_target =  tensor([[4.1007]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[3.9749]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.26423216,  0.5306609 , -0.5835067 , -1.1588606 ,  0.05119527,\n",
      "       -0.02599956,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.1142,  1.1164, -0.6221, -0.8848,  0.0756,  0.0290,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0569,  1.2819, -0.6393, -0.6882,  0.0440,  0.0651,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1696,  0.9325, -0.6138, -0.9537,  0.0940,  0.0103,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1017,  1.1566, -0.6378, -0.8755,  0.0732,  0.0589,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[1., 1., 3., 3.]])\n",
      "Rewards:  tensor([[-1.1241, -1.7555,  0.5981,  0.3241]])\n",
      "Next states:  tensor([[[-0.1205,  1.0959, -0.6314, -0.9116,  0.0789,  0.0665,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0634,  1.2658, -0.6502, -0.7157,  0.0494,  0.1088,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1757,  0.9105, -0.6026, -0.9797,  0.0922, -0.0351,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1080,  1.1363, -0.6279, -0.9012,  0.0741,  0.0186,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1074,  0.0096,  0.1308, -0.0866]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1074, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1074,  0.0097,  0.1308, -0.0865]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1308]])\n",
      " \n",
      "Q_target =  tensor([[-0.0262]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.0813]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.2700804 ,  0.5039855 , -0.58350676, -1.1855273 ,  0.0498953 ,\n",
      "       -0.02599953,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.2117,  0.7684, -0.5892, -1.0980,  0.0779, -0.0787,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1937,  0.8407, -0.6020, -1.0601,  0.0850, -0.0371,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0064,  1.3988, -0.6491, -0.5383,  0.0074,  0.1470,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2235,  0.7178, -0.5808, -1.1214,  0.0706, -0.0673,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 2., 0., 3.]])\n",
      "Rewards:  tensor([[ 0.3097,  3.3563, -1.0547,  1.0779]])\n",
      "Next states:  tensor([[[-0.2176,  0.7431, -0.5892, -1.1247,  0.0740, -0.0787,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1997,  0.8172, -0.6010, -1.0451,  0.0834, -0.0311,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0128,  1.3861, -0.6482, -0.5639,  0.0147,  0.1453,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2292,  0.6920, -0.5690, -1.1470,  0.0649, -0.1150,  0.0000,\n",
      "           0.0000]]])\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Q_current:  tensor([[-0.1071,  0.0100,  0.1311, -0.0865]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.0100, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1071,  0.0100,  0.1311, -0.0865]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1311]])\n",
      " \n",
      "Q_target =  tensor([[-0.8592]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.8692]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.2760232 ,  0.47670487, -0.5953759 , -1.212509  ,  0.05097526,\n",
      "        0.02159915,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.1937,  0.8407, -0.6020, -1.0601,  0.0850, -0.0371,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1266,  1.0762, -0.6217, -0.8776,  0.0831,  0.0829,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0954,  1.1763, -0.6297, -0.8484,  0.0703,  0.0261,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2701,  0.5040, -0.5835, -1.1855,  0.0499, -0.0260,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[2., 3., 1., 1.]])\n",
      "Rewards:  tensor([[ 3.3563,  0.2102, -1.0820, -0.9890]])\n",
      "Next states:  tensor([[[-0.1997,  0.8172, -0.6010, -1.0451,  0.0834, -0.0311,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1327,  1.0559, -0.6107, -0.9036,  0.0850,  0.0382,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1017,  1.1566, -0.6378, -0.8755,  0.0732,  0.0589,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2760,  0.4767, -0.5954, -1.2125,  0.0510,  0.0216,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1071,  0.0089,  0.1310, -0.0864]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.0089, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1070,  0.0089,  0.1311, -0.0864]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1311]])\n",
      " \n",
      "Q_target =  tensor([[-1.1027]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-1.1116]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.2820627 ,  0.4488265 , -0.6075002 , -1.2391658 ,  0.05447989,\n",
      "        0.07009275,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.1450,  1.0147, -0.6161, -0.8974,  0.0889,  0.0405,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0954,  1.1763, -0.6297, -0.8484,  0.0703,  0.0261,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1327,  1.0559, -0.6107, -0.9036,  0.0850,  0.0382,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0569,  1.2819, -0.6393, -0.6882,  0.0440,  0.0651,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[2., 1., 0., 1.]])\n",
      "Rewards:  tensor([[ 2.9928, -1.0820, -0.4136, -1.7555]])\n",
      "Next states:  tensor([[[-0.1512,  0.9951, -0.6217, -0.8734,  0.0910,  0.0416,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1017,  1.1566, -0.6378, -0.8755,  0.0732,  0.0589,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1388,  1.0349, -0.6107, -0.9303,  0.0869,  0.0382,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0634,  1.2658, -0.6502, -0.7157,  0.0494,  0.1088,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1073,  0.0070,  0.1307, -0.0865]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1073, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1073,  0.0071,  0.1307, -0.0865]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1307]])\n",
      " \n",
      "Q_target =  tensor([[-0.5709]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.4636]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.2881023 ,  0.42034817, -0.60750014, -1.2658334 ,  0.05798452,\n",
      "        0.07009272,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.1205,  1.0959, -0.6314, -0.9116,  0.0789,  0.0665,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0254,  1.3589, -0.6289, -0.6175,  0.0230,  0.0678,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0891,  1.1954, -0.6394, -0.8219,  0.0690,  0.0652,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1696,  0.9325, -0.6138, -0.9537,  0.0940,  0.0103,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[2., 1., 3., 3.]])\n",
      "Rewards:  tensor([[ 4.5218, -1.7017,  0.1767,  0.5981]])\n",
      "Next states:  tensor([[[-0.1266,  1.0762, -0.6217, -0.8776,  0.0831,  0.0829,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0317,  1.3444, -0.6385, -0.6444,  0.0283,  0.1061,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0954,  1.1763, -0.6297, -0.8484,  0.0703,  0.0261,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1757,  0.9105, -0.6026, -0.9797,  0.0922, -0.0351,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1081,  0.0069,  0.1306, -0.0865]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.1306, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1080,  0.0069,  0.1306, -0.0865]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1306]])\n",
      " \n",
      "Q_target =  tensor([[2.1170]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[1.9864]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.2942647 ,  0.39216664, -0.6193711 , -1.252639  ,  0.06108151,\n",
      "        0.06193958,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.1450,  1.0147, -0.6161, -0.8974,  0.0889,  0.0405,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1142,  1.1164, -0.6221, -0.8848,  0.0756,  0.0290,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2235,  0.7178, -0.5808, -1.1214,  0.0706, -0.0673,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2642,  0.5307, -0.5835, -1.1589,  0.0512, -0.0260,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[2., 1., 3., 0.]])\n",
      "Rewards:  tensor([[ 2.9928, -1.1241,  1.0779, -0.1557]])\n",
      "Next states:  tensor([[[-0.1512,  0.9951, -0.6217, -0.8734,  0.0910,  0.0416,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1205,  1.0959, -0.6314, -0.9116,  0.0789,  0.0665,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2292,  0.6920, -0.5690, -1.1470,  0.0649, -0.1150,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2701,  0.5040, -0.5835, -1.1855,  0.0499, -0.0260,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1076,  0.0076,  0.1338, -0.0864]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.0076, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1076,  0.0076,  0.1339, -0.0864]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1339]])\n",
      " \n",
      "Q_target =  tensor([[-1.5043]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-1.5119]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.30052406,  0.3633879 , -0.63152134, -1.2792934 ,  0.06660687,\n",
      "        0.11050709,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.2292,  0.6920, -0.5690, -1.1470,  0.0649, -0.1150,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2760,  0.4767, -0.5954, -1.2125,  0.0510,  0.0216,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2821,  0.4488, -0.6075, -1.2392,  0.0545,  0.0701,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2701,  0.5040, -0.5835, -1.1855,  0.0499, -0.0260,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[1., 1., 0., 1.]])\n",
      "Rewards:  tensor([[-0.1971, -1.2325, -0.7003, -0.9890]])\n",
      "Next states:  tensor([[[-0.2350,  0.6656, -0.5790, -1.1739,  0.0612, -0.0748,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2821,  0.4488, -0.6075, -1.2392,  0.0545,  0.0701,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2881,  0.4203, -0.6075, -1.2658,  0.0580,  0.0701,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2760,  0.4767, -0.5954, -1.2125,  0.0510,  0.0216,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1072,  0.0062,  0.1343, -0.0862]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.0862, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1072,  0.0063,  0.1343, -0.0861]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1343]])\n",
      " \n",
      "Q_target =  tensor([[-0.3804]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.2942]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.30670696,  0.33401957, -0.62191886, -1.3054258 ,  0.07019997,\n",
      "        0.07186188,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.2235,  0.7178, -0.5808, -1.1214,  0.0706, -0.0673,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0506,  1.2974, -0.6298, -0.7239,  0.0407,  0.0709,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0191,  1.3728, -0.6365, -0.5909,  0.0196,  0.0983,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2409,  0.6386, -0.5790, -1.2005,  0.0574, -0.0748,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 2., 3., 1.]])\n",
      "Rewards:  tensor([[ 1.0779,  2.9091, -0.2772, -0.2496]])\n",
      "Next states:  tensor([[[-0.2292,  0.6920, -0.5690, -1.1470,  0.0649, -0.1150,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0569,  1.2819, -0.6393, -0.6882,  0.0440,  0.0651,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0254,  1.3589, -0.6289, -0.6175,  0.0230,  0.0678,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2468,  0.6110, -0.5874, -1.2274,  0.0553, -0.0413,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1079,  0.0052,  0.1335, -0.0869]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.1335, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1079,  0.0052,  0.1336, -0.0869]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1336]])\n",
      " \n",
      "Q_target =  tensor([[3.9868]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[3.8533]], grad_fn=<SubBackward0>)\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current state:  (array([-0.3130253 ,  0.305517  , -0.63515216, -1.2669421 ,  0.07348399,\n",
      "        0.06568059,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.1388,  1.0349, -0.6107, -0.9303,  0.0869,  0.0382,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2526,  0.5834, -0.5834, -1.2250,  0.0536, -0.0351,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1450,  1.0147, -0.6161, -0.8974,  0.0889,  0.0405,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1205,  1.0959, -0.6314, -0.9116,  0.0789,  0.0665,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[2., 2., 2., 2.]])\n",
      "Rewards:  tensor([[3.8434, 5.8967, 2.9928, 4.5218]])\n",
      "Next states:  tensor([[[-0.1450,  1.0147, -0.6161, -0.8974,  0.0889,  0.0405,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2584,  0.5567, -0.5752, -1.1858,  0.0525, -0.0220,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1512,  0.9951, -0.6217, -0.8734,  0.0910,  0.0416,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1266,  1.0762, -0.6217, -0.8776,  0.0831,  0.0829,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1076,  0.0057,  0.1390, -0.0867]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1076, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1076,  0.0057,  0.1390, -0.0868]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1390]])\n",
      " \n",
      "Q_target =  tensor([[-1.0747]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.9671]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.31934363,  0.2764145 , -0.63515204, -1.2936097 ,  0.07676803,\n",
      "        0.06568053,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0381,  1.3293, -0.6385, -0.6711,  0.0336,  0.1061,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0763,  1.2318, -0.6502, -0.7690,  0.0603,  0.1088,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1080,  1.1363, -0.6279, -0.9012,  0.0741,  0.0186,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0064,  1.3988, -0.6491, -0.5383,  0.0074,  0.1470,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 0., 2., 0.]])\n",
      "Rewards:  tensor([[-0.1573, -0.8517,  3.1481, -1.0547]])\n",
      "Next states:  tensor([[[-0.0443,  1.3137, -0.6298, -0.6972,  0.0372,  0.0709,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0827,  1.2139, -0.6502, -0.7957,  0.0657,  0.1088,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1142,  1.1164, -0.6221, -0.8848,  0.0756,  0.0290,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0128,  1.3861, -0.6482, -0.5639,  0.0147,  0.1453,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1095,  0.0048,  0.1384, -0.0869]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.1384, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1094,  0.0049,  0.1384, -0.0869]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1384]])\n",
      " \n",
      "Q_target =  tensor([[2.4605]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[2.3221]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.32556635,  0.2476375 , -0.62625456, -1.2791883 ,  0.08071131,\n",
      "        0.07886563,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0698,  1.2491, -0.6502, -0.7424,  0.0548,  0.1088,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1017,  1.1566, -0.6378, -0.8755,  0.0732,  0.0589,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0254,  1.3589, -0.6289, -0.6175,  0.0230,  0.0678,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0443,  1.3137, -0.6298, -0.6972,  0.0372,  0.0709,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 3., 1., 0.]])\n",
      "Rewards:  tensor([[-0.8776,  0.3241, -1.7017, -0.7459]])\n",
      "Next states:  tensor([[[-0.0763,  1.2318, -0.6502, -0.7690,  0.0603,  0.1088,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1080,  1.1363, -0.6279, -0.9012,  0.0741,  0.0186,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0317,  1.3444, -0.6385, -0.6444,  0.0283,  0.1061,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0506,  1.2974, -0.6298, -0.7239,  0.0407,  0.0709,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1095,  0.0049,  0.1416, -0.0869]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1095, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1094,  0.0050,  0.1416, -0.0869]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1416]])\n",
      " \n",
      "Q_target =  tensor([[-1.4637]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-1.3543]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.33178908,  0.21826066, -0.62625444, -1.3058562 ,  0.08465458,\n",
      "        0.07886562,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.2176,  0.7431, -0.5892, -1.1247,  0.0740, -0.0787,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2409,  0.6386, -0.5790, -1.2005,  0.0574, -0.0748,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1388,  1.0349, -0.6107, -0.9303,  0.0869,  0.0382,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0954,  1.1763, -0.6297, -0.8484,  0.0703,  0.0261,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[2., 1., 2., 1.]])\n",
      "Rewards:  tensor([[ 2.9584, -0.2496,  3.8434, -1.0820]])\n",
      "Next states:  tensor([[[-0.2235,  0.7178, -0.5808, -1.1214,  0.0706, -0.0673,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2468,  0.6110, -0.5874, -1.2274,  0.0553, -0.0413,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1450,  1.0147, -0.6161, -0.8974,  0.0889,  0.0405,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1017,  1.1566, -0.6378, -0.8755,  0.0732,  0.0589,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1106,  0.0059,  0.1422, -0.0866]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.0059, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1106,  0.0059,  0.1423, -0.0866]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1423]])\n",
      " \n",
      "Q_target =  tensor([[-2.2258]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-2.2317]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.33808047,  0.18828379, -0.63484997, -1.3326395 ,  0.09031674,\n",
      "        0.11324326,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.1696,  0.9325, -0.6138, -0.9537,  0.0940,  0.0103,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2642,  0.5307, -0.5835, -1.1589,  0.0512, -0.0260,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1450,  1.0147, -0.6161, -0.8974,  0.0889,  0.0405,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1142,  1.1164, -0.6221, -0.8848,  0.0756,  0.0290,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 0., 2., 1.]])\n",
      "Rewards:  tensor([[ 0.5981, -0.1557,  2.9928, -1.1241]])\n",
      "Next states:  tensor([[[-0.1757,  0.9105, -0.6026, -0.9797,  0.0922, -0.0351,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2701,  0.5040, -0.5835, -1.1855,  0.0499, -0.0260,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1512,  0.9951, -0.6217, -0.8734,  0.0910,  0.0416,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1205,  1.0959, -0.6314, -0.9116,  0.0789,  0.0665,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1107,  0.0029,  0.1421, -0.0867]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.0029, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1107,  0.0029,  0.1422, -0.0866]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1422]])\n",
      " \n",
      "Q_target =  tensor([[-2.6130]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-2.6158]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.3444427 ,  0.15770882, -0.64372605, -1.3593621 ,  0.09775099,\n",
      "        0.14868493,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0827,  1.2139, -0.6502, -0.7957,  0.0657,  0.1088,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3444,  0.1577, -0.6437, -1.3594,  0.0978,  0.1487,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3130,  0.3055, -0.6352, -1.2669,  0.0735,  0.0657,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3381,  0.1883, -0.6348, -1.3326,  0.0903,  0.1132,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 2., 0., 1.]])\n",
      "Rewards:  tensor([[ 0.0685,  2.2553, -1.2122, -2.7537]])\n",
      "Next states:  tensor([[[-0.0891,  1.1954, -0.6394, -0.8219,  0.0690,  0.0652,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3510,  0.1280, -0.6631, -1.3195,  0.1047,  0.1394,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3193,  0.2764, -0.6352, -1.2936,  0.0768,  0.0657,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3444,  0.1577, -0.6437, -1.3594,  0.0978,  0.1487,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-1.1033e-01, -7.9862e-05,  1.4251e-01, -8.6328e-02]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.1425, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-1.1031e-01, -3.1753e-05,  1.4256e-01, -8.6319e-02]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1426]])\n",
      " \n",
      "Q_target =  tensor([[2.3964]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[2.2539]], grad_fn=<SubBackward0>)\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current state:  (array([-0.35100308,  0.12802993, -0.66306055, -1.3195344 ,  0.10472268,\n",
      "        0.13943376,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.3005,  0.3634, -0.6315, -1.2793,  0.0666,  0.1105,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2584,  0.5567, -0.5752, -1.1858,  0.0525, -0.0220,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0317,  1.3444, -0.6385, -0.6444,  0.0283,  0.1061,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2821,  0.4488, -0.6075, -1.2392,  0.0545,  0.0701,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 2., 0., 0.]])\n",
      "Rewards:  tensor([[-0.5133,  3.9762, -0.9525, -0.7003]])\n",
      "Next states:  tensor([[[-0.3067,  0.3340, -0.6219, -1.3054,  0.0702,  0.0719,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2642,  0.5307, -0.5835, -1.1589,  0.0512, -0.0260,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0381,  1.3293, -0.6385, -0.6711,  0.0336,  0.1061,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2881,  0.4203, -0.6075, -1.2658,  0.0580,  0.0701,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1105, -0.0003,  0.1453, -0.0865]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1105, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1105, -0.0003,  0.1453, -0.0865]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1453]])\n",
      " \n",
      "Q_target =  tensor([[-2.6473]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-2.5367]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.35756344,  0.09775172, -0.66305995, -1.3462054 ,  0.11169434,\n",
      "        0.13943326,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.2881,  0.4203, -0.6075, -1.2658,  0.0580,  0.0701,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2292,  0.6920, -0.5690, -1.1470,  0.0649, -0.1150,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1142,  1.1164, -0.6221, -0.8848,  0.0756,  0.0290,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1877,  0.8646, -0.5933, -1.0327,  0.0868, -0.0724,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[2., 1., 1., 1.]])\n",
      "Rewards:  tensor([[ 1.9877, -0.1971, -1.1241, -0.4649]])\n",
      "Next states:  tensor([[[-0.2943,  0.3922, -0.6194, -1.2526,  0.0611,  0.0619,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2350,  0.6656, -0.5790, -1.1739,  0.0612, -0.0748,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1205,  1.0959, -0.6314, -0.9116,  0.0789,  0.0665,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1937,  0.8407, -0.6020, -1.0601,  0.0850, -0.0371,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1139, -0.0003,  0.1453, -0.0864]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1139, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1139, -0.0003,  0.1453, -0.0864]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1453]])\n",
      " \n",
      "Q_target =  tensor([[-2.9034]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-2.7895]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.3641239 ,  0.06687427, -0.66305923, -1.372876  ,  0.11866598,\n",
      "        0.13943276,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.3318,  0.2183, -0.6263, -1.3059,  0.0847,  0.0789,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1142,  1.1164, -0.6221, -0.8848,  0.0756,  0.0290,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2821,  0.4488, -0.6075, -1.2392,  0.0545,  0.0701,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3256,  0.2476, -0.6263, -1.2792,  0.0807,  0.0789,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[1., 1., 0., 0.]])\n",
      "Rewards:  tensor([[-2.3666, -1.1241, -0.7003, -1.6039]])\n",
      "Next states:  tensor([[[-0.3381,  0.1883, -0.6348, -1.3326,  0.0903,  0.1132,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1205,  1.0959, -0.6314, -0.9116,  0.0789,  0.0665,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2881,  0.4203, -0.6075, -1.2658,  0.0580,  0.0701,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3318,  0.2183, -0.6263, -1.3059,  0.0847,  0.0789,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-1.1751e-01, -4.8645e-05,  1.4549e-01, -8.6240e-02]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.1455, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-1.1747e-01, -6.9551e-06,  1.4550e-01, -8.6182e-02]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1455]])\n",
      " \n",
      "Q_target =  tensor([[0.9659]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.8204]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.3709158 ,  0.03675507, -0.6856311 , -1.3391564 ,  0.12506929,\n",
      "        0.12806614,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.3005,  0.3634, -0.6315, -1.2793,  0.0666,  0.1105,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2468,  0.6110, -0.5874, -1.2274,  0.0553, -0.0413,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0763,  1.2318, -0.6502, -0.7690,  0.0603,  0.1088,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1937,  0.8407, -0.6020, -1.0601,  0.0850, -0.0371,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 2., 0., 2.]])\n",
      "Rewards:  tensor([[-0.5133,  2.5799, -0.8517,  3.3563]])\n",
      "Next states:  tensor([[[-0.3067,  0.3340, -0.6219, -1.3054,  0.0702,  0.0719,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2526,  0.5834, -0.5834, -1.2250,  0.0536, -0.0351,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0827,  1.2139, -0.6502, -0.7957,  0.0657,  0.1088,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1997,  0.8172, -0.6010, -1.0451,  0.0834, -0.0311,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1177, -0.0003,  0.1463, -0.0863]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1177, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1177, -0.0003,  0.1464, -0.0863]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1464]])\n",
      " \n",
      "Q_target =  tensor([[-3.3765]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-3.2588]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.37770778,  0.00603641, -0.68563044, -1.3658264 ,  0.13147257,\n",
      "        0.12806575,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.2943,  0.3922, -0.6194, -1.2526,  0.0611,  0.0619,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3256,  0.2476, -0.6263, -1.2792,  0.0807,  0.0789,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2468,  0.6110, -0.5874, -1.2274,  0.0553, -0.0413,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2409,  0.6386, -0.5790, -1.2005,  0.0574, -0.0748,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[1., 0., 2., 1.]])\n",
      "Rewards:  tensor([[-1.6369, -1.6039,  2.5799, -0.2496]])\n",
      "Next states:  tensor([[[-0.3005,  0.3634, -0.6315, -1.2793,  0.0666,  0.1105,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3318,  0.2183, -0.6263, -1.3059,  0.0847,  0.0789,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2526,  0.5834, -0.5834, -1.2250,  0.0536, -0.0351,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2468,  0.6110, -0.5874, -1.2274,  0.0553, -0.0413,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1217,  0.0002,  0.1468, -0.0861]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.0002, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1217,  0.0003,  0.1468, -0.0861]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1468]])\n",
      " \n",
      "Q_target =  tensor([[-4.5805]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-4.5807]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.3845931 , -0.02531128, -0.69737226, -1.3940334 ,  0.14028342,\n",
      "        0.17621687,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.1266,  1.0762, -0.6217, -0.8776,  0.0831,  0.0829,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3444,  0.1577, -0.6437, -1.3594,  0.0978,  0.1487,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1080,  1.1363, -0.6279, -0.9012,  0.0741,  0.0186,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0064,  1.3988, -0.6491, -0.5383,  0.0074,  0.1470,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 2., 2., 0.]])\n",
      "Rewards:  tensor([[ 0.2102,  2.2553,  3.1481, -1.0547]])\n",
      "Next states:  tensor([[[-0.1327,  1.0559, -0.6107, -0.9036,  0.0850,  0.0382,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3510,  0.1280, -0.6631, -1.3195,  0.1047,  0.1394,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1142,  1.1164, -0.6221, -0.8848,  0.0756,  0.0290,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0128,  1.3861, -0.6482, -0.5639,  0.0147,  0.1453,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1223, -0.0067,  0.1459, -0.0863]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1223, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1223, -0.0067,  0.1460, -0.0863]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1460]])\n",
      " \n",
      "Q_target =  tensor([[15.8517]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[15.9740]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.39147845, -0.05725793, -0.6973709 , -1.4207064 ,  0.14909421,\n",
      "        0.17621587,  1.        ,  1.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.2821,  0.4488, -0.6075, -1.2392,  0.0545,  0.0701,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2058,  0.7931, -0.6010, -1.0718,  0.0819, -0.0311,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3256,  0.2476, -0.6263, -1.2792,  0.0807,  0.0789,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3005,  0.3634, -0.6315, -1.2793,  0.0666,  0.1105,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 3., 0., 3.]])\n",
      "Rewards:  tensor([[-0.7003,  0.8688, -1.6039, -0.5133]])\n",
      "Next states:  tensor([[[-0.2881,  0.4203, -0.6075, -1.2658,  0.0580,  0.0701,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2117,  0.7684, -0.5892, -1.0980,  0.0779, -0.0787,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3318,  0.2183, -0.6263, -1.3059,  0.0847,  0.0789,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3067,  0.3340, -0.6219, -1.3054,  0.0702,  0.0719,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1003, -0.0059,  0.1466, -0.0864]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.1466, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1003, -0.0059,  0.1467, -0.0864]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1467]])\n",
      " \n",
      "Q_target =  tensor([[9.1330]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[8.9864]], grad_fn=<SubBackward0>)\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 1/2000 [00:01<1:04:34,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current state:  (array([-0.3982746 , -0.08627418, -0.69478434, -1.2914861 ,  0.1604691 ,\n",
      "        0.22264414,  1.        ,  1.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.3318,  0.2183, -0.6263, -1.3059,  0.0847,  0.0789,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1817,  0.8878, -0.6026, -1.0064,  0.0905, -0.0351,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3193,  0.2764, -0.6352, -1.2936,  0.0768,  0.0657,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0317,  1.3444, -0.6385, -0.6444,  0.0283,  0.1061,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[1., 3., 2., 0.]])\n",
      "Rewards:  tensor([[-2.3666,  0.6866,  2.3234, -0.9525]])\n",
      "Next states:  tensor([[[-0.3381,  0.1883, -0.6348, -1.3326,  0.0903,  0.1132,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1877,  0.8646, -0.5933, -1.0327,  0.0868, -0.0724,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3256,  0.2476, -0.6263, -1.2792,  0.0807,  0.0789,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0381,  1.3293, -0.6385, -0.6711,  0.0336,  0.1061,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1004, -0.0059,  0.1587, -0.0865]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.0059, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1004, -0.0058,  0.1587, -0.0865]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1587]])\n",
      " \n",
      "Q_target =  tensor([[-100.]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-99.9941]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Score:  -64.13998657742175\n",
      "Iteration number:  1\n",
      "Current state:  (array([-0.00269213,  1.4000256 , -0.27269295, -0.4842059 ,  0.00312623,\n",
      "        0.06176902,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.2058,  0.7931, -0.6010, -1.0718,  0.0819, -0.0311,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0827,  1.2139, -0.6502, -0.7957,  0.0657,  0.1088,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0443,  1.3137, -0.6298, -0.6972,  0.0372,  0.0709,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0064,  1.3988, -0.6491, -0.5383,  0.0074,  0.1470,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 3., 0., 0.]])\n",
      "Rewards:  tensor([[ 0.8688,  0.0685, -0.7459, -1.0547]])\n",
      "Next states:  tensor([[[-0.2117,  0.7684, -0.5892, -1.0980,  0.0779, -0.0787,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0891,  1.1954, -0.6394, -0.8219,  0.0690,  0.0652,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0506,  1.2974, -0.6298, -0.7239,  0.0407,  0.0709,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0128,  1.3861, -0.6482, -0.5639,  0.0147,  0.1453,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1014, -0.1400,  0.1544, -0.0857]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.1544, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1013, -0.1399,  0.1544, -0.0857]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1544]])\n",
      " \n",
      "Q_target =  tensor([[1.0790]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.9246]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.00542774,  1.3892932 , -0.27644405, -0.47700965,  0.00597764,\n",
      "        0.05703409,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0634,  1.2658, -0.6502, -0.7157,  0.0494,  0.1088,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1635,  0.9540, -0.6228, -0.9281,  0.0935,  0.0467,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0827,  1.2139, -0.6502, -0.7957,  0.0657,  0.1088,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3777,  0.0060, -0.6856, -1.3658,  0.1315,  0.1281,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 3., 3., 1.]])\n",
      "Rewards:  tensor([[-0.9017,  0.2737,  0.0685, -4.7258]])\n",
      "Next states:  tensor([[[-0.0698,  1.2491, -0.6502, -0.7424,  0.0548,  0.1088,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1696,  0.9325, -0.6138, -0.9537,  0.0940,  0.0103,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0891,  1.1954, -0.6394, -0.8219,  0.0690,  0.0652,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3846, -0.0253, -0.6974, -1.3940,  0.1403,  0.1762,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1012, -0.1397,  0.1560, -0.0855]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.1560, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1011, -0.1396,  0.1560, -0.0855]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1560]])\n",
      " \n",
      "Q_target =  tensor([[2.5254]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[2.3694]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.00825911,  1.3791863 , -0.28557846, -0.4492028 ,  0.00839131,\n",
      "        0.04827814,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.1937,  0.8407, -0.6020, -1.0601,  0.0850, -0.0371,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0954,  1.1763, -0.6297, -0.8484,  0.0703,  0.0261,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0191,  1.3728, -0.6365, -0.5909,  0.0196,  0.0983,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0506,  1.2974, -0.6298, -0.7239,  0.0407,  0.0709,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[2., 1., 3., 2.]])\n",
      "Rewards:  tensor([[ 3.3563, -1.0820, -0.2772,  2.9091]])\n",
      "Next states:  tensor([[[-0.1997,  0.8172, -0.6010, -1.0451,  0.0834, -0.0311,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1017,  1.1566, -0.6378, -0.8755,  0.0732,  0.0589,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0254,  1.3589, -0.6289, -0.6175,  0.0230,  0.0678,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0569,  1.2819, -0.6393, -0.6882,  0.0440,  0.0651,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1014, -0.1399,  0.1589, -0.0857]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1014, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1013, -0.1398,  0.1590, -0.0856]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1590]])\n",
      " \n",
      "Q_target =  tensor([[-1.2856]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-1.1843]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.01109047,  1.3684794 , -0.28558543, -0.47588208,  0.01080501,\n",
      "        0.04827849,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.1937,  0.8407, -0.6020, -1.0601,  0.0850, -0.0371,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1573,  0.9749, -0.6120, -0.8996,  0.0911,  0.0025,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2468,  0.6110, -0.5874, -1.2274,  0.0553, -0.0413,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0763,  1.2318, -0.6502, -0.7690,  0.0603,  0.1088,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[2., 1., 2., 0.]])\n",
      "Rewards:  tensor([[ 3.3563, -1.2708,  2.5799, -0.8517]])\n",
      "Next states:  tensor([[[-0.1997,  0.8172, -0.6010, -1.0451,  0.0834, -0.0311,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1635,  0.9540, -0.6228, -0.9281,  0.0935,  0.0467,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2526,  0.5834, -0.5834, -1.2250,  0.0536, -0.0351,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0827,  1.2139, -0.6502, -0.7957,  0.0657,  0.1088,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1025, -0.1393,  0.1594, -0.0855]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.0855, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1025, -0.1392,  0.1594, -0.0854]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1594]])\n",
      " \n",
      "Q_target =  tensor([[-0.6908]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.6053]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.01385613,  1.3571796 , -0.27732706, -0.5022203 ,  0.01155942,\n",
      "        0.01508954,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.3510,  0.1280, -0.6631, -1.3195,  0.1047,  0.1394,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1266,  1.0762, -0.6217, -0.8776,  0.0831,  0.0829,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3777,  0.0060, -0.6856, -1.3658,  0.1315,  0.1281,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2584,  0.5567, -0.5752, -1.1858,  0.0525, -0.0220,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 3., 1., 2.]])\n",
      "Rewards:  tensor([[-2.7911,  0.2102, -4.7258,  3.9762]])\n",
      "Next states:  tensor([[[-0.3576,  0.0978, -0.6631, -1.3462,  0.1117,  0.1394,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1327,  1.0559, -0.6107, -0.9036,  0.0850,  0.0382,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3846, -0.0253, -0.6974, -1.3940,  0.1403,  0.1762,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2642,  0.5307, -0.5835, -1.1589,  0.0512, -0.0260,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1020, -0.1386,  0.1599, -0.0860]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1386, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1020, -0.1386,  0.1599, -0.0859]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1599]])\n",
      " \n",
      "Q_target =  tensor([[-1.9259]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-1.7873]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.01671553,  1.3452733 , -0.2890839 , -0.5291942 ,  0.01467264,\n",
      "        0.06227007,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.3641,  0.0669, -0.6631, -1.3729,  0.1187,  0.1394,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2117,  0.7684, -0.5892, -1.0980,  0.0779, -0.0787,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1388,  1.0349, -0.6107, -0.9303,  0.0869,  0.0382,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1757,  0.9105, -0.6026, -0.9797,  0.0922, -0.0351,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[2., 0., 2., 0.]])\n",
      "Rewards:  tensor([[ 8.2186e-01,  3.0974e-01,  3.8434e+00, -1.0662e-03]])\n",
      "Next states:  tensor([[[-0.3709,  0.0368, -0.6856, -1.3392,  0.1251,  0.1281,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2176,  0.7431, -0.5892, -1.1247,  0.0740, -0.0787,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1450,  1.0147, -0.6161, -0.8974,  0.0889,  0.0405,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1817,  0.8878, -0.6026, -1.0064,  0.0905, -0.0351,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1022, -0.1413,  0.1595, -0.0861]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.1595, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1022, -0.1412,  0.1596, -0.0861]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1596]])\n",
      " \n",
      "Q_target =  tensor([[0.9186]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.7590]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.01967678,  1.3335298 , -0.29880437, -0.52196395,  0.0173344 ,\n",
      "        0.05323994,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0167,  1.3453, -0.2891, -0.5292,  0.0147,  0.0623,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1142,  1.1164, -0.6221, -0.8848,  0.0756,  0.0290,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3983, -0.0863, -0.6948, -1.2915,  0.1605,  0.2226,  1.0000,\n",
      "           1.0000],\n",
      "         [-0.1817,  0.8878, -0.6026, -1.0064,  0.0905, -0.0351,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[2., 1., 1., 3.]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards:  tensor([[   0.7606,   -1.1241, -100.0000,    0.6866]])\n",
      "Next states:  tensor([[[-0.0197,  1.3335, -0.2988, -0.5220,  0.0173,  0.0532,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1205,  1.0959, -0.6314, -0.9116,  0.0789,  0.0665,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.4047, -0.0955, -0.5261, -0.0150,  0.0292, -0.3989,  1.0000,\n",
      "           1.0000],\n",
      "         [-0.1877,  0.8646, -0.5933, -1.0327,  0.0868, -0.0724,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1027, -0.1405,  0.1610, -0.0860]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1405, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1028, -0.1405,  0.1612, -0.0866]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1612]])\n",
      " \n",
      "Q_target =  tensor([[-1.9300]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-1.7895]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.022717  ,  1.3211827 , -0.30871946, -0.5488111 ,  0.02198387,\n",
      "        0.09299751,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.3318,  0.2183, -0.6263, -1.3059,  0.0847,  0.0789,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0111,  1.3685, -0.2856, -0.4759,  0.0108,  0.0483,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3915, -0.0573, -0.6974, -1.4207,  0.1491,  0.1762,  1.0000,\n",
      "           1.0000],\n",
      "         [-0.3846, -0.0253, -0.6974, -1.3940,  0.1403,  0.1762,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[1., 3., 2., 0.]])\n",
      "Rewards:  tensor([[-2.3666, -0.8486,  8.9878, 15.7072]])\n",
      "Next states:  tensor([[[-0.3381,  0.1883, -0.6348, -1.3326,  0.0903,  0.1132,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0139,  1.3572, -0.2773, -0.5022,  0.0116,  0.0151,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3983, -0.0863, -0.6948, -1.2915,  0.1605,  0.2226,  1.0000,\n",
      "           1.0000],\n",
      "         [-0.3915, -0.0573, -0.6974, -1.4207,  0.1491,  0.1762,  1.0000,\n",
      "           1.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1022, -0.1423,  0.1614, -0.0857]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.0857, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1023, -0.1415,  0.1619, -0.0855]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1619]])\n",
      " \n",
      "Q_target =  tensor([[-0.8728]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.7871]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.02569561,  1.3082337 , -0.3009862 , -0.5755615 ,  0.02507943,\n",
      "        0.06191675,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.1573,  0.9749, -0.6120, -0.8996,  0.0911,  0.0025,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3641,  0.0669, -0.6631, -1.3729,  0.1187,  0.1394,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0634,  1.2658, -0.6502, -0.7157,  0.0494,  0.1088,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3777,  0.0060, -0.6856, -1.3658,  0.1315,  0.1281,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[1., 2., 0., 1.]])\n",
      "Rewards:  tensor([[-1.2708,  0.8219, -0.9017, -4.7258]])\n",
      "Next states:  tensor([[[-0.1635,  0.9540, -0.6228, -0.9281,  0.0935,  0.0467,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3709,  0.0368, -0.6856, -1.3392,  0.1251,  0.1281,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0698,  1.2491, -0.6502, -0.7424,  0.0548,  0.1088,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3846, -0.0253, -0.6974, -1.3940,  0.1403,  0.1762,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1023, -0.1437,  0.1606, -0.0871]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1023, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1023, -0.1437,  0.1606, -0.0871]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1606]])\n",
      " \n",
      "Q_target =  tensor([[-1.1772]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-1.0749]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.02867432,  1.2946849 , -0.30099437, -0.60223025,  0.02817561,\n",
      "        0.06192908,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.2584,  0.5567, -0.5752, -1.1858,  0.0525, -0.0220,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3444,  0.1577, -0.6437, -1.3594,  0.0978,  0.1487,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0317,  1.3444, -0.6385, -0.6444,  0.0283,  0.1061,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1205,  1.0959, -0.6314, -0.9116,  0.0789,  0.0665,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[2., 2., 0., 2.]])\n",
      "Rewards:  tensor([[ 3.9762,  2.2553, -0.9525,  4.5218]])\n",
      "Next states:  tensor([[[-0.2642,  0.5307, -0.5835, -1.1589,  0.0512, -0.0260,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3510,  0.1280, -0.6631, -1.3195,  0.1047,  0.1394,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0381,  1.3293, -0.6385, -0.6711,  0.0336,  0.1061,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1266,  1.0762, -0.6217, -0.8776,  0.0831,  0.0829,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1039, -0.1439,  0.1604, -0.0872]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.0872, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1039, -0.1438,  0.1604, -0.0872]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1604]])\n",
      " \n",
      "Q_target =  tensor([[-0.6321]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.5450]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.03159132,  1.2805481 , -0.29325375, -0.6283329 ,  0.02971199,\n",
      "        0.03073029,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0111,  1.3685, -0.2856, -0.4759,  0.0108,  0.0483,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0027,  1.4000, -0.2727, -0.4842,  0.0031,  0.0618,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1817,  0.8878, -0.6026, -1.0064,  0.0905, -0.0351,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3444,  0.1577, -0.6437, -1.3594,  0.0978,  0.1487,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 2., 3., 2.]])\n",
      "Rewards:  tensor([[-0.8486,  0.9261,  0.6866,  2.2553]])\n",
      "Next states:  tensor([[[-0.0139,  1.3572, -0.2773, -0.5022,  0.0116,  0.0151,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0054,  1.3893, -0.2764, -0.4770,  0.0060,  0.0570,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1877,  0.8646, -0.5933, -1.0327,  0.0868, -0.0724,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3510,  0.1280, -0.6631, -1.3195,  0.1047,  0.1394,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1042, -0.1444,  0.1600, -0.0887]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1444, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1042, -0.1443,  0.1600, -0.0887]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1600]])\n",
      " \n",
      "Q_target =  tensor([[-1.5977]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-1.4533]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.03458099,  1.2657989 , -0.30237067, -0.655586  ,  0.03308122,\n",
      "        0.06739078,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0254,  1.3589, -0.6289, -0.6175,  0.0230,  0.0678,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0891,  1.1954, -0.6394, -0.8219,  0.0690,  0.0652,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1080,  1.1363, -0.6279, -0.9012,  0.0741,  0.0186,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2642,  0.5307, -0.5835, -1.1589,  0.0512, -0.0260,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[1., 3., 2., 0.]])\n",
      "Rewards:  tensor([[-1.7017,  0.1767,  3.1481, -0.1557]])\n",
      "Next states:  tensor([[[-0.0317,  1.3444, -0.6385, -0.6444,  0.0283,  0.1061,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0954,  1.1763, -0.6297, -0.8484,  0.0703,  0.0261,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1142,  1.1164, -0.6221, -0.8848,  0.0756,  0.0290,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2701,  0.5040, -0.5835, -1.1855,  0.0499, -0.0260,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1043, -0.1465,  0.1599, -0.0882]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.0882, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1043, -0.1465,  0.1600, -0.0882]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1600]])\n",
      " \n",
      "Q_target =  tensor([[-0.4119]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.3237]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.03747463,  1.2504435 , -0.29032376, -0.68248534,  0.03403642,\n",
      "        0.01910569,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0128,  1.3861, -0.6482, -0.5639,  0.0147,  0.1453,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3130,  0.3055, -0.6352, -1.2669,  0.0735,  0.0657,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0891,  1.1954, -0.6394, -0.8219,  0.0690,  0.0652,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2584,  0.5567, -0.5752, -1.1858,  0.0525, -0.0220,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 0., 3., 2.]])\n",
      "Rewards:  tensor([[-0.1359, -1.2122,  0.1767,  3.9762]])\n",
      "Next states:  tensor([[[-0.0191,  1.3728, -0.6365, -0.5909,  0.0196,  0.0983,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3193,  0.2764, -0.6352, -1.2936,  0.0768,  0.0657,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0954,  1.1763, -0.6297, -0.8484,  0.0703,  0.0261,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2642,  0.5307, -0.5835, -1.1589,  0.0512, -0.0260,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1041, -0.1461,  0.1602, -0.0885]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1461, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1040, -0.1461,  0.1603, -0.0885]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1603]])\n",
      " \n",
      "Q_target =  tensor([[-1.3199]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-1.1738]], grad_fn=<SubBackward0>)\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current state:  (array([-0.04043512,  1.2344921 , -0.29872304, -0.7090093 ,  0.03667299,\n",
      "        0.0527365 ,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.3709,  0.0368, -0.6856, -1.3392,  0.1251,  0.1281,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3576,  0.0978, -0.6631, -1.3462,  0.1117,  0.1394,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0197,  1.3335, -0.2988, -0.5220,  0.0173,  0.0532,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0763,  1.2318, -0.6502, -0.7690,  0.0603,  0.1088,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 1., 0.]])\n",
      "Rewards:  tensor([[-3.5214, -3.0472, -2.0896, -0.8517]])\n",
      "Next states:  tensor([[[-0.3777,  0.0060, -0.6856, -1.3658,  0.1315,  0.1281,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3641,  0.0669, -0.6631, -1.3729,  0.1187,  0.1394,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0227,  1.3212, -0.3087, -0.5488,  0.0220,  0.0930,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0827,  1.2139, -0.6502, -0.7957,  0.0657,  0.1088,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1036, -0.1471,  0.1603, -0.0884]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.0884, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1035, -0.1471,  0.1604, -0.0884]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1604]])\n",
      " \n",
      "Q_target =  tensor([[-0.3657]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.2773]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.0433136 ,  1.2179399 , -0.2884304 , -0.73566514,  0.03724478,\n",
      "        0.01143679,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0569,  1.2819, -0.6393, -0.6882,  0.0440,  0.0651,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0891,  1.1954, -0.6394, -0.8219,  0.0690,  0.0652,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0634,  1.2658, -0.6502, -0.7157,  0.0494,  0.1088,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1142,  1.1164, -0.6221, -0.8848,  0.0756,  0.0290,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[1., 3., 0., 1.]])\n",
      "Rewards:  tensor([[-1.7555,  0.1767, -0.9017, -1.1241]])\n",
      "Next states:  tensor([[[-0.0634,  1.2658, -0.6502, -0.7157,  0.0494,  0.1088,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0954,  1.1763, -0.6297, -0.8484,  0.0703,  0.0261,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0698,  1.2491, -0.6502, -0.7424,  0.0548,  0.1088,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1205,  1.0959, -0.6314, -0.9116,  0.0789,  0.0665,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1045, -0.1483,  0.1597, -0.0890]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1045, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1045, -0.1482,  0.1597, -0.0890]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1597]])\n",
      " \n",
      "Q_target =  tensor([[-0.6845]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.5800]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.04619217,  1.2007878 , -0.28843108, -0.7623338 ,  0.03781769,\n",
      "        0.01145945,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0763,  1.2318, -0.6502, -0.7690,  0.0603,  0.1088,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1450,  1.0147, -0.6161, -0.8974,  0.0889,  0.0405,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1757,  0.9105, -0.6026, -0.9797,  0.0922, -0.0351,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3576,  0.0978, -0.6631, -1.3462,  0.1117,  0.1394,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 2., 0., 0.]])\n",
      "Rewards:  tensor([[-8.5175e-01,  2.9928e+00, -1.0662e-03, -3.0472e+00]])\n",
      "Next states:  tensor([[[-0.0827,  1.2139, -0.6502, -0.7957,  0.0657,  0.1088,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1512,  0.9951, -0.6217, -0.8734,  0.0910,  0.0416,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1817,  0.8878, -0.6026, -1.0064,  0.0905, -0.0351,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3641,  0.0669, -0.6631, -1.3729,  0.1187,  0.1394,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1048, -0.1476,  0.1602, -0.0888]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.1602, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1048, -0.1476,  0.1602, -0.0888]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1602]])\n",
      " \n",
      "Q_target =  tensor([[4.5104]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[4.3503]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-4.9240969e-02,  1.1845176e+00, -3.0483493e-01, -7.2311592e-01,\n",
      "        3.7765924e-02, -1.0355962e-03,  0.0000000e+00,  0.0000000e+00],\n",
      "      dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.1450,  1.0147, -0.6161, -0.8974,  0.0889,  0.0405,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0197,  1.3335, -0.2988, -0.5220,  0.0173,  0.0532,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0254,  1.3589, -0.6289, -0.6175,  0.0230,  0.0678,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1573,  0.9749, -0.6120, -0.8996,  0.0911,  0.0025,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[2., 1., 1., 1.]])\n",
      "Rewards:  tensor([[ 2.9928, -2.0896, -1.7017, -1.2708]])\n",
      "Next states:  tensor([[[-0.1512,  0.9951, -0.6217, -0.8734,  0.0910,  0.0416,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0227,  1.3212, -0.3087, -0.5488,  0.0220,  0.0930,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0317,  1.3444, -0.6385, -0.6444,  0.0283,  0.1061,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1635,  0.9540, -0.6228, -0.9281,  0.0935,  0.0467,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1051, -0.1480,  0.1656, -0.0893]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.0893, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1051, -0.1480,  0.1656, -0.0892]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1656]])\n",
      " \n",
      "Q_target =  tensor([[-0.0838]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.0054]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.05221949,  1.1676617 , -0.2959917 , -0.74910665,  0.0359344 ,\n",
      "       -0.03663392,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.3846, -0.0253, -0.6974, -1.3940,  0.1403,  0.1762,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1142,  1.1164, -0.6221, -0.8848,  0.0756,  0.0290,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0569,  1.2819, -0.6393, -0.6882,  0.0440,  0.0651,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1817,  0.8878, -0.6026, -1.0064,  0.0905, -0.0351,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 1., 1., 3.]])\n",
      "Rewards:  tensor([[15.7072, -1.1241, -1.7555,  0.6866]])\n",
      "Next states:  tensor([[[-0.3915, -0.0573, -0.6974, -1.4207,  0.1491,  0.1762,  1.0000,\n",
      "           1.0000],\n",
      "         [-0.1205,  1.0959, -0.6314, -0.9116,  0.0789,  0.0665,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0634,  1.2658, -0.6502, -0.7157,  0.0494,  0.1088,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1877,  0.8646, -0.5933, -1.0327,  0.0868, -0.0724,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1046, -0.1473,  0.1661, -0.0887]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.1661, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1046, -0.1463,  0.1668, -0.0882]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1668]])\n",
      " \n",
      "Q_target =  tensor([[4.8481]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[4.6820]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.0553401 ,  1.1517065 , -0.30971044, -0.7090686 ,  0.03360598,\n",
      "       -0.0465726 ,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.3510,  0.1280, -0.6631, -1.3195,  0.1047,  0.1394,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1635,  0.9540, -0.6228, -0.9281,  0.0935,  0.0467,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2117,  0.7684, -0.5892, -1.0980,  0.0779, -0.0787,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1817,  0.8878, -0.6026, -1.0064,  0.0905, -0.0351,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 3., 0., 3.]])\n",
      "Rewards:  tensor([[-2.7911,  0.2737,  0.3097,  0.6866]])\n",
      "Next states:  tensor([[[-0.3576,  0.0978, -0.6631, -1.3462,  0.1117,  0.1394,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1696,  0.9325, -0.6138, -0.9537,  0.0940,  0.0103,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2176,  0.7431, -0.5892, -1.1247,  0.0740, -0.0787,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1877,  0.8646, -0.5933, -1.0327,  0.0868, -0.0724,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1044, -0.1468,  0.1728, -0.0886]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1468, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1043, -0.1468,  0.1729, -0.0886]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1729]])\n",
      " \n",
      "Q_target =  tensor([[-1.1205]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.9737]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.05854998,  1.1351483 , -0.32089847, -0.7359127 ,  0.03352198,\n",
      "       -0.00168049,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0585,  1.1351, -0.3209, -0.7359,  0.0335, -0.0017,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0553,  1.1517, -0.3097, -0.7091,  0.0336, -0.0466,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3641,  0.0669, -0.6631, -1.3729,  0.1187,  0.1394,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0698,  1.2491, -0.6502, -0.7424,  0.0548,  0.1088,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[2., 1., 2., 0.]])\n",
      "Rewards:  tensor([[ 1.5387, -1.2916,  0.8219, -0.8776]])\n",
      "Next states:  tensor([[[-0.0619,  1.1187, -0.3299, -0.7300,  0.0331, -0.0089,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0585,  1.1351, -0.3209, -0.7359,  0.0335, -0.0017,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3709,  0.0368, -0.6856, -1.3392,  0.1251,  0.1281,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0763,  1.2318, -0.6502, -0.7690,  0.0603,  0.1088,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1047, -0.1485,  0.1725, -0.0891]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.1725, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1046, -0.1485,  0.1725, -0.0891]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1725]])\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q_target =  tensor([[1.7096]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[1.5370]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.06185341,  1.1187223 , -0.32989964, -0.73003983,  0.03307622,\n",
      "       -0.00891577,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0139,  1.3572, -0.2773, -0.5022,  0.0116,  0.0151,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3256,  0.2476, -0.6263, -1.2792,  0.0807,  0.0789,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3641,  0.0669, -0.6631, -1.3729,  0.1187,  0.1394,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2176,  0.7431, -0.5892, -1.1247,  0.0740, -0.0787,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[1., 0., 2., 2.]])\n",
      "Rewards:  tensor([[-2.0843, -1.6039,  0.8219,  2.9584]])\n",
      "Next states:  tensor([[[-0.0167,  1.3453, -0.2891, -0.5292,  0.0147,  0.0623,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3318,  0.2183, -0.6263, -1.3059,  0.0847,  0.0789,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3709,  0.0368, -0.6856, -1.3392,  0.1251,  0.1281,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2235,  0.7178, -0.5808, -1.1214,  0.0706, -0.0673,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1044, -0.1480,  0.1751, -0.0888]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1044, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1043, -0.1480,  0.1751, -0.0888]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1751]])\n",
      " \n",
      "Q_target =  tensor([[-0.5392]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.4348]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.06515684,  1.101696  , -0.32989767, -0.75671387,  0.03263134,\n",
      "       -0.00889828,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-2.6423e-01,  5.3066e-01, -5.8351e-01, -1.1589e+00,  5.1195e-02,\n",
      "          -2.6000e-02,  0.0000e+00,  0.0000e+00],\n",
      "         [-1.6962e-01,  9.3252e-01, -6.1382e-01, -9.5371e-01,  9.3972e-02,\n",
      "           1.0282e-02,  0.0000e+00,  0.0000e+00],\n",
      "         [-3.7092e-01,  3.6755e-02, -6.8563e-01, -1.3392e+00,  1.2507e-01,\n",
      "           1.2807e-01,  0.0000e+00,  0.0000e+00],\n",
      "         [-4.9241e-02,  1.1845e+00, -3.0483e-01, -7.2312e-01,  3.7766e-02,\n",
      "          -1.0356e-03,  0.0000e+00,  0.0000e+00]]])\n",
      "Actions:  tensor([[0., 3., 0., 3.]])\n",
      "Rewards:  tensor([[-0.1557,  0.5981, -3.5214, -0.2478]])\n",
      "Next states:  tensor([[[-0.2701,  0.5040, -0.5835, -1.1855,  0.0499, -0.0260,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1757,  0.9105, -0.6026, -0.9797,  0.0922, -0.0351,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3777,  0.0060, -0.6856, -1.3658,  0.1315,  0.1281,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0522,  1.1677, -0.2960, -0.7491,  0.0359, -0.0366,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1049, -0.1480,  0.1750, -0.0887]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.1750, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1049, -0.1479,  0.1750, -0.0887]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1750]])\n",
      " \n",
      "Q_target =  tensor([[3.4918]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[3.3168]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.06855259,  1.0852333 , -0.3388007 , -0.73165876,  0.0318753 ,\n",
      "       -0.01512214,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.1512,  0.9951, -0.6217, -0.8734,  0.0910,  0.0416,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0287,  1.2947, -0.3010, -0.6022,  0.0282,  0.0619,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3915, -0.0573, -0.6974, -1.4207,  0.1491,  0.1762,  1.0000,\n",
      "           1.0000],\n",
      "         [-0.2176,  0.7431, -0.5892, -1.1247,  0.0740, -0.0787,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 3., 2., 2.]])\n",
      "Rewards:  tensor([[ 0.2667, -0.7909,  8.9878,  2.9584]])\n",
      "Next states:  tensor([[[-0.1573,  0.9749, -0.6120, -0.8996,  0.0911,  0.0025,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0316,  1.2805, -0.2933, -0.6283,  0.0297,  0.0307,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3983, -0.0863, -0.6948, -1.2915,  0.1605,  0.2226,  1.0000,\n",
      "           1.0000],\n",
      "         [-0.2235,  0.7178, -0.5808, -1.1214,  0.0706, -0.0673,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1052, -0.1471,  0.1800, -0.0884]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.0884, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1052, -0.1470,  0.1800, -0.0884]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1800]])\n",
      " \n",
      "Q_target =  tensor([[0.1980]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.2864]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.07185106,  1.0681665 , -0.32663447, -0.75845784,  0.02868595,\n",
      "       -0.06379286,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.2526,  0.5834, -0.5834, -1.2250,  0.0536, -0.0351,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1877,  0.8646, -0.5933, -1.0327,  0.0868, -0.0724,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0139,  1.3572, -0.2773, -0.5022,  0.0116,  0.0151,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0553,  1.1517, -0.3097, -0.7091,  0.0336, -0.0466,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[2., 1., 1., 1.]])\n",
      "Rewards:  tensor([[ 5.8967, -0.4649, -2.0843, -1.2916]])\n",
      "Next states:  tensor([[[-0.2584,  0.5567, -0.5752, -1.1858,  0.0525, -0.0220,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1937,  0.8407, -0.6020, -1.0601,  0.0850, -0.0371,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0167,  1.3453, -0.2891, -0.5292,  0.0147,  0.0623,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0585,  1.1351, -0.3209, -0.7359,  0.0335, -0.0017,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1052, -0.1484,  0.1790, -0.0887]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1484, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1052, -0.1484,  0.1790, -0.0887]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1790]])\n",
      " \n",
      "Q_target =  tensor([[-0.7390]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.5906]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.07521238,  1.0504942 , -0.33449665, -0.7854064 ,  0.02707703,\n",
      "       -0.03218133,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.1817,  0.8878, -0.6026, -1.0064,  0.0905, -0.0351,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1997,  0.8172, -0.6010, -1.0451,  0.0834, -0.0311,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3130,  0.3055, -0.6352, -1.2669,  0.0735,  0.0657,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0404,  1.2345, -0.2987, -0.7090,  0.0367,  0.0527,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 0., 0., 3.]])\n",
      "Rewards:  tensor([[ 0.6866,  0.0279, -1.2122, -0.5245]])\n",
      "Next states:  tensor([[[-0.1877,  0.8646, -0.5933, -1.0327,  0.0868, -0.0724,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2058,  0.7931, -0.6010, -1.0718,  0.0819, -0.0311,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3193,  0.2764, -0.6352, -1.2936,  0.0768,  0.0657,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0433,  1.2179, -0.2884, -0.7357,  0.0372,  0.0114,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1051, -0.1489,  0.1793, -0.0884]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1051, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1050, -0.1488,  0.1794, -0.0884]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1794]])\n",
      " \n",
      "Q_target =  tensor([[-0.4081]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.3031]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.07857323,  1.0322005 , -0.33445257, -0.81302875,  0.02546744,\n",
      "       -0.03219184,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0287,  1.2947, -0.3010, -0.6022,  0.0282,  0.0619,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1512,  0.9951, -0.6217, -0.8734,  0.0910,  0.0416,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0064,  1.3988, -0.6491, -0.5383,  0.0074,  0.1470,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0763,  1.2318, -0.6502, -0.7690,  0.0603,  0.1088,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 3., 0., 0.]])\n",
      "Rewards:  tensor([[-0.7909,  0.2667, -1.0547, -0.8517]])\n",
      "Next states:  tensor([[[-0.0316,  1.2805, -0.2933, -0.6283,  0.0297,  0.0307,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1573,  0.9749, -0.6120, -0.8996,  0.0911,  0.0025,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0128,  1.3861, -0.6482, -0.5639,  0.0147,  0.1453,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0827,  1.2139, -0.6502, -0.7957,  0.0657,  0.1088,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1062, -0.1499,  0.1785, -0.0887]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1499, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1062, -0.1498,  0.1786, -0.0887]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1786]])\n",
      " \n",
      "Q_target =  tensor([[-0.8964]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.7466]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.08201885,  1.0133133 , -0.34508723, -0.83944416,  0.02598446,\n",
      "        0.01034047,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0316,  1.2805, -0.2933, -0.6283,  0.0297,  0.0307,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1635,  0.9540, -0.6228, -0.9281,  0.0935,  0.0467,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1388,  1.0349, -0.6107, -0.9303,  0.0869,  0.0382,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0522,  1.1677, -0.2960, -0.7491,  0.0359, -0.0366,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[1., 3., 2., 2.]])\n",
      "Rewards:  tensor([[-1.7562,  0.2737,  3.8434,  4.6829]])\n",
      "Next states:  tensor([[[-0.0346,  1.2658, -0.3024, -0.6556,  0.0331,  0.0674,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1696,  0.9325, -0.6138, -0.9537,  0.0940,  0.0103,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1450,  1.0147, -0.6161, -0.8974,  0.0889,  0.0405,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0553,  1.1517, -0.3097, -0.7091,  0.0336, -0.0466,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1058, -0.1505,  0.1788, -0.0887]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.0887, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1058, -0.1505,  0.1788, -0.0887]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1788]])\n",
      " \n",
      "Q_target =  tensor([[0.2763]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.3650]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.08536625,  0.9938354 , -0.33277017, -0.86564213,  0.02403101,\n",
      "       -0.03906894,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.3641,  0.0669, -0.6631, -1.3729,  0.1187,  0.1394,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0433,  1.2179, -0.2884, -0.7357,  0.0372,  0.0114,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1388,  1.0349, -0.6107, -0.9303,  0.0869,  0.0382,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2058,  0.7931, -0.6010, -1.0718,  0.0819, -0.0311,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[2., 0., 2., 3.]])\n",
      "Rewards:  tensor([[ 0.8219, -0.8426,  3.8434,  0.8688]])\n",
      "Next states:  tensor([[[-0.3709,  0.0368, -0.6856, -1.3392,  0.1251,  0.1281,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0462,  1.2008, -0.2884, -0.7623,  0.0378,  0.0115,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1450,  1.0147, -0.6161, -0.8974,  0.0889,  0.0405,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2117,  0.7684, -0.5892, -1.0980,  0.0779, -0.0787,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1055, -0.1500,  0.1792, -0.0879]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1055, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1054, -0.1499,  0.1792, -0.0879]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1792]])\n",
      " \n",
      "Q_target =  tensor([[-0.1507]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.0453]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.08871374,  0.9737579 , -0.33277014, -0.89230907,  0.02207756,\n",
      "       -0.03906896,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0854,  0.9938, -0.3328, -0.8656,  0.0240, -0.0391,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0027,  1.4000, -0.2727, -0.4842,  0.0031,  0.0618,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2526,  0.5834, -0.5834, -1.2250,  0.0536, -0.0351,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3709,  0.0368, -0.6856, -1.3392,  0.1251,  0.1281,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 2., 2., 0.]])\n",
      "Rewards:  tensor([[-0.3281,  0.9261,  5.8967, -3.5214]])\n",
      "Next states:  tensor([[[-0.0887,  0.9738, -0.3328, -0.8923,  0.0221, -0.0391,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0054,  1.3893, -0.2764, -0.4770,  0.0060,  0.0570,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2584,  0.5567, -0.5752, -1.1858,  0.0525, -0.0220,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3777,  0.0060, -0.6856, -1.3658,  0.1315,  0.1281,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1057, -0.1501,  0.1792, -0.0882]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.1792, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1057, -0.1501,  0.1792, -0.0882]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1792]])\n",
      " \n",
      "Q_target =  tensor([[5.8406]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[5.6614]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.09204054,  0.95460624, -0.33090422, -0.8511606 ,  0.02032319,\n",
      "       -0.03508744,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.1997,  0.8172, -0.6010, -1.0451,  0.0834, -0.0311,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1696,  0.9325, -0.6138, -0.9537,  0.0940,  0.0103,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0443,  1.3137, -0.6298, -0.6972,  0.0372,  0.0709,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0054,  1.3893, -0.2764, -0.4770,  0.0060,  0.0570,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 3., 0., 2.]])\n",
      "Rewards:  tensor([[ 0.0279,  0.5981, -0.7459,  2.3709]])\n",
      "Next states:  tensor([[[-0.2058,  0.7931, -0.6010, -1.0718,  0.0819, -0.0311,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1757,  0.9105, -0.6026, -0.9797,  0.0922, -0.0351,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0506,  1.2974, -0.6298, -0.7239,  0.0407,  0.0709,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0083,  1.3792, -0.2856, -0.4492,  0.0084,  0.0483,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1059, -0.1503,  0.1865, -0.0880]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.0880, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1059, -0.1502,  0.1866, -0.0880]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1866]])\n",
      " \n",
      "Q_target =  tensor([[0.2989]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.3869]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.09528685,  0.9348471 , -0.32081035, -0.87812775,  0.0165501 ,\n",
      "       -0.07546157,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.1937,  0.8407, -0.6020, -1.0601,  0.0850, -0.0371,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0191,  1.3728, -0.6365, -0.5909,  0.0196,  0.0983,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3709,  0.0368, -0.6856, -1.3392,  0.1251,  0.1281,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2821,  0.4488, -0.6075, -1.2392,  0.0545,  0.0701,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[2., 3., 0., 0.]])\n",
      "Rewards:  tensor([[ 3.3563, -0.2772, -3.5214, -0.7003]])\n",
      "Next states:  tensor([[[-0.1997,  0.8172, -0.6010, -1.0451,  0.0834, -0.0311,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0254,  1.3589, -0.6289, -0.6175,  0.0230,  0.0678,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3777,  0.0060, -0.6856, -1.3658,  0.1315,  0.1281,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2881,  0.4203, -0.6075, -1.2658,  0.0580,  0.0701,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1057, -0.1498,  0.1871, -0.0871]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.0871, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1057, -0.1498,  0.1872, -0.0871]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1872]])\n",
      " \n",
      "Q_target =  tensor([[0.6903]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.7774]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.0984355 ,  0.9144944 , -0.30855477, -0.90450704,  0.0103212 ,\n",
      "       -0.12457806,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0191,  1.3728, -0.6365, -0.5909,  0.0196,  0.0983,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2821,  0.4488, -0.6075, -1.2392,  0.0545,  0.0701,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1573,  0.9749, -0.6120, -0.8996,  0.0911,  0.0025,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0954,  1.1763, -0.6297, -0.8484,  0.0703,  0.0261,  0.0000,\n",
      "           0.0000]]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions:  tensor([[3., 0., 1., 1.]])\n",
      "Rewards:  tensor([[-0.2772, -0.7003, -1.2708, -1.0820]])\n",
      "Next states:  tensor([[[-0.0254,  1.3589, -0.6289, -0.6175,  0.0230,  0.0678,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2881,  0.4203, -0.6075, -1.2658,  0.0580,  0.0701,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1635,  0.9540, -0.6228, -0.9281,  0.0935,  0.0467,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1017,  1.1566, -0.6378, -0.8755,  0.0732,  0.0589,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1060, -0.1502,  0.1868, -0.0862]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.0862, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1059, -0.1502,  0.1868, -0.0862]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1868]])\n",
      " \n",
      "Q_target =  tensor([[0.7156]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.8019]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.10151301,  0.89353293, -0.29963833, -0.93158734,  0.00230839,\n",
      "       -0.16025622,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0054,  1.3893, -0.2764, -0.4770,  0.0060,  0.0570,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1877,  0.8646, -0.5933, -1.0327,  0.0868, -0.0724,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1080,  1.1363, -0.6279, -0.9012,  0.0741,  0.0186,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3915, -0.0573, -0.6974, -1.4207,  0.1491,  0.1762,  1.0000,\n",
      "           1.0000]]])\n",
      "Actions:  tensor([[2., 1., 2., 2.]])\n",
      "Rewards:  tensor([[ 2.3709, -0.4649,  3.1481,  8.9878]])\n",
      "Next states:  tensor([[[-0.0083,  1.3792, -0.2856, -0.4492,  0.0084,  0.0483,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1937,  0.8407, -0.6020, -1.0601,  0.0850, -0.0371,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1142,  1.1164, -0.6221, -0.8848,  0.0756,  0.0290,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3983, -0.0863, -0.6948, -1.2915,  0.1605,  0.2226,  1.0000,\n",
      "           1.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1060, -0.1494,  0.1873, -0.0851]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.0851, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1060, -0.1494,  0.1872, -0.0851]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1872]])\n",
      " \n",
      "Q_target =  tensor([[-0.4823]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.3972]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.10451174,  0.87198347, -0.28975782, -0.95777684, -0.00768351,\n",
      "       -0.19983831,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0317,  1.3444, -0.6385, -0.6444,  0.0283,  0.1061,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1696,  0.9325, -0.6138, -0.9537,  0.0940,  0.0103,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0854,  0.9938, -0.3328, -0.8656,  0.0240, -0.0391,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3709,  0.0368, -0.6856, -1.3392,  0.1251,  0.1281,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 3., 0., 0.]])\n",
      "Rewards:  tensor([[-0.9525,  0.5981, -0.3281, -3.5214]])\n",
      "Next states:  tensor([[[-0.0381,  1.3293, -0.6385, -0.6711,  0.0336,  0.1061,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1757,  0.9105, -0.6026, -0.9797,  0.0922, -0.0351,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0887,  0.9738, -0.3328, -0.8923,  0.0221, -0.0391,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3777,  0.0060, -0.6856, -1.3658,  0.1315,  0.1281,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1058, -0.1501,  0.1869, -0.0858]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.0858, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1058, -0.1501,  0.1869, -0.0858]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1869]])\n",
      " \n",
      "Q_target =  tensor([[-1.1997]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-1.1138]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.10743666,  0.8498275 , -0.28048438, -0.9848139 , -0.01953351,\n",
      "       -0.23700008,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.3983, -0.0863, -0.6948, -1.2915,  0.1605,  0.2226,  1.0000,\n",
      "           1.0000],\n",
      "         [-0.2701,  0.5040, -0.5835, -1.1855,  0.0499, -0.0260,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0953,  0.9348, -0.3208, -0.8781,  0.0166, -0.0755,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0167,  1.3453, -0.2891, -0.5292,  0.0147,  0.0623,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[1., 1., 3., 2.]])\n",
      "Rewards:  tensor([[-100.0000,   -0.9890,    0.5049,    0.7606]])\n",
      "Next states:  tensor([[[-0.4047, -0.0955, -0.5261, -0.0150,  0.0292, -0.3989,  1.0000,\n",
      "           1.0000],\n",
      "         [-0.2760,  0.4767, -0.5954, -1.2125,  0.0510,  0.0216,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0984,  0.9145, -0.3086, -0.9045,  0.0103, -0.1246,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0197,  1.3335, -0.2988, -0.5220,  0.0173,  0.0532,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1053, -0.1484,  0.1879, -0.0869]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1053, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1056, -0.1484,  0.1882, -0.0875]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1882]])\n",
      " \n",
      "Q_target =  tensor([[-1.3483]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-1.2430]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.11036148,  0.8270735 , -0.2804846 , -1.0114924 , -0.0313834 ,\n",
      "       -0.23699784,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.3576,  0.0978, -0.6631, -1.3462,  0.1117,  0.1394,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0254,  1.3589, -0.6289, -0.6175,  0.0230,  0.0678,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2881,  0.4203, -0.6075, -1.2658,  0.0580,  0.0701,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0064,  1.3988, -0.6491, -0.5383,  0.0074,  0.1470,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 1., 2., 0.]])\n",
      "Rewards:  tensor([[-3.0472, -1.7017,  1.9877, -1.0547]])\n",
      "Next states:  tensor([[[-0.3641,  0.0669, -0.6631, -1.3729,  0.1187,  0.1394,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0317,  1.3444, -0.6385, -0.6444,  0.0283,  0.1061,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2943,  0.3922, -0.6194, -1.2526,  0.0611,  0.0619,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0128,  1.3861, -0.6482, -0.5639,  0.0147,  0.1453,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1074, -0.1499,  0.1869, -0.0870]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.1869, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1074, -0.1499,  0.1869, -0.0870]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1869]])\n",
      " \n",
      "Q_target =  tensor([[1.8999]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[1.7129]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.11334153,  0.80459964, -0.2856604 , -0.9991501 , -0.04357357,\n",
      "       -0.24380365,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0698,  1.2491, -0.6502, -0.7424,  0.0548,  0.1088,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0346,  1.2658, -0.3024, -0.6556,  0.0331,  0.0674,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1015,  0.8935, -0.2996, -0.9316,  0.0023, -0.1603,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0381,  1.3293, -0.6385, -0.6711,  0.0336,  0.1061,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 3., 3., 3.]])\n",
      "Rewards:  tensor([[-0.8776, -0.5702, -0.6676, -0.1573]])\n",
      "Next states:  tensor([[[-0.0763,  1.2318, -0.6502, -0.7690,  0.0603,  0.1088,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0375,  1.2504, -0.2903, -0.6825,  0.0340,  0.0191,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1045,  0.8720, -0.2898, -0.9578, -0.0077, -0.1998,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0443,  1.3137, -0.6298, -0.6972,  0.0372,  0.0709,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1077, -0.1505,  0.1888, -0.0877]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1505, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1076, -0.1504,  0.1888, -0.0877]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1888]])\n",
      " \n",
      "Q_target =  tensor([[-1.4136]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-1.2631]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.11640339,  0.78153795, -0.2959462 , -1.0252925 , -0.05369699,\n",
      "       -0.20246811,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0686,  1.0852, -0.3388, -0.7317,  0.0319, -0.0151,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2584,  0.5567, -0.5752, -1.1858,  0.0525, -0.0220,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0920,  0.9546, -0.3309, -0.8512,  0.0203, -0.0351,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0887,  0.9738, -0.3328, -0.8923,  0.0221, -0.0391,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 2., 3., 2.]])\n",
      "Rewards:  tensor([[0.0198, 3.9762, 0.1142, 5.6631]])\n",
      "Next states:  tensor([[[-0.0719,  1.0682, -0.3266, -0.7585,  0.0287, -0.0638,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2642,  0.5307, -0.5835, -1.1589,  0.0512, -0.0260,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0953,  0.9348, -0.3208, -0.8781,  0.0166, -0.0755,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0920,  0.9546, -0.3309, -0.8512,  0.0203, -0.0351,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1072, -0.1515,  0.1892, -0.0876]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1515, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1072, -0.1515,  0.1893, -0.0876]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1893]])\n",
      " \n",
      "Q_target =  tensor([[-1.2215]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-1.0699]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.11954899,  0.7578725 , -0.30643138, -1.0521078 , -0.06172656,\n",
      "       -0.16059121,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0462,  1.2008, -0.2884, -0.7623,  0.0378,  0.0115,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2117,  0.7684, -0.5892, -1.0980,  0.0779, -0.0787,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3983, -0.0863, -0.6948, -1.2915,  0.1605,  0.2226,  1.0000,\n",
      "           1.0000],\n",
      "         [-0.3318,  0.2183, -0.6263, -1.3059,  0.0847,  0.0789,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[2., 0., 1., 1.]])\n",
      "Rewards:  tensor([[   4.3518,    0.3097, -100.0000,   -2.3666]])\n",
      "Next states:  tensor([[[-4.9241e-02,  1.1845e+00, -3.0483e-01, -7.2312e-01,  3.7766e-02,\n",
      "          -1.0356e-03,  0.0000e+00,  0.0000e+00],\n",
      "         [-2.1763e-01,  7.4308e-01, -5.8919e-01, -1.1247e+00,  7.4015e-02,\n",
      "          -7.8677e-02,  0.0000e+00,  0.0000e+00],\n",
      "         [-4.0474e-01, -9.5464e-02, -5.2608e-01, -1.4989e-02,  2.9210e-02,\n",
      "          -3.9895e-01,  1.0000e+00,  1.0000e+00],\n",
      "         [-3.3808e-01,  1.8828e-01, -6.3485e-01, -1.3326e+00,  9.0317e-02,\n",
      "           1.1324e-01,  0.0000e+00,  0.0000e+00]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1072, -0.1514,  0.1903, -0.0867]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1072, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1074, -0.1513,  0.1904, -0.0873]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1904]])\n",
      " \n",
      "Q_target =  tensor([[-0.8331]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.7259]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.12269449,  0.73360807, -0.30643186, -1.0787799 , -0.06975607,\n",
      "       -0.16059048,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0619,  1.1187, -0.3299, -0.7300,  0.0331, -0.0089,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0443,  1.3137, -0.6298, -0.6972,  0.0372,  0.0709,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3915, -0.0573, -0.6974, -1.4207,  0.1491,  0.1762,  1.0000,\n",
      "           1.0000],\n",
      "         [-0.0719,  1.0682, -0.3266, -0.7585,  0.0287, -0.0638,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 2., 1.]])\n",
      "Rewards:  tensor([[-0.7125, -0.7459,  8.9878, -0.9162]])\n",
      "Next states:  tensor([[[-0.0652,  1.1017, -0.3299, -0.7567,  0.0326, -0.0089,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0506,  1.2974, -0.6298, -0.7239,  0.0407,  0.0709,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3983, -0.0863, -0.6948, -1.2915,  0.1605,  0.2226,  1.0000,\n",
      "           1.0000],\n",
      "         [-0.0752,  1.0505, -0.3345, -0.7854,  0.0271, -0.0322,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1085, -0.1520,  0.1898, -0.0869]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.1898, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1085, -0.1519,  0.1898, -0.0869]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1898]])\n",
      " \n",
      "Q_target =  tensor([[2.1400]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[1.9501]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.12565069,  0.70938104, -0.2882449 , -1.0771172 , -0.07704898,\n",
      "       -0.14585844,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0462,  1.2008, -0.2884, -0.7623,  0.0378,  0.0115,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0128,  1.3861, -0.6482, -0.5639,  0.0147,  0.1453,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3915, -0.0573, -0.6974, -1.4207,  0.1491,  0.1762,  1.0000,\n",
      "           1.0000],\n",
      "         [-0.2292,  0.6920, -0.5690, -1.1470,  0.0649, -0.1150,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[2., 3., 2., 1.]])\n",
      "Rewards:  tensor([[ 4.3518, -0.1359,  8.9878, -0.1971]])\n",
      "Next states:  tensor([[[-4.9241e-02,  1.1845e+00, -3.0483e-01, -7.2312e-01,  3.7766e-02,\n",
      "          -1.0356e-03,  0.0000e+00,  0.0000e+00],\n",
      "         [-1.9132e-02,  1.3728e+00, -6.3652e-01, -5.9091e-01,  1.9610e-02,\n",
      "           9.8252e-02,  0.0000e+00,  0.0000e+00],\n",
      "         [-3.9827e-01, -8.6274e-02, -6.9478e-01, -1.2915e+00,  1.6047e-01,\n",
      "           2.2264e-01,  1.0000e+00,  1.0000e+00],\n",
      "         [-2.3505e-01,  6.6562e-01, -5.7905e-01, -1.1739e+00,  6.1157e-02,\n",
      "          -7.4837e-02,  0.0000e+00,  0.0000e+00]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1086, -0.1519,  0.1925, -0.0868]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.0868, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1086, -0.1519,  0.1926, -0.0868]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1926]])\n",
      " \n",
      "Q_target =  tensor([[-0.7487]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.6619]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.12851839,  0.6845471 , -0.27716014, -1.1042571 , -0.08656887,\n",
      "       -0.19039813,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.2350,  0.6656, -0.5790, -1.1739,  0.0612, -0.0748,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0820,  1.0133, -0.3451, -0.8394,  0.0260,  0.0103,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0920,  0.9546, -0.3309, -0.8512,  0.0203, -0.0351,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1512,  0.9951, -0.6217, -0.8734,  0.0910,  0.0416,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 3., 3., 3.]])\n",
      "Rewards:  tensor([[0.3152, 0.0993, 0.1142, 0.2667]])\n",
      "Next states:  tensor([[[-0.2409,  0.6386, -0.5790, -1.2005,  0.0574, -0.0748,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0854,  0.9938, -0.3328, -0.8656,  0.0240, -0.0391,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0953,  0.9348, -0.3208, -0.8781,  0.0166, -0.0755,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1573,  0.9749, -0.6120, -0.8996,  0.0911,  0.0025,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1082, -0.1529,  0.1918, -0.0883]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1529, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1082, -0.1528,  0.1919, -0.0883]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1919]])\n",
      " \n",
      "Q_target =  tensor([[-0.9203]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.7674]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.13144818,  0.6591239 , -0.28496593, -1.1304032 , -0.09451428,\n",
      "       -0.15890786,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.2821,  0.4488, -0.6075, -1.2392,  0.0545,  0.0701,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1285,  0.6845, -0.2772, -1.1043, -0.0866, -0.1904,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3709,  0.0368, -0.6856, -1.3392,  0.1251,  0.1281,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1142,  1.1164, -0.6221, -0.8848,  0.0756,  0.0290,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 1., 0., 1.]])\n",
      "Rewards:  tensor([[-0.7003, -1.1102, -3.5214, -1.1241]])\n",
      "Next states:  tensor([[[-0.2881,  0.4203, -0.6075, -1.2658,  0.0580,  0.0701,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1314,  0.6591, -0.2850, -1.1304, -0.0945, -0.1589,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3777,  0.0060, -0.6856, -1.3658,  0.1315,  0.1281,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1205,  1.0959, -0.6314, -0.9116,  0.0789,  0.0665,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1079, -0.1533,  0.1924, -0.0879]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1079, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1079, -0.1533,  0.1924, -0.0878]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1924]])\n",
      " \n",
      "Q_target =  tensor([[-0.7024]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.5944]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.13437796,  0.63310164, -0.28496665, -1.157075  , -0.10245962,\n",
      "       -0.15890716,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States:  tensor([[[-0.0197,  1.3335, -0.2988, -0.5220,  0.0173,  0.0532,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2176,  0.7431, -0.5892, -1.1247,  0.0740, -0.0787,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2350,  0.6656, -0.5790, -1.1739,  0.0612, -0.0748,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0854,  0.9938, -0.3328, -0.8656,  0.0240, -0.0391,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[1., 2., 0., 0.]])\n",
      "Rewards:  tensor([[-2.0896,  2.9584,  0.3152, -0.3281]])\n",
      "Next states:  tensor([[[-0.0227,  1.3212, -0.3087, -0.5488,  0.0220,  0.0930,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2235,  0.7178, -0.5808, -1.1214,  0.0706, -0.0673,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2409,  0.6386, -0.5790, -1.2005,  0.0574, -0.0748,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0887,  0.9738, -0.3328, -0.8923,  0.0221, -0.0391,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1091, -0.1540,  0.1918, -0.0883]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1540, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1091, -0.1540,  0.1918, -0.0883]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1918]])\n",
      " \n",
      "Q_target =  tensor([[-0.6351]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.4811]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.1373951 ,  0.6064988 , -0.2959537 , -1.1827552 , -0.10817794,\n",
      "       -0.11436631,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0585,  1.1351, -0.3209, -0.7359,  0.0335, -0.0017,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0227,  1.3212, -0.3087, -0.5488,  0.0220,  0.0930,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1877,  0.8646, -0.5933, -1.0327,  0.0868, -0.0724,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3983, -0.0863, -0.6948, -1.2915,  0.1605,  0.2226,  1.0000,\n",
      "           1.0000]]])\n",
      "Actions:  tensor([[2., 3., 1., 1.]])\n",
      "Rewards:  tensor([[   1.5387,   -1.0330,   -0.4649, -100.0000]])\n",
      "Next states:  tensor([[[-0.0619,  1.1187, -0.3299, -0.7300,  0.0331, -0.0089,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0257,  1.3082, -0.3010, -0.5756,  0.0251,  0.0619,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1937,  0.8407, -0.6020, -1.0601,  0.0850, -0.0371,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.4047, -0.0955, -0.5261, -0.0150,  0.0292, -0.3989,  1.0000,\n",
      "           1.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1093, -0.1540,  0.1921, -0.0882]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1540, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1093, -0.1539,  0.1921, -0.0885]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1921]])\n",
      " \n",
      "Q_target =  tensor([[-0.3487]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.1947]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.14048462,  0.5793186 , -0.3050602 , -1.2082934 , -0.11203704,\n",
      "       -0.07718183,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.3641,  0.0669, -0.6631, -1.3729,  0.1187,  0.1394,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3915, -0.0573, -0.6974, -1.4207,  0.1491,  0.1762,  1.0000,\n",
      "           1.0000],\n",
      "         [-0.1997,  0.8172, -0.6010, -1.0451,  0.0834, -0.0311,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3381,  0.1883, -0.6348, -1.3326,  0.0903,  0.1132,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[2., 2., 0., 1.]])\n",
      "Rewards:  tensor([[ 0.8219,  8.9878,  0.0279, -2.7537]])\n",
      "Next states:  tensor([[[-0.3709,  0.0368, -0.6856, -1.3392,  0.1251,  0.1281,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3983, -0.0863, -0.6948, -1.2915,  0.1605,  0.2226,  1.0000,\n",
      "           1.0000],\n",
      "         [-0.2058,  0.7931, -0.6010, -1.0718,  0.0819, -0.0311,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3444,  0.1577, -0.6437, -1.3594,  0.0978,  0.1487,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1085, -0.1527,  0.1933, -0.0870]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.1933, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1085, -0.1527,  0.1933, -0.0870]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1933]])\n",
      " \n",
      "Q_target =  tensor([[2.6342]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[2.4409]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.14354658,  0.55225986, -0.3021898 , -1.2029206 , -0.11601867,\n",
      "       -0.0796326 ,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0404,  1.2345, -0.2987, -0.7090,  0.0367,  0.0527,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2235,  0.7178, -0.5808, -1.1214,  0.0706, -0.0673,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1327,  1.0559, -0.6107, -0.9036,  0.0850,  0.0382,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0381,  1.3293, -0.6385, -0.6711,  0.0336,  0.1061,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 3., 0., 3.]])\n",
      "Rewards:  tensor([[-0.5245,  1.0779, -0.4136, -0.1573]])\n",
      "Next states:  tensor([[[-0.0433,  1.2179, -0.2884, -0.7357,  0.0372,  0.0114,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2292,  0.6920, -0.5690, -1.1470,  0.0649, -0.1150,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1388,  1.0349, -0.6107, -0.9303,  0.0869,  0.0382,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0443,  1.3137, -0.6298, -0.6972,  0.0372,  0.0709,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1094, -0.1551,  0.1948, -0.0882]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.1948, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1093, -0.1550,  0.1949, -0.0882]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1949]])\n",
      " \n",
      "Q_target =  tensor([[3.7011]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[3.5062]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.14668302,  0.52565736, -0.30891794, -1.1827072 , -0.12071997,\n",
      "       -0.09402576,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.1635,  0.9540, -0.6228, -0.9281,  0.0935,  0.0467,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0083,  1.3792, -0.2856, -0.4492,  0.0084,  0.0483,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0827,  1.2139, -0.6502, -0.7957,  0.0657,  0.1088,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1080,  1.1363, -0.6279, -0.9012,  0.0741,  0.0186,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 0., 3., 2.]])\n",
      "Rewards:  tensor([[ 0.2737, -1.4430,  0.0685,  3.1481]])\n",
      "Next states:  tensor([[[-0.1696,  0.9325, -0.6138, -0.9537,  0.0940,  0.0103,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0111,  1.3685, -0.2856, -0.4759,  0.0108,  0.0483,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0891,  1.1954, -0.6394, -0.8219,  0.0690,  0.0652,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1142,  1.1164, -0.6221, -0.8848,  0.0756,  0.0290,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1096, -0.1553,  0.1994, -0.0883]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.1994, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1095, -0.1552,  0.1995, -0.0883]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.1995]])\n",
      " \n",
      "Q_target =  tensor([[4.2246]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[4.0252]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.14959784,  0.4994644 , -0.28740165, -1.164466  , -0.12477864,\n",
      "       -0.08117362,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-4.9241e-02,  1.1845e+00, -3.0483e-01, -7.2312e-01,  3.7766e-02,\n",
      "          -1.0356e-03,  0.0000e+00,  0.0000e+00],\n",
      "         [-9.5287e-02,  9.3485e-01, -3.2081e-01, -8.7813e-01,  1.6550e-02,\n",
      "          -7.5462e-02,  0.0000e+00,  0.0000e+00],\n",
      "         [-1.6962e-01,  9.3252e-01, -6.1382e-01, -9.5371e-01,  9.3972e-02,\n",
      "           1.0282e-02,  0.0000e+00,  0.0000e+00],\n",
      "         [-1.8170e-01,  8.8783e-01, -6.0256e-01, -1.0064e+00,  9.0466e-02,\n",
      "          -3.5068e-02,  0.0000e+00,  0.0000e+00]]])\n",
      "Actions:  tensor([[3., 3., 3., 3.]])\n",
      "Rewards:  tensor([[-0.2478,  0.5049,  0.5981,  0.6866]])\n",
      "Next states:  tensor([[[-0.0522,  1.1677, -0.2960, -0.7491,  0.0359, -0.0366,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0984,  0.9145, -0.3086, -0.9045,  0.0103, -0.1246,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1757,  0.9105, -0.6026, -0.9797,  0.0922, -0.0351,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1877,  0.8646, -0.5933, -1.0327,  0.0868, -0.0724,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1092, -0.1547,  0.2053, -0.0883]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1547, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1091, -0.1546,  0.2054, -0.0883]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.2054]])\n",
      " \n",
      "Q_target =  tensor([[-0.3320]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.1773]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.15260907,  0.47268292, -0.29948896, -1.1904224 , -0.12640212,\n",
      "       -0.03246978,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.2701,  0.5040, -0.5835, -1.1855,  0.0499, -0.0260,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0139,  1.3572, -0.2773, -0.5022,  0.0116,  0.0151,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1467,  0.5257, -0.3089, -1.1827, -0.1207, -0.0940,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1133,  0.8046, -0.2857, -0.9992, -0.0436, -0.2438,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[1., 1., 2., 1.]])\n",
      "Rewards:  tensor([[-0.9890, -2.0843,  4.0271, -1.6005]])\n",
      "Next states:  tensor([[[-0.2760,  0.4767, -0.5954, -1.2125,  0.0510,  0.0216,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0167,  1.3453, -0.2891, -0.5292,  0.0147,  0.0623,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1496,  0.4995, -0.2874, -1.1645, -0.1248, -0.0812,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1164,  0.7815, -0.2959, -1.0253, -0.0537, -0.2025,  0.0000,\n",
      "           0.0000]]])\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Q_current:  tensor([[-0.1089, -0.1546,  0.2056, -0.0884]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.0884, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1088, -0.1546,  0.2056, -0.0884]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.2056]])\n",
      " \n",
      "Q_target =  tensor([[-0.0932]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.0048]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.15553045,  0.44529262, -0.28824306, -1.2176836 , -0.13028805,\n",
      "       -0.07771836,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.1997,  0.8172, -0.6010, -1.0451,  0.0834, -0.0311,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0698,  1.2491, -0.6502, -0.7424,  0.0548,  0.1088,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0443,  1.3137, -0.6298, -0.6972,  0.0372,  0.0709,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3444,  0.1577, -0.6437, -1.3594,  0.0978,  0.1487,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 2.]])\n",
      "Rewards:  tensor([[ 0.0279, -0.8776, -0.7459,  2.2553]])\n",
      "Next states:  tensor([[[-0.2058,  0.7931, -0.6010, -1.0718,  0.0819, -0.0311,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0763,  1.2318, -0.6502, -0.7690,  0.0603,  0.1088,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0506,  1.2974, -0.6298, -0.7239,  0.0407,  0.0709,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3510,  0.1280, -0.6631, -1.3195,  0.1047,  0.1394,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1094, -0.1551,  0.2053, -0.0879]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1094, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1094, -0.1550,  0.2053, -0.0879]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.2053]])\n",
      " \n",
      "Q_target =  tensor([[-0.2518]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.1424]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.15845184,  0.41730252, -0.2882433 , -1.2443515 , -0.13417396,\n",
      "       -0.0777183 ,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.3576,  0.0978, -0.6631, -1.3462,  0.1117,  0.1394,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1195,  0.7579, -0.3064, -1.0521, -0.0617, -0.1606,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1015,  0.8935, -0.2996, -0.9316,  0.0023, -0.1603,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1285,  0.6845, -0.2772, -1.1043, -0.0866, -0.1904,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 3., 1.]])\n",
      "Rewards:  tensor([[-3.0472, -1.0216, -0.6676, -1.1102]])\n",
      "Next states:  tensor([[[-0.3641,  0.0669, -0.6631, -1.3729,  0.1187,  0.1394,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1227,  0.7336, -0.3064, -1.0788, -0.0698, -0.1606,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1045,  0.8720, -0.2898, -0.9578, -0.0077, -0.1998,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1314,  0.6591, -0.2850, -1.1304, -0.0945, -0.1589,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1087, -0.1542,  0.2059, -0.0882]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.0882, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1087, -0.1542,  0.2060, -0.0882]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.2060]])\n",
      " \n",
      "Q_target =  tensor([[-0.4030]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.3147]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.16127682,  0.38868135, -0.27609295, -1.2726445 , -0.14055365,\n",
      "       -0.12759355,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0891,  1.1954, -0.6394, -0.8219,  0.0690,  0.0652,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2468,  0.6110, -0.5874, -1.2274,  0.0553, -0.0413,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0619,  1.1187, -0.3299, -0.7300,  0.0331, -0.0089,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0953,  0.9348, -0.3208, -0.8781,  0.0166, -0.0755,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 2., 0., 3.]])\n",
      "Rewards:  tensor([[ 0.1767,  2.5799, -0.7125,  0.5049]])\n",
      "Next states:  tensor([[[-0.0954,  1.1763, -0.6297, -0.8484,  0.0703,  0.0261,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2526,  0.5834, -0.5834, -1.2250,  0.0536, -0.0351,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0652,  1.1017, -0.3299, -0.7567,  0.0326, -0.0089,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0984,  0.9145, -0.3086, -0.9045,  0.0103, -0.1246,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1093, -0.1549,  0.2054, -0.0886]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1549, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1092, -0.1548,  0.2054, -0.0886]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.2054]])\n",
      " \n",
      "Q_target =  tensor([[-0.4517]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.2968]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.16417703,  0.3594718 , -0.28554237, -1.2986315 , -0.14502363,\n",
      "       -0.0893992 ,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.1015,  0.8935, -0.2996, -0.9316,  0.0023, -0.1603,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0027,  1.4000, -0.2727, -0.4842,  0.0031,  0.0618,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3067,  0.3340, -0.6219, -1.3054,  0.0702,  0.0719,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1877,  0.8646, -0.5933, -1.0327,  0.0868, -0.0724,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 2., 2., 1.]])\n",
      "Rewards:  tensor([[-0.6676,  0.9261,  3.8546, -0.4649]])\n",
      "Next states:  tensor([[[-0.1045,  0.8720, -0.2898, -0.9578, -0.0077, -0.1998,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0054,  1.3893, -0.2764, -0.4770,  0.0060,  0.0570,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3130,  0.3055, -0.6352, -1.2669,  0.0735,  0.0657,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1937,  0.8407, -0.6020, -1.0601,  0.0850, -0.0371,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1093, -0.1552,  0.2055, -0.0887]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.2055, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1093, -0.1551,  0.2055, -0.0887]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.2055]])\n",
      " \n",
      "Q_target =  tensor([[2.5266]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[2.3211]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.16717348,  0.3304622 , -0.29436836, -1.2898391 , -0.15029067,\n",
      "       -0.10534084,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.3709,  0.0368, -0.6856, -1.3392,  0.1251,  0.1281,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1526,  0.4727, -0.2995, -1.1904, -0.1264, -0.0325,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2584,  0.5567, -0.5752, -1.1858,  0.0525, -0.0220,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3444,  0.1577, -0.6437, -1.3594,  0.0978,  0.1487,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 3., 2., 2.]])\n",
      "Rewards:  tensor([[-3.5214, -0.2968,  3.9762,  2.2553]])\n",
      "Next states:  tensor([[[-0.3777,  0.0060, -0.6856, -1.3658,  0.1315,  0.1281,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1555,  0.4453, -0.2882, -1.2177, -0.1303, -0.0777,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2642,  0.5307, -0.5835, -1.1589,  0.0512, -0.0260,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3510,  0.1280, -0.6631, -1.3195,  0.1047,  0.1394,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1087, -0.1542,  0.2094, -0.0881]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.0881, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1086, -0.1541,  0.2095, -0.0881]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.2095]])\n",
      " \n",
      "Q_target =  tensor([[-0.6255]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.5375]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.17007522,  0.30082163, -0.28243715, -1.3181604 , -0.15801388,\n",
      "       -0.15446416,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0317,  1.3444, -0.6385, -0.6444,  0.0283,  0.1061,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0698,  1.2491, -0.6502, -0.7424,  0.0548,  0.1088,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2760,  0.4767, -0.5954, -1.2125,  0.0510,  0.0216,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1496,  0.4995, -0.2874, -1.1645, -0.1248, -0.0812,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 1., 1.]])\n",
      "Rewards:  tensor([[-0.9525, -0.8776, -1.2325, -0.5354]])\n",
      "Next states:  tensor([[[-0.0381,  1.3293, -0.6385, -0.6711,  0.0336,  0.1061,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0763,  1.2318, -0.6502, -0.7690,  0.0603,  0.1088,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2821,  0.4488, -0.6075, -1.2392,  0.0545,  0.0701,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1526,  0.4727, -0.2995, -1.1904, -0.1264, -0.0325,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1095, -0.1554,  0.2086, -0.0892]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1554, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1095, -0.1553,  0.2086, -0.0892]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.2086]])\n",
      " \n",
      "Q_target =  tensor([[-0.6467]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.4914]], grad_fn=<SubBackward0>)\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current state:  (array([-0.17306098,  0.27060565, -0.29302838, -1.3435346 , -0.16356547,\n",
      "       -0.11103139,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.3444,  0.1577, -0.6437, -1.3594,  0.0978,  0.1487,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0752,  1.0505, -0.3345, -0.7854,  0.0271, -0.0322,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1405,  0.5793, -0.3051, -1.2083, -0.1120, -0.0772,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2701,  0.5040, -0.5835, -1.1855,  0.0499, -0.0260,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[2., 0., 2., 1.]])\n",
      "Rewards:  tensor([[ 2.2553, -0.5857,  2.4428, -0.9890]])\n",
      "Next states:  tensor([[[-0.3510,  0.1280, -0.6631, -1.3195,  0.1047,  0.1394,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0786,  1.0322, -0.3345, -0.8130,  0.0255, -0.0322,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1435,  0.5523, -0.3022, -1.2029, -0.1160, -0.0796,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2760,  0.4767, -0.5954, -1.2125,  0.0510,  0.0216,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1088, -0.1552,  0.2090, -0.0892]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.2090, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1088, -0.1552,  0.2091, -0.0892]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.2091]])\n",
      " \n",
      "Q_target =  tensor([[5.8870]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[5.6780]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.17588997,  0.24129869, -0.27730533, -1.3031584 , -0.16915926,\n",
      "       -0.11187623,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.1017,  1.1566, -0.6378, -0.8755,  0.0732,  0.0589,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1701,  0.3008, -0.2824, -1.3182, -0.1580, -0.1545,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0381,  1.3293, -0.6385, -0.6711,  0.0336,  0.1061,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1435,  0.5523, -0.3022, -1.2029, -0.1160, -0.0796,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 1., 3., 2.]])\n",
      "Rewards:  tensor([[ 0.3241, -0.8533, -0.1573,  3.5081]])\n",
      "Next states:  tensor([[[-0.1080,  1.1363, -0.6279, -0.9012,  0.0741,  0.0186,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1731,  0.2706, -0.2930, -1.3435, -0.1636, -0.1110,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0443,  1.3137, -0.6298, -0.6972,  0.0372,  0.0709,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1467,  0.5257, -0.3089, -1.1827, -0.1207, -0.0940,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1092, -0.1556,  0.2165, -0.0893]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.2165, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1092, -0.1556,  0.2166, -0.0893]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.2166]])\n",
      " \n",
      "Q_target =  tensor([[4.7908]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[4.5743]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.17852697,  0.21266606, -0.25833377, -1.2731816 , -0.17454007,\n",
      "       -0.10761626,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0317,  1.3444, -0.6385, -0.6444,  0.0283,  0.1061,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3256,  0.2476, -0.6263, -1.2792,  0.0807,  0.0789,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0433,  1.2179, -0.2884, -0.7357,  0.0372,  0.0114,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1585,  0.4173, -0.2882, -1.2444, -0.1342, -0.0777,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 3.]])\n",
      "Rewards:  tensor([[-0.9525, -1.6039, -0.8426, -0.6069]])\n",
      "Next states:  tensor([[[-0.0381,  1.3293, -0.6385, -0.6711,  0.0336,  0.1061,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3318,  0.2183, -0.6263, -1.3059,  0.0847,  0.0789,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0462,  1.2008, -0.2884, -0.7623,  0.0378,  0.0115,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1613,  0.3887, -0.2761, -1.2726, -0.1406, -0.1276,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1091, -0.1554,  0.2227, -0.0893]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1091, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1091, -0.1554,  0.2228, -0.0892]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.2228]])\n",
      " \n",
      "Q_target =  tensor([[-0.9470]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.8379]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.18116398,  0.18343385, -0.25833437, -1.2998503 , -0.1799209 ,\n",
      "       -0.10761609,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.1266,  1.0762, -0.6217, -0.8776,  0.0831,  0.0829,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3005,  0.3634, -0.6315, -1.2793,  0.0666,  0.1105,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1327,  1.0559, -0.6107, -0.9036,  0.0850,  0.0382,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0167,  1.3453, -0.2891, -0.5292,  0.0147,  0.0623,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 3., 0., 2.]])\n",
      "Rewards:  tensor([[ 0.2102, -0.5133, -0.4136,  0.7606]])\n",
      "Next states:  tensor([[[-0.1327,  1.0559, -0.6107, -0.9036,  0.0850,  0.0382,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3067,  0.3340, -0.6219, -1.3054,  0.0702,  0.0719,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1388,  1.0349, -0.6107, -0.9303,  0.0869,  0.0382,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0197,  1.3335, -0.2988, -0.5220,  0.0173,  0.0532,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1104, -0.1555,  0.2225, -0.0890]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(0.2225, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1104, -0.1554,  0.2226, -0.0890]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.2226]])\n",
      " \n",
      "Q_target =  tensor([[1.4725]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[1.2500]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.18364134,  0.1541902 , -0.24277337, -1.3003285 , -0.18490002,\n",
      "       -0.09958259,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.1642,  0.3595, -0.2855, -1.2986, -0.1450, -0.0894,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1757,  0.9105, -0.6026, -0.9797,  0.0922, -0.0351,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0786,  1.0322, -0.3345, -0.8130,  0.0255, -0.0322,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3576,  0.0978, -0.6631, -1.3462,  0.1117,  0.1394,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[2., 0., 1., 0.]])\n",
      "Rewards:  tensor([[ 2.3231e+00, -1.0662e-03, -1.0732e+00, -3.0472e+00]])\n",
      "Next states:  tensor([[[-0.1672,  0.3305, -0.2944, -1.2898, -0.1503, -0.1053,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1817,  0.8878, -0.6026, -1.0064,  0.0905, -0.0351,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0820,  1.0133, -0.3451, -0.8394,  0.0260,  0.0103,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3641,  0.0669, -0.6631, -1.3729,  0.1187,  0.1394,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1101, -0.1550,  0.2248, -0.0891]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1101, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1101, -0.1550,  0.2248, -0.0891]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.2248]])\n",
      " \n",
      "Q_target =  tensor([[-1.3025]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-1.1924]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.18611869,  0.12434686, -0.24277397, -1.326997  , -0.18987915,\n",
      "       -0.09958241,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.1195,  0.7579, -0.3064, -1.0521, -0.0617, -0.1606,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3381,  0.1883, -0.6348, -1.3326,  0.0903,  0.1132,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0462,  1.2008, -0.2884, -0.7623,  0.0378,  0.0115,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0287,  1.2947, -0.3010, -0.6022,  0.0282,  0.0619,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 1., 2., 3.]])\n",
      "Rewards:  tensor([[-1.0216, -2.7537,  4.3518, -0.7909]])\n",
      "Next states:  tensor([[[-1.2269e-01,  7.3361e-01, -3.0643e-01, -1.0788e+00, -6.9756e-02,\n",
      "          -1.6059e-01,  0.0000e+00,  0.0000e+00],\n",
      "         [-3.4444e-01,  1.5771e-01, -6.4373e-01, -1.3594e+00,  9.7751e-02,\n",
      "           1.4868e-01,  0.0000e+00,  0.0000e+00],\n",
      "         [-4.9241e-02,  1.1845e+00, -3.0483e-01, -7.2312e-01,  3.7766e-02,\n",
      "          -1.0356e-03,  0.0000e+00,  0.0000e+00],\n",
      "         [-3.1591e-02,  1.2805e+00, -2.9325e-01, -6.2833e-01,  2.9712e-02,\n",
      "           3.0730e-02,  0.0000e+00,  0.0000e+00]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1117, -0.1552,  0.2244, -0.0893]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1117, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1117, -0.1551,  0.2245, -0.0893]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.2245]])\n",
      " \n",
      "Q_target =  tensor([[-1.5844]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-1.4727]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.18859597,  0.09390386, -0.24277453, -1.3536657 , -0.19485825,\n",
      "       -0.09958236,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0375,  1.2504, -0.2903, -0.6825,  0.0340,  0.0191,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2821,  0.4488, -0.6075, -1.2392,  0.0545,  0.0701,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0227,  1.3212, -0.3087, -0.5488,  0.0220,  0.0930,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0953,  0.9348, -0.3208, -0.8781,  0.0166, -0.0755,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[1., 0., 3., 3.]])\n",
      "Rewards:  tensor([[-1.4786, -0.7003, -1.0330,  0.5049]])\n",
      "Next states:  tensor([[[-0.0404,  1.2345, -0.2987, -0.7090,  0.0367,  0.0527,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2881,  0.4203, -0.6075, -1.2658,  0.0580,  0.0701,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0257,  1.3082, -0.3010, -0.5756,  0.0251,  0.0619,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0984,  0.9145, -0.3086, -0.9045,  0.0103, -0.1246,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1140, -0.1555,  0.2241, -0.0894]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1140, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1139, -0.1555,  0.2242, -0.0894]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.2242]])\n",
      " \n",
      "Q_target =  tensor([[-1.9485]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-1.8345]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.19107313,  0.06286119, -0.24277511, -1.3803344 , -0.19983736,\n",
      "       -0.09958224,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0553,  1.1517, -0.3097, -0.7091,  0.0336, -0.0466,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1405,  0.5793, -0.3051, -1.2083, -0.1120, -0.0772,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1696,  0.9325, -0.6138, -0.9537,  0.0940,  0.0103,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3709,  0.0368, -0.6856, -1.3392,  0.1251,  0.1281,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[1., 2., 3., 0.]])\n",
      "Rewards:  tensor([[-1.2916,  2.4428,  0.5981, -3.5214]])\n",
      "Next states:  tensor([[[-0.0585,  1.1351, -0.3209, -0.7359,  0.0335, -0.0017,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1435,  0.5523, -0.3022, -1.2029, -0.1160, -0.0796,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1757,  0.9105, -0.6026, -0.9797,  0.0922, -0.0351,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3777,  0.0060, -0.6856, -1.3658,  0.1315,  0.1281,  0.0000,\n",
      "           0.0000]]])\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 2/2000 [00:04<1:09:25,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Q_current:  tensor([[-0.1163, -0.1552,  0.2247, -0.0891]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1163, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1163, -0.1551,  0.2247, -0.0891]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.2247]])\n",
      " \n",
      "Q_target =  tensor([[-2.3932]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-2.2769]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-0.1935503 ,  0.03121878, -0.24277572, -1.407003  , -0.20481645,\n",
      "       -0.09958205,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0375,  1.2504, -0.2903, -0.6825,  0.0340,  0.0191,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1467,  0.5257, -0.3089, -1.1827, -0.1207, -0.0940,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1512,  0.9951, -0.6217, -0.8734,  0.0910,  0.0416,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1080,  1.1363, -0.6279, -0.9012,  0.0741,  0.0186,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[1., 2., 3., 2.]])\n",
      "Rewards:  tensor([[-1.4786,  4.0271,  0.2667,  3.1481]])\n",
      "Next states:  tensor([[[-0.0404,  1.2345, -0.2987, -0.7090,  0.0367,  0.0527,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1496,  0.4995, -0.2874, -1.1645, -0.1248, -0.0812,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1573,  0.9749, -0.6120, -0.8996,  0.0911,  0.0025,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1142,  1.1164, -0.6221, -0.8848,  0.0756,  0.0290,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1196, -0.1555,  0.2243, -0.0892]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.1196, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1196, -0.1555,  0.2243, -0.0892]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.2243]])\n",
      " \n",
      "Q_target =  tensor([[7.0976]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[7.2172]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([-1.9602747e-01, -1.0232353e-03, -2.4277635e-01, -1.4336717e+00,\n",
      "       -2.0979555e-01, -9.9581853e-02,  1.0000000e+00,  0.0000000e+00],\n",
      "      dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.1133,  0.8046, -0.2857, -0.9992, -0.0436, -0.2438,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0652,  1.1017, -0.3299, -0.7567,  0.0326, -0.0089,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2468,  0.6110, -0.5874, -1.2274,  0.0553, -0.0413,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0111,  1.3685, -0.2856, -0.4759,  0.0108,  0.0483,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[1., 2., 2., 3.]])\n",
      "Rewards:  tensor([[-1.6005,  3.3185,  2.5799, -0.8486]])\n",
      "Next states:  tensor([[[-0.1164,  0.7815, -0.2959, -1.0253, -0.0537, -0.2025,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0686,  1.0852, -0.3388, -0.7317,  0.0319, -0.0151,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2526,  0.5834, -0.5834, -1.2250,  0.0536, -0.0351,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0139,  1.3572, -0.2773, -0.5022,  0.0116,  0.0151,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-0.1097, -0.1554,  0.2244, -0.0895]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-0.0895, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-0.1097, -0.1553,  0.2244, -0.0895]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[0.2244]])\n",
      " \n",
      "Q_target =  tensor([[-100.]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-99.9105]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Score:  -86.16405625299512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████▉| 1998/2000 [40:07<00:02,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number:  1998\n",
      "Current state:  (array([ 0.00543795,  1.4153314 ,  0.55079067,  0.19604398, -0.00629445,\n",
      "       -0.12476232,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.1005,  0.3791,  0.1551, -1.5532, -0.0965,  0.0029,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0394,  1.3983, -0.4981, -0.1634,  0.0447,  0.1115,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1134,  1.2110,  0.7650, -0.7795, -0.1286, -0.1710,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0266,  1.0771,  0.1418, -1.0209, -0.0302, -0.0317,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[ 0.7416, -1.0381, -1.0459, -0.4481]])\n",
      "Next states:  tensor([[[ 0.1020,  0.3435,  0.1551, -1.5799, -0.0963,  0.0029,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0443,  1.3940, -0.4981, -0.1901,  0.0503,  0.1115,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1210,  1.1928,  0.7650, -0.8062, -0.1371, -0.1710,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0280,  1.0535,  0.1419, -1.0475, -0.0318, -0.0317,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-82.4009, -99.2224, -92.7728, -96.8852]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-82.4009, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-82.4007, -99.2221, -92.7726, -96.8850]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-82.4007]])\n",
      " \n",
      "Q_target =  tensor([[-81.6978]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.7031]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.01087599,  1.4191651 ,  0.55004585,  0.17034921, -0.01245942,\n",
      "       -0.12330998,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0352,  1.4841, -0.2965,  0.1245,  0.0399,  0.0663,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1641,  1.1844, -0.6918, -0.7267,  0.1858,  0.1544,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2708,  0.6356,  0.5705, -1.3458, -0.3064, -0.1273,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3526, -0.1205, -0.3493, -1.6002,  0.4185,  0.4943,  1.0000,\n",
      "           1.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[   0.3758,   -1.1423,   -0.5011, -100.0000]])\n",
      "Next states:  tensor([[[-0.0381,  1.4863, -0.2965,  0.0978,  0.0432,  0.0663,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1710,  1.1675, -0.6918, -0.7534,  0.1935,  0.1544,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2764,  0.6048,  0.5705, -1.3724, -0.3128, -0.1273,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3613, -0.1276, -0.7108,  0.1101,  0.3432, -0.6737,  1.0000,\n",
      "           1.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-82.3571, -99.2212, -92.7717, -96.8841]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-82.3571, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-82.3552, -99.2187, -92.7694, -96.8816]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-82.3552]])\n",
      " \n",
      "Q_target =  tensor([[-81.7471]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.6099]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.01631441,  1.4223999 ,  0.55006534,  0.14370632, -0.01862081,\n",
      "       -0.12323926,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0161,  1.4068,  0.0856, -0.2497, -0.0182, -0.0191,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2531,  0.5953, -0.6399, -1.4276,  0.2861,  0.1428,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.4847,  0.3446, -0.7537, -1.5845,  0.5477,  0.1682,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1005,  1.2699, -0.7819, -0.6425,  0.1139,  0.1748,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[-2.0107, -0.4188, -1.8631, -1.1548]])\n",
      "Next states:  tensor([[[ 0.0169,  1.4006,  0.0856, -0.2763, -0.0192, -0.0191,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2594,  0.5626, -0.6399, -1.4542,  0.2933,  0.1428,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.4921,  0.3084, -0.7537, -1.6112,  0.5561,  0.1682,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1082,  1.2548, -0.7819, -0.6692,  0.1227,  0.1748,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-82.3219, -99.2239, -92.7742, -96.8867]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-82.3219, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-82.3218, -99.2239, -92.7741, -96.8867]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-82.3218]])\n",
      " \n",
      "Q_target =  tensor([[-81.7722]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.5496]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.02175293,  1.4250352 ,  0.5500835 ,  0.11703257, -0.0247814 ,\n",
      "       -0.12322316,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-5.3490e-01, -9.1532e-02,  5.3058e-06,  6.6425e-07,  3.7528e-01,\n",
      "          -7.1248e-07,  1.0000e+00,  1.0000e+00],\n",
      "         [ 6.5166e-02,  1.2711e+00,  5.0711e-01, -6.3809e-01, -7.3897e-02,\n",
      "          -1.1340e-01,  0.0000e+00,  0.0000e+00],\n",
      "         [ 6.9876e-02,  1.4695e+00,  4.4183e-01, -3.7271e-02, -7.9190e-02,\n",
      "          -9.8750e-02,  0.0000e+00,  0.0000e+00],\n",
      "         [-5.8897e-03,  1.3752e+00, -9.9297e-02, -3.3129e-01,  6.7043e-03,\n",
      "           2.2235e-02,  0.0000e+00,  0.0000e+00]]])\n",
      "Actions:  tensor([[3., 0., 0., 0.]])\n",
      "Rewards:  tensor([[-0.3297, -1.2063, -0.6767, -1.8688]])\n",
      "Next states:  tensor([[[-5.3487e-01, -9.1522e-02,  3.0211e-03,  4.3505e-04,  3.7526e-01,\n",
      "          -3.5538e-04,  1.0000e+00,  1.0000e+00],\n",
      "         [ 7.0180e-02,  1.2562e+00,  5.0712e-01, -6.6476e-01, -7.9566e-02,\n",
      "          -1.1339e-01,  0.0000e+00,  0.0000e+00],\n",
      "         [ 7.4245e-02,  1.4681e+00,  4.4184e-01, -6.3942e-02, -8.4127e-02,\n",
      "          -9.8735e-02,  0.0000e+00,  0.0000e+00],\n",
      "         [-6.8714e-03,  1.3671e+00, -9.9300e-02, -3.5796e-01,  7.8158e-03,\n",
      "           2.2232e-02,  0.0000e+00,  0.0000e+00]]])\n",
      " \n",
      "All Q_current:  tensor([[-82.2651, -99.1944, -92.7476, -96.8582]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-82.2651, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-82.2652, -99.1945, -92.7477, -96.8583]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-82.2652]])\n",
      " \n",
      "Q_target =  tensor([[-81.7792]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.4859]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.02719164,  1.4270711 ,  0.5501016 ,  0.09036138, -0.030941  ,\n",
      "       -0.12320372,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0050,  1.4181, -0.1701,  0.0801,  0.0058,  0.0381,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0841,  0.6470,  0.1934, -1.3455, -0.0951, -0.0432,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3986,  0.5271, -0.8059, -1.4409,  0.4507,  0.1798,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0667,  1.1163, -0.3971, -0.9837,  0.0756,  0.0887,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[ 0.6604,  0.1780, -1.1379, -0.6778]])\n",
      "Next states:  tensor([[[-0.0067,  1.4193, -0.1701,  0.0535,  0.0077,  0.0381,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0860,  0.6162,  0.1934, -1.3722, -0.0973, -0.0432,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.4066,  0.4941, -0.8059, -1.4676,  0.4597,  0.1798,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0707,  1.0936, -0.3971, -1.0103,  0.0801,  0.0887,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-82.2557, -99.2206, -92.7712, -96.8835]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-82.2557, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-82.2558, -99.2206, -92.7713, -96.8835]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-82.2558]])\n",
      " \n",
      "Q_target =  tensor([[-81.8361]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.4196]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.03263054,  1.4285073 ,  0.55011976,  0.06369033, -0.03709962,\n",
      "       -0.12318391,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-6.4169e-02,  4.6736e-01, -1.0468e-01, -1.4901e+00,  7.2526e-02,\n",
      "           2.3368e-02,  0.0000e+00,  0.0000e+00],\n",
      "         [-3.3016e-04,  1.2099e+00, -2.0910e-03, -7.5846e-01,  3.8148e-04,\n",
      "           4.6761e-04,  0.0000e+00,  0.0000e+00],\n",
      "         [ 1.9717e-01,  1.3489e+00,  7.1217e-01, -4.5994e-01, -2.2347e-01,\n",
      "          -1.5893e-01,  0.0000e+00,  0.0000e+00],\n",
      "         [-6.4996e-02,  1.2498e+00, -5.9774e-01, -7.8456e-01,  7.3754e-02,\n",
      "           1.3372e-01,  0.0000e+00,  0.0000e+00]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[ 0.5861, -0.9026, -1.2945, -1.0147]])\n",
      "Next states:  tensor([[[-6.5205e-02,  4.3324e-01, -1.0468e-01, -1.5168e+00,  7.3695e-02,\n",
      "           2.3368e-02,  0.0000e+00,  0.0000e+00],\n",
      "         [-3.5086e-04,  1.1922e+00, -2.0911e-03, -7.8513e-01,  4.0484e-04,\n",
      "           4.6751e-04,  0.0000e+00,  0.0000e+00],\n",
      "         [ 2.0421e-01,  1.3380e+00,  7.1217e-01, -4.8661e-01, -2.3142e-01,\n",
      "          -1.5893e-01,  0.0000e+00,  0.0000e+00],\n",
      "         [-7.0906e-02,  1.2315e+00, -5.9775e-01, -8.1123e-01,  8.0439e-02,\n",
      "           1.3370e-01,  0.0000e+00,  0.0000e+00]]])\n",
      " \n",
      "All Q_current:  tensor([[-82.2321, -99.2234, -92.7738, -96.8862]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-82.2321, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-82.2318, -99.2231, -92.7736, -96.8859]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-82.2318]])\n",
      " \n",
      "Q_target =  tensor([[-81.8811]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.3509]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.03806963,  1.429344  ,  0.5501378 ,  0.03701919, -0.04325727,\n",
      "       -0.12316443,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0060,  1.3996,  0.2039, -0.1943, -0.0069, -0.0457,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1883,  0.9664,  0.3173, -1.1166, -0.2127, -0.0708,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0816,  1.5210, -0.4859,  0.0744,  0.0925,  0.1086,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0943,  1.1326, -0.3178, -0.7999,  0.1067,  0.0709,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[-1.6347, -0.4611, -0.3463, -1.0127]])\n",
      "Next states:  tensor([[[ 0.0081,  1.3946,  0.2039, -0.2210, -0.0092, -0.0457,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1914,  0.9407,  0.3173, -1.1433, -0.2163, -0.0708,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0865,  1.5221, -0.4859,  0.0477,  0.0980,  0.1086,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0974,  1.1140, -0.3178, -0.8266,  0.1102,  0.0709,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-82.2099, -99.2224, -92.7729, -96.8853]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-82.2099, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-82.2100, -99.2226, -92.7731, -96.8854]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-82.2100]])\n",
      " \n",
      "Q_target =  tensor([[-81.9299]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.2800]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.04350901,  1.4295812 ,  0.55015576,  0.01034786, -0.04941393,\n",
      "       -0.12314484,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.1178,  0.2080,  0.2247, -1.7026, -0.1331, -0.0502,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0339,  1.4631,  0.4899,  0.2512, -0.0385, -0.1097,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3411,  0.7627,  0.6271, -1.2453, -0.3858, -0.1399,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2189,  0.8441, -0.5606, -1.1518,  0.2838,  0.1674,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[ 0.2735,  0.0976, -0.7498, -0.8244]])\n",
      "Next states:  tensor([[[ 0.1200,  0.1691,  0.2247, -1.7293, -0.1356, -0.0502,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0387,  1.4681,  0.4899,  0.2245, -0.0440, -0.1097,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3473,  0.7341,  0.6271, -1.2719, -0.3928, -0.1399,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2244,  0.8176, -0.5606, -1.1784,  0.2921,  0.1674,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-82.1873, -99.2155, -92.7667, -96.8786]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-82.1873, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-82.1871, -99.2152, -92.7665, -96.8783]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-82.1871]])\n",
      " \n",
      "Q_target =  tensor([[-81.9784]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.2089]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.04894857,  1.429219  ,  0.5501737 , -0.01632362, -0.05556963,\n",
      "       -0.12312537,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0399,  1.3104, -0.5047, -0.6516,  0.0453,  0.1130,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1204,  1.1806, -0.2647, -0.8231,  0.1361,  0.0591,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1641,  0.7870, -0.3951, -1.2077,  0.1855,  0.0882,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0722,  1.4428, -0.4388, -0.0975,  0.0798,  0.0876,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 3., 0.]])\n",
      "Rewards:  tensor([[-1.1814, -0.9636,  0.2375, -0.8360]])\n",
      "Next states:  tensor([[[-0.0449,  1.2952, -0.5047, -0.6783,  0.0510,  0.1130,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1230,  1.1615, -0.2647, -0.8498,  0.1390,  0.0591,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1679,  0.7593, -0.3861, -1.2328,  0.1880,  0.0505,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0766,  1.4400, -0.4389, -0.1242,  0.0841,  0.0876,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-82.1851, -99.2290, -92.7789, -96.8917]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-82.1851, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-82.1851, -99.2290, -92.7789, -96.8916]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-82.1851]])\n",
      " \n",
      "Q_target =  tensor([[-82.0476]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.1376]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.05438833,  1.428257  ,  0.5501915 , -0.0429952 , -0.06172438,\n",
      "       -0.12310598,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0383,  0.7765, -0.1249, -1.3103,  0.0433,  0.0279,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3725,  0.2056,  0.6493, -1.6851, -0.4212, -0.1449,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1166,  1.2666, -0.5897, -0.5745,  0.1320,  0.1317,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1797,  1.3116,  0.7905, -0.4859, -0.2034, -0.1764,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[ 0.2035, 31.1819, -1.2524, -1.2779]])\n",
      "Next states:  tensor([[[-0.0395,  0.7465, -0.1249, -1.3370,  0.0447,  0.0279,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3774,  0.1711,  0.4759, -1.5322, -0.4219, -0.0364,  1.0000,\n",
      "           0.0000],\n",
      "         [-0.1224,  1.2530, -0.5897, -0.6011,  0.1386,  0.1317,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1875,  1.3001,  0.7905, -0.5126, -0.2123, -0.1764,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-82.1707, -99.2215, -92.7722, -96.8845]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-82.1707, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-82.1702, -99.2209, -92.7716, -96.8838]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-82.1702]])\n",
      " \n",
      "Q_target =  tensor([[-82.1029]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.0678]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.05982838,  1.4266955 ,  0.5502093 , -0.06966688, -0.06787815,\n",
      "       -0.12308668,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.2939,  1.3942, -0.7723, -0.5298,  0.3445,  0.2189,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0189,  1.4274,  0.4789,  0.1429, -0.0216, -0.1073,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0497,  0.2323, -0.0932, -1.6772,  0.0562,  0.0208,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1670,  0.0932,  0.2222, -1.7646, -0.1090, -0.0092,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[-1.5697, -0.1102,  0.9428, -1.2977]])\n",
      "Next states:  tensor([[[-0.3016,  1.3817, -0.7723, -0.5565,  0.3554,  0.2189,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0237,  1.4300,  0.4790,  0.1163, -0.0269, -0.1073,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0507,  0.1939, -0.0932, -1.7039,  0.0573,  0.0208,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1692,  0.0528,  0.2222, -1.7913, -0.1095, -0.0092,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-82.1676, -99.2226, -92.7731, -96.8854]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-82.1676, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-82.1676, -99.2226, -92.7731, -96.8854]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-82.1676]])\n",
      " \n",
      "Q_target =  tensor([[-82.1686]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.0011]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.06526852,  1.4245346 ,  0.5502269 , -0.0963387 , -0.07403094,\n",
      "       -0.12306742,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0243,  1.4433,  0.4100,  0.1734, -0.0277, -0.0918,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0052,  0.2976, -0.0030, -1.6118, -0.0307, -0.0308,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2460,  1.2119,  0.7048, -0.7160, -0.3427, -0.2397,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0143,  1.3804,  0.0413, -0.4928, -0.0162, -0.0092,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[ 0.1715,  0.8652, -1.6271, -1.5356]])\n",
      "Next states:  tensor([[[ 0.0284,  1.4466,  0.4100,  0.1467, -0.0322, -0.0918,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0052,  0.2607, -0.0030, -1.6385, -0.0323, -0.0308,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2529,  1.1953,  0.7048, -0.7427, -0.3547, -0.2397,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0147,  1.3687,  0.0413, -0.5195, -0.0166, -0.0092,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-82.1620, -99.2156, -92.7668, -96.8787]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-82.1620, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-82.1621, -99.2157, -92.7670, -96.8788]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-82.1621]])\n",
      " \n",
      "Q_target =  tensor([[-82.2289]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.0669]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.07070885,  1.4217739 ,  0.55024445, -0.12301067, -0.0801828 ,\n",
      "       -0.12304822,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-4.0144e-01, -6.5181e-02,  3.2920e-01, -3.4605e-01,  3.1586e-01,\n",
      "          -1.3187e+00,  0.0000e+00,  1.0000e+00],\n",
      "         [ 1.6777e-03,  1.4189e+00,  5.6657e-02,  1.3230e-01, -8.2848e-04,\n",
      "          -5.0494e-03,  0.0000e+00,  0.0000e+00],\n",
      "         [ 5.6314e-02,  1.4411e+00,  7.1207e-01,  7.4496e-02, -6.3958e-02,\n",
      "          -1.5938e-01,  0.0000e+00,  0.0000e+00],\n",
      "         [-4.1322e-02,  1.4322e+00, -3.2156e-01, -8.7257e-02,  4.6870e-02,\n",
      "           7.1911e-02,  0.0000e+00,  0.0000e+00]]])\n",
      "Actions:  tensor([[0., 3., 0., 0.]])\n",
      "Rewards:  tensor([[ 5.1877,  1.5938, -0.7083, -0.9097]])\n",
      "Next states:  tensor([[[-0.3988, -0.0738,  0.3302, -0.3708,  0.2463, -1.3471,  0.0000,\n",
      "           1.0000],\n",
      "         [ 0.0023,  1.4212,  0.0642,  0.1055, -0.0026, -0.0353,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0634,  1.4422,  0.7121,  0.0478, -0.0719, -0.1594,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0445,  1.4296, -0.3216, -0.1139,  0.0505,  0.0719,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-82.1492, -99.1936, -92.7469, -96.8574]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-82.1492, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-82.1495, -99.1940, -92.7473, -96.8578]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-82.1495]])\n",
      " \n",
      "Q_target =  tensor([[-82.2786]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.1295]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.07614946,  1.4184138 ,  0.5502619 , -0.14968275, -0.08633369,\n",
      "       -0.12302911,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0101,  1.3599,  0.2044, -0.5065, -0.0115, -0.0458,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2379,  1.1643, -0.6093, -0.7963,  0.2726,  0.1676,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0839,  1.4495, -0.3691, -0.2190,  0.0950,  0.0824,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0061,  1.3464, -0.1028, -0.5443,  0.0069,  0.0230,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[-1.5129, -1.2824, -1.3034, -1.4536]])\n",
      "Next states:  tensor([[[ 0.0121,  1.3479,  0.2044, -0.5332, -0.0138, -0.0458,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2439,  1.1458, -0.6093, -0.8230,  0.2809,  0.1676,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0876,  1.4440, -0.3692, -0.2456,  0.0991,  0.0824,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0071,  1.3336, -0.1028, -0.5710,  0.0081,  0.0230,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-82.1826, -99.2264, -92.7766, -96.8891]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-82.1826, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-82.1826, -99.2265, -92.7767, -96.8892]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-82.1826]])\n",
      " \n",
      "Q_target =  tensor([[-82.3698]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.1873]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.08159037,  1.4144542 ,  0.55027926, -0.17635496, -0.09248362,\n",
      "       -0.12301004,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0317,  1.0398, -0.0746, -0.9537,  0.0875,  0.0476,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0390,  1.3565,  0.3943, -0.3618, -0.0442, -0.0882,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1230,  1.2543,  0.5924, -0.5984, -0.1392, -0.1323,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1785,  1.2665, -0.4512, -0.6814,  0.2018,  0.1007,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[-0.6946, -1.4184, -1.2395, -1.2279]])\n",
      "Next states:  tensor([[[-0.0324,  1.0178, -0.0746, -0.9803,  0.0899,  0.0476,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0429,  1.3478,  0.3943, -0.3884, -0.0486, -0.0882,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1288,  1.2403,  0.5925, -0.6251, -0.1459, -0.1323,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1829,  1.2506, -0.4512, -0.7081,  0.2068,  0.1007,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-82.1927, -99.2248, -92.7751, -96.8876]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-82.1927, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-82.1927, -99.2248, -92.7751, -96.8875]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-82.1927]])\n",
      " \n",
      "Q_target =  tensor([[-82.4336]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.2409]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.08703136,  1.4098951 ,  0.55029655, -0.20302731, -0.09863261,\n",
      "       -0.12299106,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0896,  1.4978, -0.6974,  0.1370,  0.1016,  0.1560,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0045,  1.3668,  0.0117, -0.5576, -0.0051, -0.0026,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2249,  1.4570,  0.3019, -0.5150, -0.2080, -0.0354,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1102,  0.5978, -0.2831, -1.4141,  0.1909,  0.1210,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[-0.6078, -1.3646, -1.3349, -0.0916]])\n",
      "Next states:  tensor([[[-0.0965,  1.5003, -0.6974,  0.1103,  0.1094,  0.1559,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0046,  1.3536,  0.0117, -0.5843, -0.0052, -0.0026,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2279,  1.4448,  0.3019, -0.5417, -0.2098, -0.0354,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1130,  0.5654, -0.2831, -1.4407,  0.1969,  0.1210,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-82.2073, -99.2245, -92.7748, -96.8873]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-82.2073, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-82.2075, -99.2248, -92.7751, -96.8875]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-82.2075]])\n",
      " \n",
      "Q_target =  tensor([[-82.4974]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.2901]], grad_fn=<SubBackward0>)\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current state:  (array([ 0.09247275,  1.4047363 ,  0.5503137 , -0.2296998 , -0.10478067,\n",
      "       -0.12297218,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0198,  1.4336, -0.4997,  0.2125,  0.0225,  0.1119,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0535,  1.2741,  0.4162, -0.6279, -0.0606, -0.0931,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1282,  0.9358,  0.4051, -1.0741, -0.1450, -0.0904,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0240,  1.5039, -0.1515,  0.0584,  0.0272,  0.0339,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[ 3.1336e-04, -1.2501e+00, -5.5547e-01,  5.1468e-01]])\n",
      "Next states:  tensor([[[-0.0247,  1.4378, -0.4997,  0.1859,  0.0281,  0.1119,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0576,  1.2594,  0.4162, -0.6546, -0.0653, -0.0931,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1322,  0.9111,  0.4051, -1.1008, -0.1495, -0.0904,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0255,  1.5047, -0.1515,  0.0318,  0.0289,  0.0339,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-82.2222, -99.2208, -92.7715, -96.8837]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-82.2222, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-82.2225, -99.2212, -92.7718, -96.8840]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-82.2225]])\n",
      " \n",
      "Q_target =  tensor([[-82.5562]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.3340]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.09791412,  1.398978  ,  0.55033076, -0.25637248, -0.11092778,\n",
      "       -0.12295338,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-5.8068e-02,  1.3152e+00, -2.3938e-01, -4.4611e-01,  7.5105e-02,\n",
      "           7.0239e-02,  0.0000e+00,  0.0000e+00],\n",
      "         [ 3.4904e-05,  1.4167e+00,  8.7887e-04,  2.4858e-02, -3.2929e-05,\n",
      "          -1.9662e-04,  0.0000e+00,  0.0000e+00],\n",
      "         [ 2.0069e-02,  1.3225e+00,  2.9000e-01, -6.4119e-01, -2.2800e-02,\n",
      "          -6.4927e-02,  0.0000e+00,  0.0000e+00],\n",
      "         [ 1.8372e-01,  1.4136e+00,  6.6363e-01, -3.5715e-01, -2.0827e-01,\n",
      "          -1.4810e-01,  0.0000e+00,  0.0000e+00]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[-1.6650,  2.2893, -1.2655, -1.2729]])\n",
      "Next states:  tensor([[[-6.0427e-02,  1.3046e+00, -2.3939e-01, -4.7278e-01,  7.8616e-02,\n",
      "           7.0229e-02,  0.0000e+00,  0.0000e+00],\n",
      "         [ 4.3488e-05,  1.4167e+00,  8.7890e-04, -1.8098e-03, -4.2765e-05,\n",
      "          -1.9658e-04,  0.0000e+00,  0.0000e+00],\n",
      "         [ 2.2936e-02,  1.3075e+00,  2.9001e-01, -6.6786e-01, -2.6046e-02,\n",
      "          -6.4916e-02,  0.0000e+00,  0.0000e+00],\n",
      "         [ 1.9029e-01,  1.4050e+00,  6.6363e-01, -3.8382e-01, -2.1567e-01,\n",
      "          -1.4810e-01,  0.0000e+00,  0.0000e+00]]])\n",
      " \n",
      "All Q_current:  tensor([[-82.2454, -99.2243, -92.7747, -96.8871]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-82.2454, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-82.2455, -99.2245, -92.7749, -96.8873]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-82.2455]])\n",
      " \n",
      "Q_target =  tensor([[-82.6178]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.3724]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.10335588,  1.3926201 ,  0.55034775, -0.28304523, -0.11707396,\n",
      "       -0.12293468,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0039,  1.4082,  0.3947, -0.1227, -0.0045, -0.0894,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2193,  0.7873,  0.3636, -1.2552, -0.2478, -0.0812,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3096,  0.5197,  0.7453, -1.4915, -0.3503, -0.1663,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0118,  1.4164,  0.0921, -0.1410, -0.0134, -0.0206,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[-0.8930, -0.2970, -0.7170, -2.0154]])\n",
      "Next states:  tensor([[[ 0.0078,  1.4048,  0.3942, -0.1484, -0.0089, -0.0884,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2229,  0.7584,  0.3636, -1.2818, -0.2519, -0.0812,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3169,  0.4855,  0.7453, -1.5181, -0.3586, -0.1663,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0127,  1.4127,  0.0921, -0.1677, -0.0144, -0.0206,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-82.2637, -99.2186, -92.7696, -96.8816]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-82.2637, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-82.2637, -99.2187, -92.7696, -96.8817]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-82.2637]])\n",
      " \n",
      "Q_target =  tensor([[-82.6693]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.4056]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.10879783,  1.3856627 ,  0.5503646 , -0.30971813, -0.12321921,\n",
      "       -0.12291602,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0223,  1.4236,  0.0726, -0.3826, -0.0252, -0.0162,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1755,  1.0913,  0.3636, -0.8960,  0.7518,  1.1089,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1659,  0.9199, -0.4091, -1.0665,  0.1875,  0.0913,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0943,  0.6285,  0.2445, -1.3990, -0.1066, -0.0546,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 1., 0., 0.]])\n",
      "Rewards:  tensor([[-1.7843, -6.5961, -0.6068,  0.2312]])\n",
      "Next states:  tensor([[[ 0.0230,  1.4144,  0.0726, -0.4093, -0.0260, -0.0162,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1794,  1.0711,  0.3554, -0.9283,  0.8098,  1.1608,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1699,  0.8953, -0.4091, -1.0931,  0.1921,  0.0913,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0967,  0.5964,  0.2445, -1.4257, -0.1094, -0.0546,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-82.2938, -99.2251, -92.7753, -96.8879]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-82.2938, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-82.2938, -99.2252, -92.7753, -96.8879]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-82.2938]])\n",
      " \n",
      "Q_target =  tensor([[-82.7273]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.4335]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.11423998,  1.3781055 ,  0.5503813 , -0.33639118, -0.12936354,\n",
      "       -0.12289743,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0882,  1.4667, -0.3720, -0.2036,  0.0999,  0.0830,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1162,  1.1146, -0.4196, -0.8315,  0.1314,  0.0937,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0339,  1.2637, -0.2450, -0.1821,  0.0864,  0.0665,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1677,  1.5045, -0.7379, -0.1130,  0.1899,  0.1647,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 2., 0.]])\n",
      "Rewards:  tensor([[-1.2650, -0.9819,  1.7573, -1.0484]])\n",
      "Next states:  tensor([[[-0.0919,  1.4615, -0.3720, -0.2302,  0.1041,  0.0830,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1203,  1.0953, -0.4196, -0.8581,  0.1361,  0.0937,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0363,  1.2605, -0.2444, -0.1449,  0.0902,  0.0752,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1750,  1.5014, -0.7379, -0.1397,  0.1981,  0.1647,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-82.3219, -99.2271, -92.7770, -96.8897]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-82.3219, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-82.3220, -99.2272, -92.7771, -96.8898]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-82.3220]])\n",
      " \n",
      "Q_target =  tensor([[-82.7782]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.4563]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.11968231,  1.3699491 ,  0.5503979 , -0.36306438, -0.13550693,\n",
      "       -0.12287897,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0767,  1.2747, -0.4084, -0.5587,  0.0869,  0.0912,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.5998,  0.1310, -0.6929, -1.8017,  0.5648,  0.1181,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.4430,  0.0875,  0.2597, -0.1599,  0.5715, -1.1147,  0.0000,\n",
      "           1.0000],\n",
      "         [-0.0441,  1.4183, -0.6240, -0.0095,  0.0420,  0.0439,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 3.]])\n",
      "Rewards:  tensor([[-1.3385, -3.0183,  4.9035,  0.6958]])\n",
      "Next states:  tensor([[[-0.0807,  1.2616, -0.4084, -0.5854,  0.0914,  0.0912,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.6067,  0.0899, -0.6929, -1.8284,  0.5707,  0.1181,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.4408,  0.0831,  0.2691, -0.1743,  0.5098, -1.2096,  0.0000,\n",
      "           1.0000],\n",
      "         [-0.0503,  1.4175, -0.6158, -0.0358,  0.0426,  0.0110,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-82.3436, -99.2188, -92.7695, -96.8818]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-82.3436, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-82.3435, -99.2187, -92.7694, -96.8816]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-82.3435]])\n",
      " \n",
      "Q_target =  tensor([[-82.8176]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.4740]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.12512493,  1.3611927 ,  0.55041444, -0.3897377 , -0.1416494 ,\n",
      "       -0.12286051,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.2339,  0.1842,  0.3943, -1.6962, -0.2643, -0.0880,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0372,  0.7302, -0.1143, -1.3183,  0.0813,  0.0686,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0473,  0.6585, -0.0737, -1.3682,  0.0535,  0.0164,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0467,  1.1719, -0.2363, -0.7844,  0.0529,  0.0528,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[-1.1421,  0.0159,  0.3792, -1.0076]])\n",
      "Next states:  tensor([[[ 0.2378,  0.1455,  0.3943, -1.7228, -0.2687, -0.0880,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0383,  0.6999, -0.1143, -1.3450,  0.0848,  0.0686,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0481,  0.6271, -0.0737, -1.3948,  0.0543,  0.0164,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0490,  1.1537, -0.2363, -0.8111,  0.0556,  0.0528,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-82.3725, -99.2190, -92.7698, -96.8820]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-82.3725, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-82.3721, -99.2185, -92.7693, -96.8815]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-82.3721]])\n",
      " \n",
      "Q_target =  tensor([[-82.8592]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.4867]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.13056783,  1.3518368 ,  0.5504308 , -0.41641113, -0.14779095,\n",
      "       -0.12284216,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.2186,  1.1017, -0.4512, -0.9214,  0.2471,  0.1007,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0461,  1.3532, -0.5186, -0.3919,  0.0524,  0.1161,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1592,  0.4111,  0.2333, -1.5512, -0.1799, -0.0521,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0620,  0.8530,  0.1947, -1.2002, -0.0552, -0.0065,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[-0.9048, -1.3013,  0.3012,  0.0724]])\n",
      "Next states:  tensor([[[-0.2231,  1.0804, -0.4512, -0.9481,  0.2521,  0.1007,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0513,  1.3438, -0.5186, -0.4186,  0.0582,  0.1160,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1615,  0.3756,  0.2333, -1.5779, -0.1825, -0.0521,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0640,  0.8254,  0.1947, -1.2268, -0.0556, -0.0065,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-82.4093, -99.2280, -92.7778, -96.8906]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-82.4093, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-82.4092, -99.2278, -92.7777, -96.8904]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-82.4092]])\n",
      " \n",
      "Q_target =  tensor([[-82.9047]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.4953]], grad_fn=<SubBackward0>)\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current state:  (array([ 0.13601093,  1.3418813 ,  0.55044705, -0.44308472, -0.15393162,\n",
      "       -0.12282388,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.1242,  0.0463, -0.6107, -1.6689,  2.4228,  0.0480,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2143,  1.4136,  0.7742, -0.3574, -0.2429, -0.1728,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.4034, -0.0850, -0.5417, -0.7560, -0.1974,  3.1888,  1.0000,\n",
      "           1.0000],\n",
      "         [-0.2452,  1.2594, -0.6357, -0.6806,  0.2776,  0.1419,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 0., 0., 0.]])\n",
      "Rewards:  tensor([[-100.0000,   -1.2819, -100.0000,   -1.2401]])\n",
      "Next states:  tensor([[[-0.1305,  0.0173, -0.5798, -0.0271,  2.4430,  1.7449,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2220,  1.4050,  0.7742, -0.3840, -0.2515, -0.1728,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3995, -0.1008, -0.6225, -0.5964, -0.0369,  3.6836,  1.0000,\n",
      "           1.0000],\n",
      "         [-0.2514,  1.2436, -0.6357, -0.7073,  0.2847,  0.1419,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-82.4289, -99.2138, -92.7649, -96.8769]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-82.4289, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-82.4138, -99.1943, -92.7472, -96.8580]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-82.4138]])\n",
      " \n",
      "Q_target =  tensor([[-82.9554]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.5265]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.14144659,  1.3313172 ,  0.55011576, -0.47020414, -0.16047391,\n",
      "       -0.13084881,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.3201,  0.5231, -0.8353, -1.4707,  0.4954,  0.3524,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1848,  0.0075, -0.3279, -1.8416,  0.2089,  0.0732,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0213,  1.3440, -0.2689, -0.4653,  0.0242,  0.0602,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0886,  0.9769, -0.3788, -1.1108,  0.1358,  0.1284,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 0., 0., 0.]])\n",
      "Rewards:  tensor([[-0.9534, 16.3848, -1.5241, -0.6582]])\n",
      "Next states:  tensor([[[-0.3283,  0.4896, -0.8277, -1.4947,  0.5112,  0.3162,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1881, -0.0345, -0.3279, -1.8683,  0.2125,  0.0732,  1.0000,\n",
      "           1.0000],\n",
      "         [-0.0239,  1.3329, -0.2689, -0.4919,  0.0272,  0.0602,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0923,  0.9514, -0.3788, -1.1374,  0.1422,  0.1284,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-82.4698, -99.2250, -92.7751, -96.8877]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-82.4698, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-82.4689, -99.2239, -92.7741, -96.8867]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-82.4689]])\n",
      " \n",
      "Q_target =  tensor([[-83.0093]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.5396]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.14688769,  1.3201417 ,  0.55024475, -0.49736935, -0.1666139 ,\n",
      "       -0.12280027,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.5442,  0.3150, -0.7527, -1.6218,  0.5052,  0.0945,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1235,  1.2000,  0.7808, -0.7862, -0.1399, -0.1745,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0140,  0.0698, -0.0277, -1.8359,  0.0158,  0.0062,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1905,  0.6111, -0.5206, -1.4419,  0.2154,  0.1162,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[-1.7904, -1.0478,  1.2885, -0.1103]])\n",
      "Next states:  tensor([[[-0.5517,  0.2780, -0.7527, -1.6485,  0.5100,  0.0945,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1312,  1.1818,  0.7808, -0.8129, -0.1487, -0.1745,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0143,  0.0279, -0.0277, -1.8625,  0.0161,  0.0062,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1956,  0.5780, -0.5206, -1.4686,  0.2212,  0.1162,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-82.5003, -99.2218, -92.7722, -96.8846]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-82.5003, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-82.4998, -99.2213, -92.7717, -96.8841]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-82.4998]])\n",
      " \n",
      "Q_target =  tensor([[-82.9945]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.4943]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.15232877,  1.3083665 ,  0.550244  , -0.52403915, -0.1727539 ,\n",
      "       -0.1227999 ,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.1093,  1.1396, -0.6170, -0.8910,  0.1106,  0.1193,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0823,  0.7172, -0.1321, -1.3165,  0.0930,  0.0295,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0135,  1.3719,  0.1142, -0.2910, -0.0154, -0.0255,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.4701,  0.4429,  0.6674, -1.5405, -0.3991, -0.1110,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[-0.8072,  0.1847, -1.9109, -1.1448]])\n",
      "Next states:  tensor([[[-0.1154,  1.1190, -0.6171, -0.9177,  0.1166,  0.1192,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0836,  0.6870, -0.1321, -1.3432,  0.0945,  0.0295,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0147,  1.3648,  0.1142, -0.3177, -0.0166, -0.0255,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.4767,  0.4076,  0.6674, -1.5671, -0.4047, -0.1110,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-82.5363, -99.2293, -92.7790, -96.8919]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-82.5363, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-82.5362, -99.2291, -92.7788, -96.8917]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-82.5362]])\n",
      " \n",
      "Q_target =  tensor([[-83.0236]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.4872]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.15776996,  1.2959919 ,  0.55024326, -0.550709  , -0.17889388,\n",
      "       -0.1227996 ,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0927,  0.4681,  0.2308, -1.5452, -0.1476, -0.0960,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2987,  0.9502,  0.6864, -1.0402, -0.3379, -0.1532,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1827,  0.0586, -0.2887, -1.7797,  0.2064,  0.0644,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2437,  0.1553, -0.3734, -1.7130,  0.2753,  0.0833,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[ 0.2967, -0.9273,  7.5894, -1.4571]])\n",
      "Next states:  tensor([[[ 0.0950,  0.4328,  0.2308, -1.5719, -0.1524, -0.0960,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3054,  0.9262,  0.6864, -1.0669, -0.3456, -0.1532,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1855,  0.0180, -0.2887, -1.8064,  0.2096,  0.0644,  0.0000,\n",
      "           1.0000],\n",
      "         [-0.2474,  0.1162, -0.3734, -1.7397,  0.2795,  0.0833,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-82.5588, -99.2198, -92.7704, -96.8827]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-82.5588, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-82.5582, -99.2191, -92.7697, -96.8820]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-82.5582]])\n",
      " \n",
      "Q_target =  tensor([[-83.0352]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.4764]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.16321115,  1.2830178 ,  0.55024236, -0.5773788 , -0.18503384,\n",
      "       -0.12279922,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0611,  1.3218,  0.6871, -0.5468, -0.0694, -0.1538,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0587,  0.8388,  0.1643, -1.1680, -0.1042, -0.0678,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1947,  0.9104,  0.3646, -1.1194, -0.2200, -0.0814,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0313,  0.2732,  0.0705, -1.7109, -0.0354, -0.0157,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[-1.2010, -0.3114, -0.5048,  1.1274]])\n",
      "Next states:  tensor([[[ 0.0679,  1.3089,  0.6871, -0.5734, -0.0771, -0.1537,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0603,  0.8119,  0.1643, -1.1947, -0.1075, -0.0678,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1983,  0.8846,  0.3646, -1.1461, -0.2241, -0.0814,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0320,  0.2341,  0.0705, -1.7376, -0.0362, -0.0157,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-82.5906, -99.2234, -92.7736, -96.8862]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-82.5906, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-82.5906, -99.2233, -92.7735, -96.8860]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-82.5906]])\n",
      " \n",
      "Q_target =  tensor([[-83.0541]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.4634]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.16865245,  1.269444  ,  0.5502416 , -0.60404855, -0.19117379,\n",
      "       -0.12279892,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.4927,  0.4247,  0.7021, -1.5434, -0.5759, -0.1130,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0864,  1.4895,  0.5460,  0.0181, -0.0979, -0.1220,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0470,  1.4633,  0.3965,  0.0474, -0.0533, -0.0887,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0688,  0.7951, -0.2075, -0.9083,  0.7049,  0.6231,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 2.]])\n",
      "Rewards:  tensor([[-1.2983, -0.6023, -0.2766, -2.1011]])\n",
      "Next states:  tensor([[[ 0.4997,  0.3894,  0.7021, -1.5701, -0.5816, -0.1130,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0918,  1.4893,  0.5461, -0.0085, -0.1040, -0.1220,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0510,  1.4638,  0.3965,  0.0207, -0.0578, -0.0887,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0666,  0.7751, -0.2430, -0.9068,  0.7361,  0.6235,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-82.6164, -99.2200, -92.7704, -96.8828]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-82.6164, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-82.6163, -99.2198, -92.7703, -96.8826]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-82.6163]])\n",
      " \n",
      "Q_target =  tensor([[-83.0638]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.4473]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.17409381,  1.255271  ,  0.5502407 , -0.6307183 , -0.1973137 ,\n",
      "       -0.12279824,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.4230, -0.0214, -0.4798, -1.8891,  0.3955,  0.0886,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0739,  1.4229, -0.2134, -0.4388,  0.0835,  0.0476,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0964,  1.0568,  0.3463, -0.1230, -0.1151, -0.0609,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.4829,  0.4154, -0.6875, -1.5584,  0.5457,  0.1534,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 2., 0.]])\n",
      "Rewards:  tensor([[-3.9339, -1.6152, -1.0180, -1.4848]])\n",
      "Next states:  tensor([[[-0.4278, -0.0645, -0.4798, -1.9158,  0.4000,  0.0886,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0760,  1.4125, -0.2134, -0.4655,  0.0859,  0.0476,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1000,  1.0542,  0.3550, -0.1179, -0.1180, -0.0578,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.4897,  0.3798, -0.6875, -1.5851,  0.5534,  0.1534,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-82.6450, -99.2211, -92.7714, -96.8839]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-82.6450, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-82.6446, -99.2206, -92.7710, -96.8834]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-82.6446]])\n",
      " \n",
      "Q_target =  tensor([[-83.0734]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.4284]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.1795353 ,  1.2404983 ,  0.5502398 , -0.6573881 , -0.20345359,\n",
      "       -0.12279783,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.1719,  0.1696, -0.3279, -1.7350,  0.1942,  0.0732,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2757,  1.3228, -0.7967, -0.5670,  0.3122,  0.1778,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0911,  1.1470, -0.5892, -0.9149,  0.1124,  0.1525,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1280,  1.1122, -0.6166, -0.8993,  0.1450,  0.1377,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[-0.6484, -1.3204, -0.9533, -0.9042]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next states:  tensor([[[-0.1751,  0.1300, -0.3279, -1.7616,  0.1979,  0.0732,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2836,  1.3095, -0.7967, -0.5936,  0.3211,  0.1778,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0969,  1.1258, -0.5892, -0.9416,  0.1200,  0.1524,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1341,  1.0914, -0.6167, -0.9260,  0.1518,  0.1377,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-82.6761, -99.2274, -92.7771, -96.8900]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-82.6761, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-82.6758, -99.2270, -92.7767, -96.8896]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-82.6758]])\n",
      " \n",
      "Q_target =  tensor([[-83.0836]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.4076]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.18497686,  1.225126  ,  0.55023885, -0.6840579 , -0.20959345,\n",
      "       -0.12279731,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.1411,  0.8133,  0.4757, -1.2731, -0.1596, -0.1062,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0421,  1.3537,  0.6090, -0.4432, -0.0479, -0.1363,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1332, -0.0127, -0.1504, -1.8309,  0.1551,  0.0466,  0.0000,\n",
      "           1.0000],\n",
      "         [ 0.2157,  1.2257,  0.7273, -0.6626, -0.2444, -0.1623,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[  -0.2384,   -1.2469, -100.0000,   -1.2312]])\n",
      "Next states:  tensor([[[ 1.4581e-01,  7.8411e-01,  4.7571e-01, -1.2997e+00, -1.6494e-01,\n",
      "          -1.0618e-01,  0.0000e+00,  0.0000e+00],\n",
      "         [ 4.8161e-02,  1.3431e+00,  6.0898e-01, -4.6988e-01, -5.4698e-02,\n",
      "          -1.3631e-01,  0.0000e+00,  0.0000e+00],\n",
      "         [-1.3537e-01, -4.2651e-02,  1.4616e-08,  7.5791e-15, -1.4398e-03,\n",
      "           2.1007e-07,  1.0000e+00,  1.0000e+00],\n",
      "         [ 2.2293e-01,  1.2102e+00,  7.2728e-01, -6.8931e-01, -2.5255e-01,\n",
      "          -1.6230e-01,  0.0000e+00,  0.0000e+00]]])\n",
      " \n",
      "All Q_current:  tensor([[-82.6977, -99.2234, -92.7735, -96.8862]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-82.6977, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-82.6944, -99.2191, -92.7696, -96.8819]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-82.6944]])\n",
      " \n",
      "Q_target =  tensor([[-83.0794]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.3817]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.19041844,  1.2091544 ,  0.5502379 , -0.71072763, -0.21573329,\n",
      "       -0.122797  ,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.3227,  1.3873, -0.3617, -0.3663, -0.4222, -0.9668,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2607,  0.4135,  0.4281, -1.5297, -0.3908, -0.1314,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1398,  0.9118, -0.4417, -1.1076,  0.1581,  0.0986,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1177,  1.4548, -0.7443, -0.0784,  0.1334,  0.1664,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 0., 0., 0.]])\n",
      "Rewards:  tensor([[-5.9001, -0.5466, -0.5225, -0.9874]])\n",
      "Next states:  tensor([[[-0.3267,  1.3787, -0.3548, -0.3951, -0.4721, -0.9985,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2649,  0.3785,  0.4281, -1.5564, -0.3973, -0.1314,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1441,  0.8863, -0.4417, -1.1343,  0.1630,  0.0986,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1251,  1.4525, -0.7443, -0.1051,  0.1417,  0.1663,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-82.7241, -99.2272, -92.7769, -96.8898]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-82.7241, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-82.7241, -99.2272, -92.7769, -96.8898]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-82.7241]])\n",
      " \n",
      "Q_target =  tensor([[-83.0844]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.3603]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.19586019,  1.1925832 ,  0.5502369 , -0.7373973 , -0.22187313,\n",
      "       -0.12279663,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0412,  0.7069, -0.0993, -1.2923,  0.0466,  0.0222,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0099,  1.4155,  0.3327,  0.0414, -0.0113, -0.0745,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2139,  0.5343, -0.4808, -1.4535,  0.2418,  0.1073,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1871,  0.7185, -0.3183, -1.2583,  0.3336,  0.1082,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[ 0.1864, -0.1854, -0.1834, -0.4190]])\n",
      "Next states:  tensor([[[-0.0422,  0.6772, -0.0993, -1.3189,  0.0478,  0.0222,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0132,  1.4158,  0.3327,  0.0148, -0.0150, -0.0745,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2187,  0.5010, -0.4808, -1.4801,  0.2472,  0.1073,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1902,  0.6896, -0.3182, -1.2850,  0.3390,  0.1082,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-82.7451, -99.2256, -92.7754, -96.8882]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-82.7451, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-82.7449, -99.2254, -92.7752, -96.8880]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-82.7449]])\n",
      " \n",
      "Q_target =  tensor([[-83.0790]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.3339]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.20130186,  1.1754125 ,  0.55023587, -0.76406705, -0.22801293,\n",
      "       -0.1227963 ,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.1117,  1.4406, -0.7603, -0.0996,  0.1381,  0.2076,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1981,  0.1466,  0.3868, -1.7381, -0.2286, -0.0891,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2804,  0.9265, -0.7271, -1.0603,  0.3174,  0.1623,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0367,  1.4304, -0.3114,  0.0065,  0.0517,  0.0871,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[-1.2112, -1.2523, -0.9011, -0.4583]])\n",
      "Next states:  tensor([[[-0.1192,  1.4377, -0.7604, -0.1262,  0.1485,  0.2075,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2019,  0.1070,  0.3868, -1.7647, -0.2331, -0.0891,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2876,  0.9020, -0.7271, -1.0870,  0.3255,  0.1623,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0398,  1.4300, -0.3114, -0.0201,  0.0560,  0.0871,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-82.7665, -99.2268, -92.7765, -96.8894]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-82.7665, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-82.7665, -99.2269, -92.7765, -96.8895]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-82.7665]])\n",
      " \n",
      "Q_target =  tensor([[-83.0729]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.3064]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.20674363,  1.1576424 ,  0.5502348 , -0.79073685, -0.23415273,\n",
      "       -0.12279598,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0523,  1.3738, -0.6611, -0.2996,  0.0594,  0.1480,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0130,  1.2534, -0.0664, -0.5976,  0.0256,  0.0578,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0731,  1.3484,  0.3323, -0.4062, -0.0436, -0.0312,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2031,  0.2188,  0.3402, -1.6696, -0.2977, -0.1073,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[-1.1764, -1.5366, -1.2924, -0.7304]])\n",
      "Next states:  tensor([[[-0.0588,  1.3665, -0.6611, -0.3263,  0.0668,  0.1480,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0136,  1.2394, -0.0665, -0.6243,  0.0284,  0.0578,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0764,  1.3387,  0.3323, -0.4328, -0.0451, -0.0312,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2065,  0.1807,  0.3402, -1.6963, -0.3031, -0.1073,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-82.7874, -99.2296, -92.7790, -96.8921]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-82.7874, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-82.7875, -99.2297, -92.7791, -96.8922]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-82.7875]])\n",
      " \n",
      "Q_target =  tensor([[-83.0649]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.2775]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.21218547,  1.1392727 ,  0.5502337 , -0.81740654, -0.24029252,\n",
      "       -0.12279566,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0133,  1.3550, -0.0898, -0.3522,  0.0151,  0.0201,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0308,  1.4037, -0.7781, -0.1204,  0.0351,  0.1743,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2592,  0.8915,  0.6534, -1.0946, -0.2688,  0.0872,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0232,  1.3794, -0.4686, -0.3333,  0.0264,  0.1049,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 1., 0.]])\n",
      "Rewards:  tensor([[-1.8387, -1.0143,  0.9479, -1.3107]])\n",
      "Next states:  tensor([[[-0.0142,  1.3465, -0.0898, -0.3788,  0.0161,  0.0201,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0385,  1.4004, -0.7781, -0.1471,  0.0438,  0.1743,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2657,  0.8663,  0.6459, -1.1204, -0.2629,  0.1181,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0278,  1.3713, -0.4686, -0.3599,  0.0316,  0.1049,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-82.8042, -99.2293, -92.7788, -96.8918]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-82.8042, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-82.8043, -99.2295, -92.7789, -96.8920]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-82.8043]])\n",
      " \n",
      "Q_target =  tensor([[-83.0518]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.2477]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.21762744,  1.1203034 ,  0.5502326 , -0.8440763 , -0.24643227,\n",
      "       -0.12279531,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0039,  1.4181,  0.3924,  0.3202, -0.0045, -0.0889,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0276,  1.2867,  0.2147, -0.5848, -0.0313, -0.0480,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3636,  0.4265,  0.7093, -1.5133, -0.3150, -0.1174,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1197,  0.9247,  0.2241, -1.1074, -0.1352, -0.0500,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[ 0.5237, -1.3797, -0.8994, -0.3651]])\n",
      "Next states:  tensor([[[ 0.0077,  1.4247,  0.3918,  0.2945, -0.0089, -0.0878,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0297,  1.2729,  0.2147, -0.6114, -0.0337, -0.0480,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3707,  0.3919,  0.7093, -1.5400, -0.3209, -0.1174,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1219,  0.8992,  0.2241, -1.1341, -0.1377, -0.0500,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-82.8114, -99.2193, -92.7697, -96.8821]], grad_fn=<AddmmBackward0>)\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q_current:  tensor(-82.8114, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-82.8116, -99.2196, -92.7700, -96.8824]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-82.8116]])\n",
      " \n",
      "Q_target =  tensor([[-83.0284]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.2170]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.22306947,  1.1007347 ,  0.55023146, -0.870746  , -0.25257203,\n",
      "       -0.12279488,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.1376,  0.3504,  0.2898, -1.6093, -0.1555, -0.0647,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1074,  1.3103, -0.3504, -0.5452,  0.1215,  0.0782,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3514,  0.5469, -0.5642, -1.4181,  0.4200,  0.1373,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.4412,  0.6324,  0.7112, -1.3617, -0.5013, -0.1600,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[ 0.3344, -1.3973, -0.7774, -1.0518]])\n",
      "Next states:  tensor([[[ 0.1404,  0.3136,  0.2898, -1.6360, -0.1587, -0.0647,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1109,  1.2974, -0.3504, -0.5719,  0.1254,  0.0782,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3570,  0.5145, -0.5642, -1.4448,  0.4269,  0.1373,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.4483,  0.6013,  0.7112, -1.3884, -0.5093, -0.1600,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-82.8289, -99.2246, -92.7745, -96.8873]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-82.8289, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-82.8286, -99.2242, -92.7742, -96.8869]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-82.8286]])\n",
      " \n",
      "Q_target =  tensor([[-83.0138]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.1849]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.22851162,  1.0805664 ,  0.55023026, -0.89741576, -0.25871176,\n",
      "       -0.12279449,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0809,  1.4366, -0.4389, -0.1509,  0.0885,  0.0876,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1203,  0.6829,  0.2080, -1.2924, -0.1392, -0.0729,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0256,  1.4072, -0.5176, -0.0865,  0.0291,  0.1159,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0101,  0.4928,  0.0243, -1.5188, -0.0114, -0.0054,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[-1.0002, -0.1153, -0.8420,  0.7824]])\n",
      "Next states:  tensor([[[-0.0853,  1.4326, -0.4389, -0.1775,  0.0929,  0.0876,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1224,  0.6532,  0.2080, -1.3191, -0.1429, -0.0729,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0307,  1.4046, -0.5176, -0.1132,  0.0349,  0.1159,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0103,  0.4580,  0.0243, -1.5454, -0.0117, -0.0054,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-82.8429, -99.2280, -92.7775, -96.8905]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-82.8429, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-82.8430, -99.2281, -92.7776, -96.8906]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-82.8430]])\n",
      " \n",
      "Q_target =  tensor([[-82.9962]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.1532]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.23395386,  1.0597987 ,  0.5502291 , -0.92408544, -0.26485145,\n",
      "       -0.12279419,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0451,  0.4117,  0.0571, -1.5985, -0.1721, -0.0434,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0779,  0.1267, -0.1050, -1.7481,  0.0880,  0.0234,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3091,  0.9502,  0.4579, -1.0218,  0.1458,  0.8919,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2452,  0.7416, -0.4566, -1.2625,  0.3406,  0.1424,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 1., 0.]])\n",
      "Rewards:  tensor([[ 0.7437,  0.3639, -4.7110, -0.6258]])\n",
      "Next states:  tensor([[[ 0.0457,  0.3751,  0.0571, -1.6251, -0.1742, -0.0434,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0789,  0.0867, -0.1050, -1.7747,  0.0891,  0.0234,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3141,  0.9267,  0.4462, -1.0494,  0.1927,  0.9391,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2497,  0.7126, -0.4566, -1.2891,  0.3477,  0.1424,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-82.8491, -99.2238, -92.7738, -96.8865]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-82.8491, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-82.8487, -99.2233, -92.7733, -96.8860]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-82.8487]])\n",
      " \n",
      "Q_target =  tensor([[-82.9695]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.1204]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.23939618,  1.0384313 ,  0.5502278 , -0.9507552 , -0.27099115,\n",
      "       -0.12279401,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.1625,  1.2370,  0.6322, -0.6316, -0.1843, -0.1492,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0040,  1.2066, -0.0235, -0.7475,  0.0045,  0.0053,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0540,  1.4780,  0.5459,  0.1782, -0.0612, -0.1222,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0691,  1.4193,  0.2408, -0.3614, -0.0781, -0.0538,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[-1.2702, -0.9501, -0.2035, -1.6519]])\n",
      "Next states:  tensor([[[ 0.1688,  1.2222,  0.6323, -0.6588, -0.1914, -0.1411,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0042,  1.1892, -0.0235, -0.7742,  0.0048,  0.0053,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0594,  1.4814,  0.5460,  0.1515, -0.0674, -0.1221,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0714,  1.4106,  0.2408, -0.3880, -0.0808, -0.0538,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-82.8590, -99.2273, -92.7769, -96.8898]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-82.8590, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-82.8591, -99.2274, -92.7770, -96.8900]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-82.8591]])\n",
      " \n",
      "Q_target =  tensor([[-82.9474]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.0884]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.24483852,  1.0164645 ,  0.55022657, -0.9774249 , -0.27713084,\n",
      "       -0.12279371,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.1125,  0.0486,  0.1497, -1.7762, -0.1386, -0.0379,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.4457,  0.2818, -0.7267, -1.6247,  0.5038,  0.1622,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0505,  1.2005,  0.3056, -0.5845, -0.6862, -0.5628,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1083,  0.8618,  0.3776, -1.2159, -0.1225, -0.0843,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 2., 0.]])\n",
      "Rewards:  tensor([[ 7.9825, -1.9950, -3.1198, -0.2471]])\n",
      "Next states:  tensor([[[ 0.1140,  0.0081,  0.1497, -1.8028, -0.1405, -0.0379,  1.0000,\n",
      "           0.0000],\n",
      "         [-0.4529,  0.2447, -0.7267, -1.6513,  0.5119,  0.1622,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0540,  1.1880,  0.3657, -0.5636, -0.7142, -0.5615,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1120,  0.8338,  0.3776, -1.2426, -0.1267, -0.0843,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-82.8593, -99.2206, -92.7709, -96.8834]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-82.8593, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-82.8587, -99.2199, -92.7703, -96.8828]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-82.8587]])\n",
      " \n",
      "Q_target =  tensor([[-82.9146]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.0553]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.25028095,  0.9938982 ,  0.5502253 , -1.0040946 , -0.2832705 ,\n",
      "       -0.12279339,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.2314,  0.8053,  0.6968, -1.2341, -0.2840, -0.1989,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0509,  1.4227, -0.3216, -0.1673,  0.0577,  0.0719,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.4893,  0.1145, -0.1271, -0.2139, -0.8425, -3.2306,  1.0000,\n",
      "           0.0000],\n",
      "         [ 0.0312,  1.4862,  0.2258,  0.0658, -0.0354, -0.0505,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[ -0.8033,  -1.2414, -18.5070,   0.2562]])\n",
      "Next states:  tensor([[[ 0.2382,  0.7769,  0.6968, -1.2608, -0.2939, -0.1989,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0540,  1.4184, -0.3216, -0.1939,  0.0612,  0.0719,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.4870,  0.1113, -0.1293, -0.2428, -1.0043, -3.2204,  1.0000,\n",
      "           0.0000],\n",
      "         [ 0.0335,  1.4871,  0.2258,  0.0391, -0.0379, -0.0505,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-82.8627, -99.2208, -92.7711, -96.8836]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-82.8627, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-82.8626, -99.2208, -92.7711, -96.8836]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-82.8626]])\n",
      " \n",
      "Q_target =  tensor([[-82.8862]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.0235]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.25572357,  0.9707324 ,  0.55022395, -1.0307643 , -0.28941017,\n",
      "       -0.12279306,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.3712,  0.5246, -0.5601, -1.4693,  0.4192,  0.1250,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3390,  0.2179,  0.5933, -1.6558, -0.3912, -0.1373,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0634,  1.4011,  0.2484, -0.2582,  0.2347,  0.6271,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2119,  1.2755, -0.0489, -0.7141, -0.8310, -1.0431,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 1., 2.]])\n",
      "Rewards:  tensor([[-0.7418, -1.8059, -4.2475, -4.5164]])\n",
      "Next states:  tensor([[[-3.7671e-01,  4.9096e-01, -5.6008e-01, -1.4959e+00,  4.2549e-01,\n",
      "           1.2500e-01,  0.0000e+00,  0.0000e+00],\n",
      "         [ 3.4483e-01,  1.8008e-01,  5.9333e-01, -1.6825e+00, -3.9810e-01,\n",
      "          -1.3731e-01,  0.0000e+00,  0.0000e+00],\n",
      "         [ 6.6127e-02,  1.3948e+00,  2.3904e-01, -2.8682e-01,  2.6803e-01,\n",
      "           6.6653e-01,  0.0000e+00,  0.0000e+00],\n",
      "         [-2.1228e-01,  1.2599e+00, -2.5951e-04, -7.2172e-01, -8.8254e-01,\n",
      "          -1.0316e+00,  0.0000e+00,  0.0000e+00]]])\n",
      " \n",
      "All Q_current:  tensor([[-82.8679, -99.2254, -92.7753, -96.8881]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-82.8679, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-82.8675, -99.2250, -92.7749, -96.8877]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-82.8675]])\n",
      " \n",
      "Q_target =  tensor([[-82.8592]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.0087]], grad_fn=<SubBackward0>)\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current state:  (array([ 0.2611662 ,  0.9469671 ,  0.5502226 , -1.057434  , -0.2955498 ,\n",
      "       -0.12279274,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.2337,  0.9952, -0.7162, -0.9296,  0.2628,  0.1521,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1365,  1.5046,  0.6575, -0.0689, -0.1546, -0.1468,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1613,  0.9071,  0.4478, -1.0890, -0.1445, -0.0625,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1332,  0.5326, -0.3368, -1.4967,  0.1506,  0.0752,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[-0.9615, -0.9146, -0.3944,  0.2518]])\n",
      "Next states:  tensor([[[-0.2408,  0.9737, -0.7162, -0.9563,  0.2704,  0.1521,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1430,  1.5024,  0.6576, -0.0956, -0.1619, -0.1468,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1658,  0.8820,  0.4478, -1.1157, -0.1477, -0.0625,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1365,  0.4984, -0.3368, -1.5233,  0.1544,  0.0752,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-82.8708, -99.2298, -92.7791, -96.8922]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-82.8708, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-82.8708, -99.2297, -92.7791, -96.8922]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-82.8708]])\n",
      " \n",
      "Q_target =  tensor([[-82.8311]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.0397]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.266609  ,  0.9226021 ,  0.55022126, -1.0841037 , -0.30168945,\n",
      "       -0.12279242,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0665,  1.4288, -0.6726, -0.0408,  0.0755,  0.1505,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1271,  1.0905,  0.6121, -0.9451, -0.1439, -0.1367,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0020,  1.4077,  0.2042, -0.1420, -0.0023, -0.0462,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0810,  1.1712, -0.5123, -0.8661,  0.0918,  0.1145,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[-0.8501, -0.8353, -1.3795, -0.9114]])\n",
      "Next states:  tensor([[[-0.0731,  1.4273, -0.6726, -0.0674,  0.0830,  0.1505,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1331,  1.0687,  0.6121, -0.9718, -0.1507, -0.1367,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0040,  1.4040,  0.2039, -0.1676, -0.0046, -0.0457,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0861,  1.1511, -0.5123, -0.8928,  0.0975,  0.1145,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-82.8681, -99.2294, -92.7788, -96.8918]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-82.8681, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-82.8682, -99.2296, -92.7790, -96.8920]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-82.8682]])\n",
      " \n",
      "Q_target =  tensor([[-82.7982]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.0699]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.2720519 ,  0.89763767,  0.5502199 , -1.1107733 , -0.30782905,\n",
      "       -0.12279207,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.1461,  0.1330, -0.3016, -1.7998,  0.1652,  0.0673,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1381,  1.2484, -0.6074, -0.6078,  0.1563,  0.1356,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1958,  0.1016, -0.3654, -1.7792,  0.2998,  0.1247,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0148,  1.4358,  0.2498,  0.1176, -0.0168, -0.0559,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[-0.7242, -1.2370, -2.0288,  0.5385]])\n",
      "Next states:  tensor([[[-0.1491,  0.0919, -0.3016, -1.8265,  0.1685,  0.0673,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1441,  1.2341, -0.6074, -0.6345,  0.1631,  0.1355,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1994,  0.0610, -0.3654, -1.8059,  0.3060,  0.1247,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0173,  1.4378,  0.2498,  0.0910, -0.0196, -0.0559,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-82.8603, -99.2248, -92.7746, -96.8875]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-82.8603, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-82.8599, -99.2243, -92.7742, -96.8870]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-82.8599]])\n",
      " \n",
      "Q_target =  tensor([[-82.7607]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.0996]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.2774949 ,  0.8720738 ,  0.55021846, -1.1374431 , -0.31396863,\n",
      "       -0.12279174,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.4742,  0.6603,  0.6485, -1.4134, -0.5554, -0.1519,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1374,  1.3625, -0.4964, -0.4380,  0.1555,  0.1108,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1921,  0.3349, -0.4044, -1.6303,  0.2029,  0.0498,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0384,  1.4431, -0.4850,  0.0853,  0.0436,  0.1086,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[-0.9717, -1.3594,  0.1263, -0.2987]])\n",
      "Next states:  tensor([[[ 0.4806,  0.6280,  0.6485, -1.4401, -0.5630, -0.1519,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1423,  1.3521, -0.4964, -0.4647,  0.1611,  0.1108,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1962,  0.2977, -0.4044, -1.6570,  0.2054,  0.0498,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0431,  1.4444, -0.4850,  0.0586,  0.0490,  0.1085,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-82.8559, -99.2271, -92.7767, -96.8897]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-82.8559, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-82.8557, -99.2269, -92.7765, -96.8895]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-82.8557]])\n",
      " \n",
      "Q_target =  tensor([[-82.7286]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.1273]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.2829379 ,  0.8459104 ,  0.55021703, -1.1641127 , -0.3201082 ,\n",
      "       -0.12279143,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.3161,  1.3411, -0.7264, -0.6455,  0.3576,  0.1621,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0062,  1.4035,  0.6253, -0.3308, -0.0071, -0.1416,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0375,  1.1447,  0.2229, -0.9093, -0.0424, -0.0498,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2290,  0.0840, -0.2824, -1.7998,  0.2586,  0.0630,  0.0000,\n",
      "           1.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[-1.3032, -1.0638, -0.7445, 34.4280]])\n",
      "Next states:  tensor([[[-0.3233,  1.3260, -0.7264, -0.6722,  0.3658,  0.1621,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0123,  1.3955,  0.6244, -0.3564, -0.0141, -0.1400,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0397,  1.1237,  0.2229, -0.9360, -0.0449, -0.0498,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2270,  0.0511,  0.1989, -1.4617,  0.2721,  0.2922,  0.0000,\n",
      "           1.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-82.8516, -99.2315, -92.7807, -96.8939]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-82.8516, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-82.8511, -99.2308, -92.7801, -96.8932]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-82.8511]])\n",
      " \n",
      "Q_target =  tensor([[-82.6979]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.1537]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.2883811 ,  0.81914735,  0.55021554, -1.1907824 , -0.32624775,\n",
      "       -0.12279107,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0092,  1.2722, -0.0203, -0.7345,  0.0104,  0.0045,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0328,  1.3789, -0.4742, -0.2834,  0.0373,  0.1062,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3070,  0.6083,  0.5748, -1.3686, -0.3472, -0.1283,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0087,  1.4334, -0.1511,  0.0628,  0.0236,  0.1636,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 1.]])\n",
      "Rewards:  tensor([[-0.9758, -1.2613, -0.5884, -0.9761]])\n",
      "Next states:  tensor([[[-0.0094,  1.2551, -0.0203, -0.7612,  0.0107,  0.0045,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0375,  1.3719, -0.4742, -0.3101,  0.0426,  0.1061,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3126,  0.5769,  0.5748, -1.3952, -0.3536, -0.1283,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0101,  1.4342, -0.1586,  0.0360,  0.0332,  0.1935,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-82.8415, -99.2307, -92.7800, -96.8932]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-82.8415, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-82.8415, -99.2308, -92.7801, -96.8932]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-82.8415]])\n",
      " \n",
      "Q_target =  tensor([[-82.6644]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.1770]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.29382437,  0.7917848 ,  0.55021405, -1.2174519 , -0.3323873 ,\n",
      "       -0.12279073,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.2206,  1.4914,  0.1813, -0.6331,  0.3369,  0.9835,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1958,  0.1729, -0.3559, -1.7209,  0.1497,  0.0432,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.4557,  1.3547,  0.8389, -0.4247, -0.5233, -0.1685,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.4737,  0.7732,  0.7497, -1.2749, -0.5304, -0.1187,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[1., 0., 3., 0.]])\n",
      "Rewards:  tensor([[-6.3072, -0.7028, -2.4372, -0.8204]])\n",
      "Next states:  tensor([[[ 0.2228,  1.4767,  0.1716, -0.6623,  0.3882,  1.0263,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1993,  0.1335, -0.3559, -1.7475,  0.1519,  0.0432,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.4640,  1.3446,  0.8465, -0.4548, -0.5336, -0.2075,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.4812,  0.7440,  0.7497, -1.3016, -0.5364, -0.1187,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-82.8299, -99.2299, -92.7793, -96.8923]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-82.8299, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-82.8298, -99.2297, -92.7792, -96.8922]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-82.8298]])\n",
      " \n",
      "Q_target =  tensor([[-82.6314]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.1985]], grad_fn=<SubBackward0>)\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current state:  (array([ 0.29926777,  0.7638229 ,  0.5502125 , -1.2441217 , -0.33852684,\n",
      "       -0.12279063,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.1350,  0.6415,  0.3296, -1.3619, -0.2183, -0.1155,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.6166,  0.1928, -0.7193, -1.7634,  0.6522,  0.1293,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0250,  1.3931, -0.0880,  0.2033,  0.0809,  0.0829,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.4674,  0.6490, -0.6847, -1.3993,  0.5282,  0.1528,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 2., 0.]])\n",
      "Rewards:  tensor([[-0.1894, -2.7278, -3.7187, -0.9991]])\n",
      "Next states:  tensor([[[ 0.1383,  0.6103,  0.3296, -1.3886, -0.2241, -0.1155,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.6237,  0.1526, -0.7193, -1.7900,  0.6587,  0.1293,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0257,  1.3984, -0.0779,  0.2328,  0.0858,  0.0993,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.4742,  0.6170, -0.6847, -1.4259,  0.5358,  0.1528,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-82.8133, -99.2240, -92.7738, -96.8866]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-82.8133, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-82.8130, -99.2234, -92.7734, -96.8861]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-82.8130]])\n",
      " \n",
      "Q_target =  tensor([[-82.5963]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.2170]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.30471125,  0.7352613 ,  0.55021095, -1.2707913 , -0.34466636,\n",
      "       -0.1227903 ,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-2.7108e-02,  1.5136e+00, -1.5765e-01,  5.4979e-02,  7.1463e-03,\n",
      "          -6.0291e-04,  0.0000e+00,  0.0000e+00],\n",
      "         [-1.3523e-01,  9.9262e-01, -5.0657e-01, -1.0364e+00,  1.5303e-01,\n",
      "           1.1306e-01,  0.0000e+00,  0.0000e+00],\n",
      "         [ 3.4424e-02,  1.9907e-01,  5.7991e-02, -1.6825e+00, -1.3490e-01,\n",
      "          -4.4565e-02,  0.0000e+00,  0.0000e+00],\n",
      "         [-3.0466e-01,  8.8126e-01, -5.6011e-01, -1.1492e+00,  3.4423e-01,\n",
      "           1.2501e-01,  0.0000e+00,  0.0000e+00]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[ 0.6154, -0.6695,  0.8762, -0.7217]])\n",
      "Next states:  tensor([[[-2.8685e-02,  1.5142e+00, -1.5765e-01,  2.8310e-02,  7.1162e-03,\n",
      "          -6.0282e-04,  0.0000e+00,  0.0000e+00],\n",
      "         [-1.4024e-01,  9.6871e-01, -5.0657e-01, -1.0631e+00,  1.5869e-01,\n",
      "           1.1306e-01,  0.0000e+00,  0.0000e+00],\n",
      "         [ 3.4982e-02,  1.6061e-01,  5.7990e-02, -1.7092e+00, -1.3712e-01,\n",
      "          -4.4565e-02,  0.0000e+00,  0.0000e+00],\n",
      "         [-3.1020e-01,  8.5483e-01, -5.6010e-01, -1.1759e+00,  3.5048e-01,\n",
      "           1.2501e-01,  0.0000e+00,  0.0000e+00]]])\n",
      " \n",
      "All Q_current:  tensor([[-82.8021, -99.2267, -92.7764, -96.8893]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-82.8021, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-82.8022, -99.2268, -92.7764, -96.8893]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-82.8022]])\n",
      " \n",
      "Q_target =  tensor([[-82.5709]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.2312]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.3101549 ,  0.70610017,  0.55020934, -1.2974609 , -0.35080585,\n",
      "       -0.12278996,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0086,  0.8088,  0.0217, -1.1896, -0.0097, -0.0048,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2935,  0.6518, -0.7483, -1.3372,  0.3796,  0.2026,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2671,  1.0439,  0.7369, -0.8930, -0.3033, -0.1676,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3073,  0.7115, -0.4780, -1.3326,  0.3471,  0.1067,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[ 0.0458, -0.8835, -1.0936, -0.4468]])\n",
      "Next states:  tensor([[[ 0.0088,  0.7814,  0.0217, -1.2163, -0.0099, -0.0048,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3009,  0.6211, -0.7483, -1.3639,  0.3897,  0.2026,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2744,  1.0232,  0.7369, -0.9197, -0.3117, -0.1676,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3120,  0.6809, -0.4780, -1.3593,  0.3524,  0.1067,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-82.7889, -99.2280, -92.7776, -96.8906]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-82.7889, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-82.7887, -99.2277, -92.7773, -96.8902]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-82.7887]])\n",
      " \n",
      "Q_target =  tensor([[-82.5472]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.2418]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.31559858,  0.67633957,  0.55020773, -1.3241305 , -0.35694534,\n",
      "       -0.12278968,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 1.2205e-01,  5.2307e-01,  2.2859e-01, -1.4380e+00, -1.3794e-01,\n",
      "          -5.1026e-02,  0.0000e+00,  0.0000e+00],\n",
      "         [-7.2426e-02,  4.6683e-01, -1.3319e-01, -1.4834e+00,  8.1873e-02,\n",
      "           2.9734e-02,  0.0000e+00,  0.0000e+00],\n",
      "         [-1.1628e-03,  1.3688e+00, -2.3526e-02, -4.2747e-01,  1.3301e-03,\n",
      "           5.2694e-03,  0.0000e+00,  0.0000e+00],\n",
      "         [-9.2656e-02,  1.4593e+00, -6.2491e-01, -4.3682e-02,  1.0504e-01,\n",
      "           1.3969e-01,  0.0000e+00,  0.0000e+00]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[ 0.2587,  0.5274, -1.6676, -0.8262]])\n",
      "Next states:  tensor([[[ 1.2431e-01,  4.9012e-01,  2.2859e-01, -1.4646e+00, -1.4049e-01,\n",
      "          -5.1026e-02,  0.0000e+00,  0.0000e+00],\n",
      "         [-7.3743e-02,  4.3285e-01, -1.3319e-01, -1.5100e+00,  8.3360e-02,\n",
      "           2.9734e-02,  0.0000e+00,  0.0000e+00],\n",
      "         [-1.3954e-03,  1.3586e+00, -2.3527e-02, -4.5413e-01,  1.5935e-03,\n",
      "           5.2686e-03,  0.0000e+00,  0.0000e+00],\n",
      "         [-9.8835e-02,  1.4577e+00, -6.2493e-01, -7.0356e-02,  1.1202e-01,\n",
      "           1.3967e-01,  0.0000e+00,  0.0000e+00]]])\n",
      " \n",
      "All Q_current:  tensor([[-82.7718, -99.2251, -92.7750, -96.8878]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-82.7718, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-82.7715, -99.2248, -92.7747, -96.8875]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-82.7715]])\n",
      " \n",
      "Q_target =  tensor([[-82.5250]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.2468]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.32104245,  0.6459795 ,  0.55020607, -1.3508    , -0.3630848 ,\n",
      "       -0.12278936,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.2849,  1.0993,  0.3019, -1.0483, -0.2434, -0.0354,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2174,  0.9215,  0.6013, -1.2663, -3.1374, -1.5193,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.4317,  0.1618,  0.4580, -0.1156,  1.3410,  1.2258,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2476,  0.1971, -0.3630, -1.6879,  0.2980,  0.0865,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 2., 0., 0.]])\n",
      "Rewards:  tensor([[ -0.4801, -13.3203,  -6.3509,  -1.0973]])\n",
      "Next states:  tensor([[[ 0.2879,  1.0752,  0.3019, -1.0750, -0.2452, -0.0354,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2242,  0.8909,  0.6019, -1.3566, -3.2134, -1.5190,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.4270,  0.1595,  0.4582, -0.1424,  1.4023,  1.2254,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2512,  0.1585, -0.3630, -1.7146,  0.3024,  0.0865,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-82.7530, -99.2204, -92.7707, -96.8831]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-82.7530, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-82.7529, -99.2202, -92.7705, -96.8830]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-82.7529]])\n",
      " \n",
      "Q_target =  tensor([[-82.5074]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.2456]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.3264864 ,  0.61502   ,  0.5502044 , -1.3774698 , -0.36922425,\n",
      "       -0.122789  ,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.1466,  0.6891, -0.3661, -1.3167,  0.1986,  0.1112,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1975,  0.9762,  0.7994, -1.0937, -0.2236, -0.1784,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0977,  0.3544,  0.2320, -1.6392, -0.2232, -0.1160,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0788,  0.7608, -0.1504, -1.2390,  0.0891,  0.0336,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[-2.5452e-01, -8.0440e-01,  3.0828e-01,  2.7653e-04]])\n",
      "Next states:  tensor([[[-0.1502,  0.6589, -0.3661, -1.3434,  0.2041,  0.1112,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2054,  0.9510,  0.7990, -1.1208, -0.2329, -0.1864,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0999,  0.3170,  0.2320, -1.6659, -0.2290, -0.1160,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0803,  0.7323, -0.1504, -1.2657,  0.0907,  0.0336,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-82.7424, -99.2262, -92.7760, -96.8888]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-82.7424, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-82.7421, -99.2258, -92.7756, -96.8884]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-82.7421]])\n",
      " \n",
      "Q_target =  tensor([[-82.5049]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.2375]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.33193055,  0.58346075,  0.5502027 , -1.4041393 , -0.37536368,\n",
      "       -0.12278869,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.1054, -0.0334, -0.1819, -1.8733,  0.0229, -0.0058,  1.0000,\n",
      "           1.0000],\n",
      "         [ 0.4277,  0.3923,  0.6863, -1.5470, -0.4834, -0.1532,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1311,  0.7096,  0.2199, -1.4078,  0.9105,  1.2151,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0538,  1.1449,  0.1133, -0.8735, -0.0608, -0.0253,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 1., 0.]])\n",
      "Rewards:  tensor([[-100.0000,   -1.4015,   -6.2940,   -0.7539]])\n",
      "Next states:  tensor([[[-1.0609e-01, -4.2822e-02,  8.0920e-08,  4.8399e-09,  3.2488e-04,\n",
      "          -8.3882e-08,  1.0000e+00,  1.0000e+00],\n",
      "         [ 4.3451e-01,  3.5699e-01,  6.8631e-01, -1.5737e+00, -4.9111e-01,\n",
      "          -1.5316e-01,  0.0000e+00,  0.0000e+00],\n",
      "         [ 1.3366e-01,  6.7796e-01,  2.1555e-01, -1.4393e+00,  9.7323e-01,\n",
      "           1.2542e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 5.4888e-02,  1.1246e+00,  1.1330e-01, -9.0020e-01, -6.2038e-02,\n",
      "          -2.5289e-02,  0.0000e+00,  0.0000e+00]]])\n",
      " \n",
      "All Q_current:  tensor([[-82.7217, -99.2182, -92.7688, -96.8811]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-82.7217, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-82.7059, -99.1979, -92.7505, -96.8615]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-82.7059]])\n",
      " \n",
      "Q_target =  tensor([[-82.4859]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.2358]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.33737478,  0.55130196,  0.55020094, -1.4308089 , -0.3815031 ,\n",
      "       -0.12278837,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.3316,  0.7174,  0.2413, -1.1579,  0.1446,  1.0171,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0408,  1.2726,  0.1651, -0.5658, -0.0462, -0.0369,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2920,  0.4988,  0.5933, -1.4425, -0.3363, -0.1373,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0616,  0.3019,  0.1021, -1.6085, -0.0696, -0.0228,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[1., 0., 0., 0.]])\n",
      "Rewards:  tensor([[-5.6346, -1.5099, -0.6458,  0.7964]])\n",
      "Next states:  tensor([[[ 0.3345,  0.6909,  0.2302, -1.1863,  0.1977,  1.0629,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0424,  1.2593,  0.1651, -0.5935, -0.0480, -0.0368,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2979,  0.4658,  0.5933, -1.4691, -0.3432, -0.1373,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0626,  0.2651,  0.1021, -1.6351, -0.0707, -0.0228,  0.0000,\n",
      "           0.0000]]])\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Q_current:  tensor([[-82.7115, -99.2237, -92.7738, -96.8865]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-82.7115, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-82.7114, -99.2235, -92.7736, -96.8863]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-82.7114]])\n",
      " \n",
      "Q_target =  tensor([[-82.5184]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.1931]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.34281912,  0.51854366,  0.55019915, -1.4574784 , -0.3876425 ,\n",
      "       -0.12278803,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0832,  0.9379, -0.2003, -1.0479,  0.0941,  0.0447,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3607, -0.0846,  0.0382, -0.3704, -2.7328, -0.9553,  1.0000,\n",
      "           0.0000],\n",
      "         [-0.0447,  1.2882, -0.3254, -0.4386,  0.7832,  0.3929,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0175,  1.3985,  0.3534, -0.1638, -0.0199, -0.0792,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[  -0.4545, -100.0000,   -3.1148,   -1.1674]])\n",
      "Next states:  tensor([[[-0.0852,  0.9137, -0.2003, -1.0745,  0.0963,  0.0447,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3630, -0.0887,  0.2642, -0.0755, -2.7782, -0.8626,  1.0000,\n",
      "           0.0000],\n",
      "         [-0.0478,  1.2779, -0.3253, -0.4653,  0.8029,  0.3929,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0210,  1.3942,  0.3535, -0.1904, -0.0238, -0.0791,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-82.6957, -99.2187, -92.7693, -96.8816]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-82.6957, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-82.6946, -99.2173, -92.7679, -96.8802]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-82.6946]])\n",
      " \n",
      "Q_target =  tensor([[-82.5411]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.1546]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.34826356,  0.48518586,  0.5501974 , -1.484148  , -0.3937819 ,\n",
      "       -0.12278776,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0525,  1.4597,  0.5310,  0.0968, -0.0596, -0.1188,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2692,  0.6431,  0.3889, -1.4084, -0.3041, -0.0868,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0203,  1.3759, -0.6840, -0.5459,  0.0232,  0.1532,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3515,  0.9592,  0.6013, -1.0959, -0.3748, -0.1106,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[-0.3597, -0.1976, -1.1802, -0.7442]])\n",
      "Next states:  tensor([[[ 0.0577,  1.4613,  0.5311,  0.0701, -0.0655, -0.1188,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2731,  0.6108,  0.3889, -1.4350, -0.3085, -0.0868,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0270,  1.3630, -0.6840, -0.5725,  0.0308,  0.1532,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3575,  0.9340,  0.6013, -1.1226, -0.3804, -0.1106,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-82.6890, -99.2223, -92.7725, -96.8850]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-82.6890, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-82.6891, -99.2225, -92.7727, -96.8852]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-82.6891]])\n",
      " \n",
      "Q_target =  tensor([[-82.5894]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.0995]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.3537081 ,  0.45122853,  0.5501956 , -1.5108176 , -0.39992127,\n",
      "       -0.12278744,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-1.8730e-02,  3.4817e-01, -3.8822e-02, -1.5418e+00,  9.7699e-03,\n",
      "          -3.5449e-04,  0.0000e+00,  0.0000e+00],\n",
      "         [-4.7120e-03,  1.2180e+00, -3.6669e-02, -8.1940e-01,  5.3505e-03,\n",
      "           8.2003e-03,  0.0000e+00,  0.0000e+00],\n",
      "         [ 1.4807e-01,  9.5670e-01,  3.4827e-01, -1.0303e+00, -1.6739e-01,\n",
      "          -7.7735e-02,  0.0000e+00,  0.0000e+00],\n",
      "         [-1.1642e-02,  1.3558e+00, -9.8145e-02, -3.5074e-01,  1.3213e-02,\n",
      "           2.1954e-02,  0.0000e+00,  0.0000e+00]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[ 0.8569, -0.8018, -0.6244, -1.8364]])\n",
      "Next states:  tensor([[[-1.9119e-02,  3.1288e-01, -3.8822e-02, -1.5685e+00,  9.7522e-03,\n",
      "          -3.5451e-04,  0.0000e+00,  0.0000e+00],\n",
      "         [-5.0745e-03,  1.1990e+00, -3.6670e-02, -8.4606e-01,  5.7604e-03,\n",
      "           8.1998e-03,  0.0000e+00,  0.0000e+00],\n",
      "         [ 1.5152e-01,  9.3293e-01,  3.4827e-01, -1.0569e+00, -1.7128e-01,\n",
      "          -7.7735e-02,  0.0000e+00,  0.0000e+00],\n",
      "         [-1.2612e-02,  1.3473e+00, -9.8148e-02, -3.7741e-01,  1.4311e-02,\n",
      "           2.1952e-02,  0.0000e+00,  0.0000e+00]]])\n",
      " \n",
      "All Q_current:  tensor([[-82.6853, -99.2252, -92.7752, -96.8879]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-82.6853, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-82.6850, -99.2249, -92.7749, -96.8876]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-82.6850]])\n",
      " \n",
      "Q_target =  tensor([[-82.6564]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.0289]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.35915288,  0.41667166,  0.5501938 , -1.5374871 , -0.40606064,\n",
      "       -0.1227871 ,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0219,  0.8376, -0.0651, -1.1901,  0.0248,  0.0145,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0176,  1.3072,  0.2229, -0.6693, -0.0200, -0.0499,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3594,  0.8864, -0.9874, -0.4646,  0.4950,  0.2625,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0549,  1.4329,  0.6175,  0.0016, -0.0624, -0.1382,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[-4.8256e-04, -1.2223e+00, -1.8340e+00, -7.1238e-01]])\n",
      "Next states:  tensor([[[-0.0225,  0.8103, -0.0651, -1.2168,  0.0255,  0.0145,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0198,  1.2916,  0.2229, -0.6959, -0.0225, -0.0499,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3692,  0.8754, -0.9874, -0.4913,  0.5081,  0.2625,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0610,  1.4323,  0.6175, -0.0250, -0.0693, -0.1382,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-82.6854, -99.2276, -92.7773, -96.8902]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-82.6854, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-82.6854, -99.2275, -92.7772, -96.8901]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-82.6854]])\n",
      " \n",
      "Q_target =  tensor([[-82.7479]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.0625]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.3645977 ,  0.3815152 ,  0.55019194, -1.5641568 , -0.41219997,\n",
      "       -0.12278674,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-1.5460e-01,  1.2286e+00, -7.1095e-01, -6.4877e-01,  1.7505e-01,\n",
      "           1.5872e-01,  0.0000e+00,  0.0000e+00],\n",
      "         [-1.1279e-01,  1.4746e+00, -7.6919e-01,  1.2956e-03,  1.3368e-01,\n",
      "           2.1033e-01,  0.0000e+00,  0.0000e+00],\n",
      "         [-9.0166e-02,  1.3611e+00, -3.9664e-01, -3.8982e-01,  1.0209e-01,\n",
      "           8.8540e-02,  0.0000e+00,  0.0000e+00],\n",
      "         [-1.0501e-02,  1.4436e+00, -2.6555e-01,  3.2345e-01,  1.1973e-02,\n",
      "           5.9486e-02,  0.0000e+00,  0.0000e+00]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[-1.1981, -1.1011, -1.4367,  1.0574]])\n",
      "Next states:  tensor([[[-0.1616,  1.2135, -0.7110, -0.6755,  0.1830,  0.1587,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1204,  1.4740, -0.7692, -0.0254,  0.1442,  0.2103,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0941,  1.3517, -0.3967, -0.4165,  0.1065,  0.0885,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0131,  1.4503, -0.2656,  0.2968,  0.0149,  0.0595,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-82.6926, -99.2317, -92.7809, -96.8941]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-82.6926, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-82.6927, -99.2319, -92.7811, -96.8943]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-82.6927]])\n",
      " \n",
      "Q_target =  tensor([[-82.8701]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.1775]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.3700427 ,  0.34575927,  0.55019003, -1.5908262 , -0.41833928,\n",
      "       -0.12278644,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0198,  1.3861,  0.3996, -0.2736, -0.0225, -0.0895,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0294,  1.1457,  0.1102, -0.7841, -0.0333, -0.0246,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.5436,  0.1294,  0.1155, -1.4927,  1.6467,  1.4659,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1546,  0.7371, -0.4738, -1.3352,  0.1749,  0.1058,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 3., 0.]])\n",
      "Rewards:  tensor([[-1.3345, -0.9439, -8.5854, -0.1515]])\n",
      "Next states:  tensor([[[ 0.0237,  1.3794,  0.3996, -0.3003, -0.0269, -0.0895,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0305,  1.1275,  0.1102, -0.8108, -0.0345, -0.0246,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.5447,  0.0964,  0.1150, -1.5131,  1.7176,  1.4180,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1593,  0.7065, -0.4738, -1.3619,  0.1801,  0.1057,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-82.6974, -99.2240, -92.7739, -96.8866]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-82.6974, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-82.6974, -99.2240, -92.7739, -96.8866]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-82.6974]])\n",
      " \n",
      "Q_target =  tensor([[-83.0171]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.3197]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.3754878 ,  0.30940378,  0.5501881 , -1.6174958 , -0.4244786 ,\n",
      "       -0.12278609,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0373,  1.0836,  0.0921, -0.8887, -0.0422, -0.0206,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0854,  1.0406, -0.1515, -1.0359,  0.0965,  0.0338,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1047,  1.3376, -0.4074, -0.4598,  0.1185,  0.0909,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0216,  1.4214,  0.7416,  0.1568, -0.0236, -0.1448,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[-0.7006, -0.4387, -1.4173, -0.5255]])\n",
      "Next states:  tensor([[[ 0.0382,  1.0631,  0.0921, -0.9154, -0.0432, -0.0205,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0869,  1.0167, -0.1515, -1.0625,  0.0982,  0.0338,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1088,  1.3267, -0.4074, -0.4865,  0.1231,  0.0909,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0290,  1.4243,  0.7416,  0.1301, -0.0309, -0.1448,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-82.7210, -99.2293, -92.7788, -96.8918]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-82.7210, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-82.7210, -99.2293, -92.7788, -96.8918]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-82.7210]])\n",
      " \n",
      "Q_target =  tensor([[-83.2141]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.4931]], grad_fn=<SubBackward0>)\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████▉| 1999/2000 [40:09<00:01,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current state:  (array([ 0.3809331 ,  0.27244872,  0.55018616, -1.6441653 , -0.43061787,\n",
      "       -0.12278581,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.2873,  0.6088,  0.7646, -1.4331, -0.3252, -0.1706,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0672,  1.4946,  0.6801,  0.2516, -0.0763, -0.1522,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3445,  0.2568, -0.5903, -1.6441,  0.3892,  0.1318,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.6033, -0.1256,  0.6008, -1.6690, -0.5534, -0.1118,  1.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[  -0.6010,   -0.4191,   -1.5315, -100.0000]])\n",
      "Next states:  tensor([[[ 0.2949,  0.5760,  0.7646, -1.4598, -0.3338, -0.1706,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0740,  1.4996,  0.6801,  0.2249, -0.0839, -0.1521,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3503,  0.2193, -0.5903, -1.6707,  0.3958,  0.1318,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.6086, -0.1329,  0.3358, -0.2730, -0.3450,  3.9709,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-82.7424, -99.2182, -92.7688, -96.8811]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-82.7424, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-82.7419, -99.2173, -92.7679, -96.8802]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-82.7419]])\n",
      " \n",
      "Q_target =  tensor([[-83.4436]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.7012]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.38637847,  0.23489414,  0.5501842 , -1.6708348 , -0.43675715,\n",
      "       -0.12278547,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.3104,  0.2495,  0.3828, -1.7241, -0.3894, -0.1307,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0034,  1.2226,  0.0243, -0.7711, -0.0038, -0.0054,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0711,  1.2515,  0.3998, -0.6204, -0.0806, -0.0893,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.5626,  0.4575,  0.7244, -1.5708, -0.5408, -0.1234,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[-1.2306, -0.8978, -1.2732, -1.4093]])\n",
      "Next states:  tensor([[[ 0.3141,  0.2101,  0.3828, -1.7508, -0.3959, -0.1307,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0036,  1.2047,  0.0243, -0.7978, -0.0041, -0.0054,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0751,  1.2370,  0.3998, -0.6470, -0.0850, -0.0893,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.5698,  0.4216,  0.7244, -1.5975, -0.5469, -0.1234,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-82.7896, -99.2237, -92.7737, -96.8864]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-82.7896, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-82.7893, -99.2233, -92.7734, -96.8860]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-82.7893]])\n",
      " \n",
      "Q_target =  tensor([[-83.7371]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.9475]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.39182407,  0.19673997,  0.5501822 , -1.6975044 , -0.4428964 ,\n",
      "       -0.12278515,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.1113,  1.2806,  0.5924, -0.5451, -0.1260, -0.1323,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0077,  0.8291,  0.0208, -1.1379, -0.0404, -0.0535,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0961,  1.4528, -0.5119, -0.1423,  0.1089,  0.1143,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1929,  0.4588,  0.4757, -1.5664, -0.2180, -0.1062,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[-1.2645e+00, -3.1382e-01, -1.0046e+00,  4.4005e-04]])\n",
      "Next states:  tensor([[[ 0.1171,  1.2678,  0.5924, -0.5717, -0.1326, -0.1323,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0079,  0.8029,  0.0208, -1.1645, -0.0430, -0.0535,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1012,  1.4490, -0.5119, -0.1689,  0.1146,  0.1143,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1976,  0.4230,  0.4757, -1.5931, -0.2233, -0.1062,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-82.8499, -99.2264, -92.7761, -96.8890]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-82.8499, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-82.8499, -99.2264, -92.7761, -96.8890]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-82.8499]])\n",
      " \n",
      "Q_target =  tensor([[-84.0829]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-1.2330]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.39726982,  0.15798634,  0.5501802 , -1.7241739 , -0.44903564,\n",
      "       -0.12278491,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.2551,  0.7705,  0.4867, -1.2314, -0.2882, -0.1086,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0967,  1.1752,  0.6520, -0.8852, -0.1096, -0.1457,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0047,  1.3446, -0.1722,  0.5814,  0.1393,  0.0429,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.4594,  0.3002, -0.7871, -1.6121,  0.5193,  0.1756,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 2., 0.]])\n",
      "Rewards:  tensor([[-0.5047, -0.9007, -3.7515, -2.0327]])\n",
      "Next states:  tensor([[[ 0.2599,  0.7422,  0.4867, -1.2581, -0.2937, -0.1086,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1031,  1.1547,  0.6520, -0.9119, -0.1169, -0.1457,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0029,  1.3581, -0.1734,  0.6003,  0.1418,  0.0510,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.4672,  0.2634, -0.7871, -1.6388,  0.5281,  0.1756,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-82.9245, -99.2249, -92.7747, -96.8875]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-82.9245, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-82.9243, -99.2247, -92.7744, -96.8873]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-82.9243]])\n",
      " \n",
      "Q_target =  tensor([[-84.4809]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-1.5564]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.4027156 ,  0.11863311,  0.5501782 , -1.7508434 , -0.45517486,\n",
      "       -0.12278457,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.1095,  1.2702, -0.6921, -0.5913,  0.1241,  0.1547,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1143,  0.5112, -0.2185, -1.4382,  0.1237,  0.0464,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1314,  1.2820,  0.4889, -0.5171, -0.1009,  0.2844,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0226,  0.2546, -0.0506, -1.7002, -0.0154, -0.0094,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 1., 0.]])\n",
      "Rewards:  tensor([[-1.2071,  0.2912,  1.5365,  1.1495]])\n",
      "Next states:  tensor([[[-0.1163,  1.2563, -0.6922, -0.6180,  0.1318,  0.1547,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1164,  0.4782, -0.2185, -1.4648,  0.1260,  0.0464,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1364,  1.2698,  0.4789, -0.5428, -0.0846,  0.3250,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0231,  0.2158, -0.0506, -1.7269, -0.0159, -0.0094,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-83.0251, -99.2316, -92.7807, -96.8940]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-83.0251, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-83.0250, -99.2315, -92.7806, -96.8939]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-83.0250]])\n",
      " \n",
      "Q_target =  tensor([[-74.9397]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[8.0855]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.40816155,  0.07868036,  0.550176  , -1.777513  , -0.46131408,\n",
      "       -0.12278432,  1.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0476,  0.4161, -0.0746, -1.5403,  0.1398,  0.0476,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2081,  0.2875,  0.4188, -1.6525, -0.1484, -0.0577,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2649,  1.1611, -0.5580, -0.8591,  0.2994,  0.1245,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1930,  0.4627, -0.4516, -1.5408,  0.1523,  0.0674,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[ 0.5898, -0.1580, -1.0558,  0.1505]])\n",
      "Next states:  tensor([[[-0.0483,  0.3808, -0.0746, -1.5670,  0.1422,  0.0476,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2123,  0.2498,  0.4188, -1.6792, -0.1512, -0.0577,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2704,  1.1412, -0.5580, -0.8858,  0.3056,  0.1245,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1975,  0.4275, -0.4516, -1.5675,  0.1557,  0.0674,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-82.5217, -99.2215, -92.7719, -96.8844]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-82.5217, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-82.5213, -99.2210, -92.7715, -96.8839]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-82.5213]])\n",
      " \n",
      "Q_target =  tensor([[-82.3792]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.1425]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.41340485,  0.03893331,  0.54140747, -1.7816132 , -0.4672384 ,\n",
      "       -0.08795334,  1.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0263,  0.5305,  0.0386, -1.4741, -0.0298, -0.0086,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1271,  1.4823,  0.8120, -0.0026, -0.1550, -0.2251,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0114,  1.2772,  0.0303, -0.6503, -0.0129, -0.0068,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1729,  0.9000, -0.5640, -1.1337,  0.1956,  0.1259,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[ 0.6614, -1.1882, -1.1747, -0.5718]])\n",
      "Next states:  tensor([[[ 0.0267,  0.4967,  0.0386, -1.5008, -0.0302, -0.0086,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1351,  1.4816,  0.8121, -0.0293, -0.1663, -0.2251,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0117,  1.2620,  0.0303, -0.6770, -0.0132, -0.0068,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1784,  0.8739, -0.5640, -1.1603,  0.2018,  0.1259,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-82.5144, -99.2236, -92.7738, -96.8863]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-82.5144, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-82.5143, -99.2234, -92.7736, -96.8861]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-82.5143]])\n",
      " \n",
      "Q_target =  tensor([[-100.]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-17.4856]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Score:  -163.8256674167724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number:  1999\n",
      "Current state:  (array([ 0.00471973,  1.4090972 ,  0.47803965, -0.08103362, -0.00546217,\n",
      "       -0.10828307,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0127,  1.2205,  0.0990, -0.8109, -0.0144, -0.0221,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3870, -0.0295,  0.7383, -1.9030, -0.4376, -0.1648,  1.0000,\n",
      "           0.0000],\n",
      "         [-0.1878,  0.6164, -0.4999, -1.4236,  0.2124,  0.1116,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.4754,  0.7720, -0.4993, -1.0118,  0.1614, -0.6188,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 3.]])\n",
      "Rewards:  tensor([[  -0.8753, -100.0000,   -0.1175,    3.2166]])\n",
      "Next states:  tensor([[[ 0.0137,  1.2017,  0.0990, -0.8375, -0.0155, -0.0221,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3964, -0.0601,  0.7824, -0.9875, -0.2429,  6.1424,  1.0000,\n",
      "           0.0000],\n",
      "         [-0.1928,  0.5838, -0.4999, -1.4503,  0.2180,  0.1116,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.4807,  0.7486, -0.4881, -1.0370,  0.1282, -0.6649,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-83.5920, -99.2313, -92.7799, -96.8936]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-83.5920, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-83.5890, -99.2270, -92.7760, -96.8894]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-83.5890]])\n",
      " \n",
      "Q_target =  tensor([[-83.4819]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.1101]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.00943947,  1.406697  ,  0.47739387, -0.10670169, -0.01081292,\n",
      "       -0.10702448,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.4533,  0.3460,  0.5874, -1.6350, -0.5122, -0.1311,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3165,  1.4038,  0.8071, -0.5314, -0.3762, -0.2158,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1611,  0.0502, -0.2361, -1.7837,  0.1820,  0.0527,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0762,  1.3139, -0.5933, -0.4918,  0.0865,  0.1327,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[-1.4640, -1.5309,  7.5958, -1.2655]])\n",
      "Next states:  tensor([[[ 0.4591,  0.3087,  0.5874, -1.6617, -0.5188, -0.1311,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3245,  1.3913,  0.8071, -0.5581, -0.3870, -0.2158,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1634,  0.0095, -0.2361, -1.8103,  0.1847,  0.0527,  0.0000,\n",
      "           1.0000],\n",
      "         [-0.0821,  1.3023, -0.5933, -0.5185,  0.0931,  0.1327,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-83.5832, -99.2287, -92.7777, -96.8911]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-83.5832, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-83.5827, -99.2281, -92.7771, -96.8905]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-83.5827]])\n",
      " \n",
      "Q_target =  tensor([[-83.6374]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.0542]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.01415949,  1.4036974 ,  0.4774107 , -0.13336985, -0.01616047,\n",
      "       -0.10696103,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0971,  1.3478,  0.3902, -0.4323, -0.0724, -0.0535,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3143,  0.6691,  0.5778, -1.3207, -0.3555, -0.1289,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1609,  1.2118, -0.3336, -0.8113,  0.1497,  0.0425,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0746,  1.2281, -0.2393, -0.6337,  0.0997,  0.0702,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[-1.3367, -0.6168, -0.8612, -1.3843]])\n",
      "Next states:  tensor([[[ 0.1009,  1.3374,  0.3901, -0.4600, -0.0751, -0.0535,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3200,  0.6388,  0.5778, -1.3474, -0.3619, -0.1289,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1643,  1.1930, -0.3336, -0.8380,  0.1518,  0.0425,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0769,  1.2132, -0.2393, -0.6604,  0.1032,  0.0702,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-83.5902, -99.2334, -92.7819, -96.8956]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-83.5902, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-83.5902, -99.2334, -92.7819, -96.8956]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-83.5902]])\n",
      " \n",
      "Q_target =  tensor([[-83.7194]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.1291]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.0188797 ,  1.4000978 ,  0.47742644, -0.16004294, -0.02150732,\n",
      "       -0.10694723,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.1175,  0.9496,  0.2241, -1.0807, -0.1327, -0.0500,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0771,  1.3139, -0.6001, -0.4920,  0.0875,  0.1342,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1908,  0.6377,  0.3574, -1.3439, -0.2157, -0.0798,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0204,  1.3386, -0.1472, -0.4028,  0.0231,  0.0329,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[-0.4190, -1.2625, -0.1378, -1.7150]])\n",
      "Next states:  tensor([[[ 0.1197,  0.9247,  0.2241, -1.1074, -0.1352, -0.0500,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0831,  1.3022, -0.6002, -0.5187,  0.0942,  0.1342,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1944,  0.6068,  0.3574, -1.3705, -0.2196, -0.0798,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0218,  1.3290, -0.1472, -0.4294,  0.0247,  0.0329,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-83.6002, -99.2359, -92.7841, -96.8980]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-83.6002, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-83.6001, -99.2358, -92.7840, -96.8979]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-83.6001]])\n",
      " \n",
      "Q_target =  tensor([[-83.7975]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.1974]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.0236001 ,  1.3958987 ,  0.4774421 , -0.18671355, -0.02685335,\n",
      "       -0.10693035,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0026,  1.4222,  0.1313,  0.2383, -0.0030, -0.0294,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2089,  1.1256, -0.7544, -0.8146,  0.2367,  0.1684,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1049,  1.2176,  0.4081, -0.6648, -0.1187, -0.0911,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2424,  0.6047,  0.3966, -1.3637, -0.1727, -0.0544,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[ 1.6758, -1.0964, -1.2269, -0.0967]])\n",
      "Next states:  tensor([[[ 0.0039,  1.4270,  0.1313,  0.2117, -0.0044, -0.0294,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2163,  1.1067, -0.7544, -0.8412,  0.2451,  0.1684,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1090,  1.2021,  0.4081, -0.6915, -0.1233, -0.0911,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2463,  0.5734,  0.3966, -1.3904, -0.1755, -0.0544,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-83.6076, -99.2300, -92.7788, -96.8923]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-83.6076, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-83.6079, -99.2303, -92.7791, -96.8926]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-83.6079]])\n",
      " \n",
      "Q_target =  tensor([[-83.8671]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.2595]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.0283206 ,  1.3911    ,  0.47745785, -0.21338381, -0.03219853,\n",
      "       -0.10691327,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-2.0226e-02,  1.3502e+00, -4.0917e-01, -5.9290e-01,  2.3026e-02,\n",
      "           9.1641e-02,  0.0000e+00,  0.0000e+00],\n",
      "         [-4.3055e-01,  3.0327e-01, -5.5799e-01, -1.6592e+00,  4.8619e-01,\n",
      "           1.2454e-01,  0.0000e+00,  0.0000e+00],\n",
      "         [-1.0085e-01,  4.0143e-01, -1.3101e-01, -1.6083e+00,  7.6031e-02,\n",
      "           1.2948e-02,  0.0000e+00,  0.0000e+00],\n",
      "         [ 1.3235e-01,  5.6654e-01,  1.7960e-01, -1.4744e+00, -2.7803e-02,\n",
      "           7.0158e-04,  0.0000e+00,  0.0000e+00]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[-1.2823, -1.5366,  0.7993,  0.5959]])\n",
      "Next states:  tensor([[[-2.4271e-02,  1.3363e+00, -4.0919e-01, -6.1957e-01,  2.7607e-02,\n",
      "           9.1626e-02,  0.0000e+00,  0.0000e+00],\n",
      "         [-4.3607e-01,  2.6539e-01, -5.5799e-01, -1.6859e+00,  4.9241e-01,\n",
      "           1.2454e-01,  0.0000e+00,  0.0000e+00],\n",
      "         [-1.0215e-01,  3.6465e-01, -1.3101e-01, -1.6349e+00,  7.6679e-02,\n",
      "           1.2948e-02,  0.0000e+00,  0.0000e+00],\n",
      "         [ 1.3414e-01,  5.3276e-01,  1.7960e-01, -1.5010e+00, -2.7768e-02,\n",
      "           7.0156e-04,  0.0000e+00,  0.0000e+00]]])\n",
      " \n",
      "All Q_current:  tensor([[-83.6283, -99.2359, -92.7841, -96.8980]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-83.6283, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-83.6281, -99.2356, -92.7839, -96.8978]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-83.6281]])\n",
      " \n",
      "Q_target =  tensor([[-83.9421]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.3138]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.03304138,  1.3857017 ,  0.47747356, -0.24005418, -0.03754287,\n",
      "       -0.10689659,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.3282,  0.2031, -0.4474, -1.6622,  0.3713,  0.0895,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0533,  1.4768, -0.5393,  0.1726,  0.0605,  0.1207,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0085,  1.4748,  0.0455, -0.1313, -0.0368, -0.0486,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0619,  1.4446, -0.7832,  0.0937,  0.0704,  0.1753,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[-1.5640, -0.1986, -2.4315, -0.7934]])\n",
      "Next states:  tensor([[[-0.3326,  0.1652, -0.4474, -1.6888,  0.3757,  0.0895,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0586,  1.4801, -0.5393,  0.1459,  0.0666,  0.1207,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0089,  1.4713,  0.0455, -0.1580, -0.0393, -0.0486,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0697,  1.4461, -0.7832,  0.0670,  0.0791,  0.1753,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-83.6441, -99.2316, -92.7802, -96.8938]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-83.6441, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-83.6440, -99.2314, -92.7800, -96.8937]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-83.6440]])\n",
      " \n",
      "Q_target =  tensor([[-84.0057]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.3616]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.03776226,  1.3797035 ,  0.47748914, -0.26672468, -0.04288635,\n",
      "       -0.10687964,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0015,  1.4062,  0.1562, -0.2079, -0.0018, -0.0354,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1449,  1.2977, -0.6373, -0.5126,  0.1640,  0.1423,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1770,  0.7174,  0.4589, -1.2980, -0.2001, -0.1024,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2169,  0.4192,  0.4362, -1.5355, -0.1429, -0.0553,  0.0000,\n",
      "           0.0000]]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[-1.7311, -1.2783, -0.2550,  0.0337]])\n",
      "Next states:  tensor([[[ 0.0031,  1.4010,  0.1560, -0.2335, -0.0035, -0.0350,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1512,  1.2856, -0.6373, -0.5393,  0.1711,  0.1422,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1815,  0.6876,  0.4589, -1.3247, -0.2052, -0.1024,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2212,  0.3840,  0.4362, -1.5622, -0.1457, -0.0553,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-83.6676, -99.2334, -92.7818, -96.8955]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-83.6676, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-83.6676, -99.2334, -92.7819, -96.8956]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-83.6676]])\n",
      " \n",
      "Q_target =  tensor([[-84.0699]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.4023]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.04248333,  1.3731059 ,  0.47750473, -0.2933953 , -0.04822901,\n",
      "       -0.10686308,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0185,  1.2769,  0.0780, -0.5548, -0.0210, -0.0174,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0495,  1.4979,  0.4170,  0.1754, -0.0561, -0.0933,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2644,  0.9872, -0.5688, -1.0152,  0.2988,  0.1269,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3277,  0.3806,  0.5864, -1.5516, -0.2923, -0.0878,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[-1.4222,  0.1492, -0.8589, -0.7058]])\n",
      "Next states:  tensor([[[ 0.0193,  1.2638,  0.0781, -0.5815, -0.0218, -0.0174,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0536,  1.5012,  0.4170,  0.1487, -0.0608, -0.0932,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2700,  0.9638, -0.5688, -1.0419,  0.3051,  0.1269,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3335,  0.3451,  0.5864, -1.5783, -0.2967, -0.0878,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-83.6924, -99.2335, -92.7819, -96.8957]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-83.6924, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-83.6925, -99.2336, -92.7820, -96.8958]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-83.6925]])\n",
      " \n",
      "Q_target =  tensor([[-84.1283]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.4359]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.04720459,  1.3659086 ,  0.47752023, -0.32006606, -0.05357085,\n",
      "       -0.10684623,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.1122,  0.0546,  0.1774, -1.7824, -0.1268, -0.0396,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1676,  1.3430,  0.6866, -0.4422, -0.2041, -0.1939,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1565,  0.8503,  0.5378, -1.2041, -0.1788, -0.1378,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2406,  0.1321, -0.4056, -1.7348,  0.2719,  0.0905,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[-1.8546, -1.5212, -0.5093,  8.2461]])\n",
      "Next states:  tensor([[[ 0.1140,  0.0139,  0.1774, -1.8090, -0.1288, -0.0396,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1744,  1.3325,  0.6862, -0.4693, -0.2142, -0.2020,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1618,  0.8226,  0.5378, -1.2308, -0.1857, -0.1378,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2446,  0.0925, -0.4056, -1.7615,  0.2764,  0.0905,  0.0000,\n",
      "           1.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-83.7139, -99.2270, -92.7760, -96.8894]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-83.7139, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-83.7135, -99.2265, -92.7756, -96.8889]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-83.7135]])\n",
      " \n",
      "Q_target =  tensor([[-84.1762]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.4624]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.05192604,  1.3581116 ,  0.47753564, -0.34673694, -0.05891184,\n",
      "       -0.10682968,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.4080,  0.2060, -0.6249, -1.6796,  0.4608,  0.1395,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1433,  1.1756,  0.3151, -0.8281, -0.1620, -0.0703,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2552,  1.3073,  0.6453, -0.6365, -0.2889, -0.1440,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1122,  1.2341, -0.5974, -0.6539,  0.1271,  0.1334,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[-2.1765, -0.9798, -1.2764, -1.1873]])\n",
      "Next states:  tensor([[[-0.4142,  0.1677, -0.6249, -1.7063,  0.4678,  0.1395,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1464,  1.1564,  0.3151, -0.8548, -0.1655, -0.0703,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2616,  1.2925,  0.6453, -0.6631, -0.2961, -0.1440,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1181,  1.2188, -0.5974, -0.6806,  0.1338,  0.1334,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-83.7479, -99.2342, -92.7825, -96.8963]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-83.7479, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-83.7476, -99.2338, -92.7822, -96.8959]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-83.7476]])\n",
      " \n",
      "Q_target =  tensor([[-84.2308]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.4828]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.05664759,  1.3497149 ,  0.47755098, -0.3734079 , -0.06425201,\n",
      "       -0.10681303,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0183,  1.4818, -0.1851,  0.1952,  0.0208,  0.0414,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0426,  0.0434, -0.0862, -1.8694,  0.0482,  0.0192,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1128,  1.1840, -0.5188, -0.7386,  0.1278,  0.1158,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0989,  1.1703,  0.5561, -0.8210, -0.1121, -0.1242,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[ 1.2786,  8.9724, -1.1128, -0.9906]])\n",
      "Next states:  tensor([[[-2.0131e-02,  1.4856e+00, -1.8514e-01,  1.6855e-01,  2.2850e-02,\n",
      "           4.1419e-02,  0.0000e+00,  0.0000e+00],\n",
      "         [-4.3462e-02,  6.9867e-04, -8.6197e-02, -1.8961e+00,  4.9138e-02,\n",
      "           1.9242e-02,  0.0000e+00,  1.0000e+00],\n",
      "         [-1.1795e-01,  1.1668e+00, -5.1886e-01, -7.6529e-01,  1.3354e-01,\n",
      "           1.1582e-01,  0.0000e+00,  0.0000e+00],\n",
      "         [ 1.0444e-01,  1.1513e+00,  5.5611e-01, -8.4763e-01, -1.1830e-01,\n",
      "          -1.2422e-01,  0.0000e+00,  0.0000e+00]]])\n",
      " \n",
      "All Q_current:  tensor([[-83.7751, -99.2311, -92.7797, -96.8933]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-83.7751, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-83.7749, -99.2308, -92.7794, -96.8930]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-83.7749]])\n",
      " \n",
      "Q_target =  tensor([[-84.2725]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.4974]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.06136932,  1.3407185 ,  0.47756624, -0.40007898, -0.06959134,\n",
      "       -0.10679652,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.1784,  0.3030, -0.2753, -1.5879,  0.1958,  0.0451,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2670,  0.5621,  0.4354, -1.4227, -0.3016, -0.0972,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0774,  1.0715,  0.4119, -1.0342, -0.0876, -0.0920,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2596,  0.5131, -0.6564, -1.5190,  0.2935,  0.1465,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 2., 0.]])\n",
      "Rewards:  tensor([[ 0.0706, -0.3059,  2.4295, -0.4192]])\n",
      "Next states:  tensor([[[-0.1811,  0.2666, -0.2753, -1.6146,  0.1980,  0.0451,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2713,  0.5295,  0.4354, -1.4494, -0.3065, -0.0972,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0815,  1.0484,  0.4147, -1.0230, -0.0923, -0.0942,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2661,  0.4783, -0.6564, -1.5457,  0.3008,  0.1465,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-83.8060, -99.2316, -92.7802, -96.8939]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-83.8060, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-83.8055, -99.2310, -92.7796, -96.8933]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-83.8055]])\n",
      " \n",
      "Q_target =  tensor([[-84.3119]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.5059]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.06609134,  1.3311225 ,  0.47758144, -0.4267502 , -0.07492986,\n",
      "       -0.10678017,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0370,  1.2859,  0.1702, -0.5325, -0.0419, -0.0380,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1800,  0.7415, -0.5518, -1.3294,  0.2036,  0.1232,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2915, -0.0365, -0.5431, -1.8518,  0.3111,  0.1040,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2054,  1.3385, -0.2007, -0.7217, -0.6212, -1.0455,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[-1.4835, -0.2588, 15.5822, -6.2425]])\n",
      "Next states:  tensor([[[ 0.0387,  1.2733,  0.1702, -0.5592, -0.0438, -0.0380,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1855,  0.7110, -0.5518, -1.3561,  0.2098,  0.1232,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2969, -0.0788, -0.5431, -1.8785,  0.3163,  0.1040,  1.0000,\n",
      "           1.0000],\n",
      "         [-0.2078,  1.3222, -0.2008, -0.7486, -0.6735, -1.0453,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-83.8402, -99.2357, -92.7837, -96.8977]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-83.8402, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-83.8399, -99.2353, -92.7834, -96.8974]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-83.8399]])\n",
      " \n",
      "Q_target =  tensor([[-84.3496]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.5094]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.07081346,  1.3209269 ,  0.4775966 , -0.45342156, -0.08026757,\n",
      "       -0.10676368,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0428,  1.3932, -0.3939, -0.2050,  0.0486,  0.0881,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.6137,  0.4133, -0.7920, -1.1823,  0.4014, -0.4608,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1871,  0.9428,  0.6760, -1.1046, -0.2121, -0.1509,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0118,  1.4472, -0.2379,  0.2696,  0.0134,  0.0533,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 3., 0., 0.]])\n",
      "Rewards:  tensor([[-1.2260,  1.8477, -0.6796,  1.1382]])\n",
      "Next states:  tensor([[[-0.0467,  1.3880, -0.3939, -0.2317,  0.0530,  0.0881,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.6218,  0.3860, -0.7818, -1.2065,  0.3761, -0.5058,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1938,  0.9173,  0.6760, -1.1312, -0.2197, -0.1509,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0141,  1.4527, -0.2379,  0.2429,  0.0161,  0.0533,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-83.8704, -99.2343, -92.7825, -96.8964]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-83.8704, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-83.8705, -99.2343, -92.7825, -96.8964]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-83.8705]])\n",
      " \n",
      "Q_target =  tensor([[-84.3789]])\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error(Q_target - Q_current):  tensor([[-0.5084]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.07553577,  1.3101315 ,  0.47761163, -0.48009297, -0.08560447,\n",
      "       -0.10674751,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.1964,  1.2302, -0.6849, -0.6518,  0.2226,  0.1529,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0335,  0.0091, -0.0462, -1.8085, -0.0102, -0.0048,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0872,  0.9059, -0.2738, -1.0987,  0.1407,  0.1165,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0312,  1.4585, -0.6308,  0.3694,  0.0355,  0.1413,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[-1.2274, 16.1020, -0.6802, -0.1830]])\n",
      "Next states:  tensor([[[-0.2032,  1.2149, -0.6849, -0.6785,  0.2302,  0.1529,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0340, -0.0322, -0.0462, -1.8352, -0.0105, -0.0048,  1.0000,\n",
      "           1.0000],\n",
      "         [-0.0899,  0.8806, -0.2738, -1.1254,  0.1465,  0.1165,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0374,  1.4662, -0.6308,  0.3427,  0.0426,  0.1412,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-83.9040, -99.2372, -92.7851, -96.8992]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-83.9040, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-83.9034, -99.2365, -92.7845, -96.8986]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-83.9034]])\n",
      " \n",
      "Q_target =  tensor([[-84.4059]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.5020]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.08025837,  1.2987365 ,  0.47762662, -0.50676453, -0.09094055,\n",
      "       -0.10673132,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.1628,  0.1348,  0.1543, -1.7744, -1.7665, -1.6181,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0209,  1.2318,  0.0782, -0.6424, -0.0236, -0.0174,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2324,  0.1979, -0.4643, -1.7321,  0.2293,  0.0706,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1811,  0.5384,  0.4162, -1.4556, -0.2047, -0.0929,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[-8.3158, -1.2313, -0.9098, -0.0214]])\n",
      "Next states:  tensor([[[-0.1611,  0.0955,  0.1534, -1.8010, -1.8473, -1.6174,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0216,  1.2167,  0.0782, -0.6691, -0.0245, -0.0174,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2370,  0.1583, -0.4643, -1.7587,  0.2328,  0.0706,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1852,  0.5050,  0.4162, -1.4823, -0.2093, -0.0929,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-83.9271, -99.2280, -92.7767, -96.8903]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-83.9271, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-83.9267, -99.2274, -92.7762, -96.8897]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-83.9267]])\n",
      " \n",
      "Q_target =  tensor([[-84.4193]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.4922]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.08498106,  1.2867417 ,  0.47764152, -0.5334361 , -0.09627582,\n",
      "       -0.10671531,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0771,  0.7286, -0.1108, -1.2309,  0.7218,  1.1545,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0470,  0.3437, -0.0932, -1.5972,  0.0531,  0.0208,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1599,  1.0093, -0.4492, -0.9635,  0.1808,  0.1002,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1895,  0.8600, -0.3142, -1.2021,  0.2141,  0.0701,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[3., 0., 0., 0.]])\n",
      "Rewards:  tensor([[-4.9682,  0.8357, -0.7977, -0.3053]])\n",
      "Next states:  tensor([[[ 0.0765,  0.7010, -0.1019, -1.2533,  0.7771,  1.1062,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0479,  0.3071, -0.0932, -1.6239,  0.0542,  0.0208,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1643,  0.9870, -0.4492, -0.9902,  0.1858,  0.1002,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1926,  0.8324, -0.3142, -1.2287,  0.2176,  0.0701,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-83.9643, -99.2364, -92.7843, -96.8984]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-83.9643, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-83.9639, -99.2359, -92.7838, -96.8979]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-83.9639]])\n",
      " \n",
      "Q_target =  tensor([[-84.4428]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.4785]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.08970394,  1.2741474 ,  0.47765636, -0.56010795, -0.10161029,\n",
      "       -0.10669907,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.1435,  1.0015,  0.4478, -0.9824, -0.1320, -0.0625,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0574,  1.3396, -0.7259, -0.4899,  0.0652,  0.1625,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1278,  1.0443, -0.5389, -0.9858,  0.1447,  0.1203,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0738,  1.2656,  0.4150, -0.5855, -0.0837, -0.0927,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[-0.5638, -1.2062, -0.7575, -1.3067]])\n",
      "Next states:  tensor([[[ 0.1480,  0.9788,  0.4478, -1.0090, -0.1352, -0.0625,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0646,  1.3279, -0.7260, -0.5166,  0.0733,  0.1625,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1332,  1.0216, -0.5389, -1.0125,  0.1507,  0.1202,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0779,  1.2519,  0.4150, -0.6121, -0.0883, -0.0927,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-83.9960, -99.2396, -92.7872, -96.9015]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-83.9960, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-83.9959, -99.2395, -92.7871, -96.9014]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-83.9959]])\n",
      " \n",
      "Q_target =  tensor([[-84.4578]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.4618]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.09442692,  1.2609534 ,  0.47767106, -0.5867798 , -0.10694394,\n",
      "       -0.10668273,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.2979,  0.9753, -0.4858, -1.1267,  0.3365,  0.1084,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1771,  0.3266, -0.2894, -1.6123,  2.4239,  0.4848,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0566,  1.0587,  0.1041, -1.0051, -0.0640, -0.0232,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1148,  1.3197,  0.6441, -0.4537,  0.0543,  0.3484,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 3., 0., 2.]])\n",
      "Rewards:  tensor([[-0.6641, -1.1282, -0.4568, -0.5954]])\n",
      "Next states:  tensor([[[-0.3027,  0.9494, -0.4858, -1.1534,  0.3419,  0.1084,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1739,  0.2901, -0.2973, -1.6338,  2.4457,  0.4351,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0576,  1.0355,  0.1041, -1.0317, -0.0651, -0.0232,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1215,  1.3100,  0.6510, -0.4332,  0.0723,  0.3590,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-84.0247, -99.2398, -92.7873, -96.9017]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-84.0247, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-84.0243, -99.2394, -92.7869, -96.9013]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-84.0243]])\n",
      " \n",
      "Q_target =  tensor([[-84.4660]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.4414]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.09915018,  1.2471595 ,  0.4776857 , -0.61345184, -0.11227681,\n",
      "       -0.1066668 ,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0503,  1.3895,  0.5085, -0.2152, -0.0570, -0.1138,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2387,  0.3092, -0.3306, -1.6315,  0.2696,  0.0738,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.4492,  0.3884,  0.5938, -1.6110, -0.7088, -0.2262,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0787,  0.8087, -0.0881, -1.1708,  0.5480,  1.2179,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 3.]])\n",
      "Rewards:  tensor([[-1.1391, -0.3199, -1.7627, -5.4626]])\n",
      "Next states:  tensor([[[ 0.0553,  1.3841,  0.5085, -0.2419, -0.0627, -0.1138,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2420,  0.2719, -0.3306, -1.6581,  0.2733,  0.0738,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.4551,  0.3517,  0.5937, -1.6377, -0.7201, -0.2262,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0784,  0.7823, -0.0778, -1.1938,  0.6064,  1.1678,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-84.0458, -99.2326, -92.7808, -96.8947]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-84.0458, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-84.0457, -99.2324, -92.7807, -96.8945]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-84.0457]])\n",
      " \n",
      "Q_target =  tensor([[-84.4645]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.4188]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.10387354,  1.232766  ,  0.47770017, -0.64012396, -0.11760889,\n",
      "       -0.10665122,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0505,  1.0442, -0.0219, -1.0541,  1.8641,  1.6498,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0158,  1.4282,  0.8001,  0.3712, -0.0181, -0.1794,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1927,  1.0178,  0.6496, -0.9705, -0.2184, -0.1450,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0420,  1.0572, -0.1063, -0.9136,  0.0476,  0.0237,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[2., 0., 0., 0.]])\n",
      "Rewards:  tensor([[-10.9678,  -0.5982,  -0.8732,  -0.6584]])\n",
      "Next states:  tensor([[[ 0.0494,  1.0206, -0.0848, -1.0992,  1.9461,  1.6410,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0237,  1.4360,  0.8002,  0.3446, -0.0271, -0.1793,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1991,  0.9953,  0.6496, -0.9972, -0.2256, -0.1450,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0431,  1.0360, -0.1063, -0.9403,  0.0487,  0.0237,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-84.0738, -99.2354, -92.7833, -96.8974]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-84.0738, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-84.0741, -99.2358, -92.7837, -96.8978]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-84.0741]])\n",
      " \n",
      "Q_target =  tensor([[-84.4675]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.3937]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.10859718,  1.2177727 ,  0.4777147 , -0.66679615, -0.12294018,\n",
      "       -0.10663513,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.2047,  0.9928,  0.3383, -0.9802,  0.4491,  0.8963,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0795,  0.9549, -0.0316, -0.9962,  0.2139,  0.9680,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0379,  1.3887,  0.4417, -0.2161,  0.0036,  0.1379,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0603,  1.0784, -0.2729, -0.9330,  0.0819,  0.0802,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 1., 1., 1.]])\n",
      "Rewards:  tensor([[-4.9137, -5.6620, -0.6174, -1.3492]])\n",
      "Next states:  tensor([[[ 0.2085,  0.9704,  0.3385, -1.0070,  0.4939,  0.8961,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0796,  0.9321, -0.0421, -1.0244,  0.2645,  1.0112,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0423,  1.3832,  0.4295, -0.2432,  0.0129,  0.1864,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0631,  1.0568, -0.2832, -0.9597,  0.0880,  0.1211,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-84.1007, -99.2390, -92.7865, -96.9008]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-84.1007, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-84.1007, -99.2389, -92.7864, -96.9008]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-84.1007]])\n",
      " \n",
      "Q_target =  tensor([[-84.4665]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.3657]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.11332102,  1.2021798 ,  0.47772902, -0.69346845, -0.12827067,\n",
      "       -0.10661912,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.1780,  0.8444, -0.4091, -1.1465,  0.2012,  0.0913,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.4231,  0.6169,  0.6013, -1.4159, -0.4412, -0.1106,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2154,  0.0038,  0.3571, -1.8260, -0.2434, -0.0797,  1.0000,\n",
      "           0.0000],\n",
      "         [-0.0229,  1.3599, -0.3309, -0.4040,  0.0260,  0.0741,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[  -0.4788,   -0.7118, -100.0000,   -1.4978]])\n",
      "Next states:  tensor([[[-0.1820,  0.8180, -0.4091, -1.1731,  0.2058,  0.0913,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.4290,  0.5845,  0.6013, -1.4426, -0.4467, -0.1106,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2182, -0.0232, -0.0552, -0.7890, -0.0541,  6.1148,  1.0000,\n",
      "           1.0000],\n",
      "         [-0.0262,  1.3502, -0.3309, -0.4307,  0.0297,  0.0741,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-84.1213, -99.2368, -92.7846, -96.8988]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-84.1213, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-84.1197, -99.2344, -92.7823, -96.8964]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-84.1197]])\n",
      " \n",
      "Q_target =  tensor([[-84.4559]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.3346]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.11804505,  1.1859872 ,  0.4777433 , -0.720141  , -0.13360035,\n",
      "       -0.10660315,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0174,  1.4411, -0.1352, -0.0568,  0.0197,  0.0302,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0164,  1.4113, -0.4139, -0.0353,  0.0187,  0.0927,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0458,  1.4976,  0.2440, -0.0372, -0.0519, -0.0545,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3366, -0.0487, -0.6375, -1.9110,  0.4064,  0.1791,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[-1.1896, -0.6421, -0.6778, 15.0744]])\n",
      "Next states:  tensor([[[-0.0187,  1.4392, -0.1352, -0.0835,  0.0212,  0.0302,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0205,  1.4099, -0.4139, -0.0620,  0.0233,  0.0927,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0482,  1.4962,  0.2440, -0.0639, -0.0546, -0.0545,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3429, -0.0923, -0.6375, -1.9376,  0.4154,  0.1791,  1.0000,\n",
      "           1.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-84.1415, -99.2361, -92.7838, -96.8980]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-84.1415, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-84.1416, -99.2363, -92.7840, -96.8982]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-84.1416]])\n",
      " \n",
      "Q_target =  tensor([[-84.5134]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.3719]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.12276717,  1.1691737 ,  0.4775644 , -0.74775857, -0.13892986,\n",
      "       -0.10659017,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.1493,  1.0120, -0.6566, -1.0646,  0.1690,  0.1466,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1779,  1.2770,  0.7770, -0.5522, -0.1852, -0.1416,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1551,  1.3681, -0.5601, -0.4292,  0.1755,  0.1250,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.4135,  0.9951,  0.7741, -1.0509, -0.4675, -0.1728,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[-0.6843, -1.1005, -1.3247, -1.0938]])\n",
      "Next states:  tensor([[[-0.1558,  0.9875, -0.6566, -1.0913,  0.1763,  0.1465,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1856,  1.2640,  0.7770, -0.5788, -0.1923, -0.1416,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1606,  1.3579, -0.5601, -0.4558,  0.1817,  0.1250,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.4212,  0.9709,  0.7741, -1.0775, -0.4761, -0.1728,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-84.1697, -99.2432, -92.7903, -96.9049]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-84.1697, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-84.1696, -99.2431, -92.7902, -96.9048]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-84.1696]])\n",
      " \n",
      "Q_target =  tensor([[-84.4391]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.2694]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.12748937,  1.1517606 ,  0.4775638 , -0.7744276 , -0.14425932,\n",
      "       -0.10658958,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0841,  1.2615, -0.6077, -0.6477,  0.0954,  0.1359,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0548,  1.4481,  0.2157, -0.0884,  0.2267,  0.5488,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0752,  1.4774,  0.5432,  0.0375, -0.0852, -0.1214,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0690,  1.2722, -0.2180, -0.6068,  0.0780,  0.0487,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 1., 0., 0.]])\n",
      "Rewards:  tensor([[-1.1729, -3.0124, -0.5436, -1.3482]])\n",
      "Next states:  tensor([[[-0.0901,  1.2464, -0.6077, -0.6744,  0.1022,  0.1359,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0571,  1.4456,  0.2051, -0.1164,  0.2563,  0.5922,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0805,  1.4776,  0.5432,  0.0109, -0.0913, -0.1214,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0711,  1.2580, -0.2180, -0.6334,  0.0805,  0.0487,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-84.1857, -99.2423, -92.7895, -96.9040]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-84.1857, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-84.1858, -99.2425, -92.7897, -96.9042]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-84.1858]])\n",
      " \n",
      "Q_target =  tensor([[-84.4210]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.2353]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.1322116 ,  1.1337477 ,  0.47756332, -0.8010967 , -0.14958878,\n",
      "       -0.10658946,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-7.0190e-04,  7.9988e-01, -2.0910e-03, -1.2394e+00,  8.0132e-04,\n",
      "           4.6558e-04,  0.0000e+00,  0.0000e+00],\n",
      "         [-2.7839e-01,  2.3410e-01, -3.9649e-01, -1.6708e+00,  3.1449e-01,\n",
      "           8.8497e-02,  0.0000e+00,  0.0000e+00],\n",
      "         [-2.1483e-01,  3.5795e-01, -3.8115e-01, -1.5685e+00,  2.4277e-01,\n",
      "           8.5076e-02,  0.0000e+00,  0.0000e+00],\n",
      "         [ 1.4592e-01,  1.3312e+00,  6.7106e-01, -4.4158e-01, -1.6521e-01,\n",
      "          -1.4981e-01,  0.0000e+00,  0.0000e+00]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[ 0.1797, -1.0286, -0.1964, -1.2764]])\n",
      "Next states:  tensor([[[-7.2250e-04,  7.7139e-01, -2.0910e-03, -1.2661e+00,  8.2458e-04,\n",
      "           4.6555e-04,  0.0000e+00,  0.0000e+00],\n",
      "         [-2.8231e-01,  1.9593e-01, -3.9649e-01, -1.6975e+00,  3.1891e-01,\n",
      "           8.8497e-02,  0.0000e+00,  0.0000e+00],\n",
      "         [-2.1860e-01,  3.2208e-01, -3.8115e-01, -1.5952e+00,  2.4702e-01,\n",
      "           8.5076e-02,  0.0000e+00,  0.0000e+00],\n",
      "         [ 1.5256e-01,  1.3206e+00,  6.7108e-01, -4.6826e-01, -1.7270e-01,\n",
      "          -1.4979e-01,  0.0000e+00,  0.0000e+00]]])\n",
      " \n",
      "All Q_current:  tensor([[-84.1958, -99.2369, -92.7846, -96.8988]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-84.1958, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-84.1954, -99.2365, -92.7842, -96.8984]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-84.1954]])\n",
      " \n",
      "Q_target =  tensor([[-84.3949]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.1992]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.1369338 ,  1.1151353 ,  0.47756281, -0.82776564, -0.15491822,\n",
      "       -0.10658927,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.3179,  0.0330,  0.3916, -1.8225, -0.3045, -0.0556,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0208,  1.1322, -0.0170, -0.1107,  0.0512,  0.0049,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0878,  1.2227,  0.2776, -0.6756, -0.0993, -0.0620,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0590,  1.3481, -0.7914, -0.4447,  0.1229,  0.4595,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 2., 0., 1.]])\n",
      "Rewards:  tensor([[-3.1160,  1.8295, -1.2280, -3.5600]])\n",
      "Next states:  tensor([[[ 0.3218, -0.0086,  0.3916, -1.8492, -0.3073, -0.0556,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0205,  1.1302, -0.0359, -0.0859,  0.0507, -0.0095,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0906,  1.2069,  0.2776, -0.7023, -0.1024, -0.0620,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0668,  1.3375, -0.7998, -0.4720,  0.1475,  0.4930,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-84.2037, -99.2316, -92.7798, -96.8937]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-84.2037, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-84.2033, -99.2312, -92.7794, -96.8933]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-84.2033]])\n",
      " \n",
      "Q_target =  tensor([[-84.3662]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.1625]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.14165612,  1.0959233 ,  0.47756228, -0.85443467, -0.16024767,\n",
      "       -0.10658903,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0030,  1.3626, -0.0756, -0.5765,  0.0034,  0.0169,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.5785, -0.0103,  0.7435, -1.8074, -0.2976, -0.0535,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1012,  1.0344, -0.2535, -0.9033,  0.1359,  0.0607,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1352,  1.4084, -0.4274, -0.4178,  0.1530,  0.0954,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[-1.3730,  6.3056, -0.8172, -1.4176]])\n",
      "Next states:  tensor([[[-0.0037,  1.3490, -0.0756, -0.6032,  0.0043,  0.0169,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.5859, -0.0516,  0.7435, -1.8341, -0.3003, -0.0535,  1.0000,\n",
      "           0.0000],\n",
      "         [-0.1037,  1.0135, -0.2535, -0.9300,  0.1390,  0.0607,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1395,  1.3984, -0.4274, -0.4445,  0.1577,  0.0954,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-84.2193, -99.2387, -92.7862, -96.9006]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-84.2193, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-84.2191, -99.2385, -92.7860, -96.9004]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-84.2191]])\n",
      " \n",
      "Q_target =  tensor([[-84.3443]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.1250]], grad_fn=<SubBackward0>)\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current state:  (array([ 0.14637843,  1.0761116 ,  0.47756177, -0.88110363, -0.1655771 ,\n",
      "       -0.10658865,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.3226,  0.7102, -0.4869, -1.3459,  0.3644,  0.1087,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1212,  0.2030,  0.2451, -1.7277, -0.1370, -0.0547,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0035,  1.3956, -0.0252, -0.2218,  0.0040,  0.0056,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0076,  1.0489, -0.0367, -1.0327,  0.0086,  0.0082,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[-0.4636,  0.2284, -2.1207, -0.3229]])\n",
      "Next states:  tensor([[[-0.3274,  0.6794, -0.4869, -1.3726,  0.3699,  0.1087,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1236,  0.1635,  0.2451, -1.7543, -0.1397, -0.0547,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0037,  1.3900, -0.0252, -0.2485,  0.0042,  0.0056,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0080,  1.0251, -0.0367, -1.0594,  0.0090,  0.0082,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-84.2270, -99.2387, -92.7862, -96.9006]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-84.2270, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-84.2267, -99.2383, -92.7859, -96.9002]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-84.2267]])\n",
      " \n",
      "Q_target =  tensor([[-84.3136]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.0866]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.15110083,  1.0557002 ,  0.47756118, -0.90777266, -0.17090653,\n",
      "       -0.1065883 ,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0087,  1.3955,  0.4422, -0.3550, -0.0100, -0.0991,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0283,  1.2794, -0.1685, -0.5572,  0.0321,  0.0377,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2206,  0.6730, -0.6375, -1.3917,  0.2495,  0.1423,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0211,  1.4285,  0.7124,  0.2346, -0.0241, -0.1596,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[-1.3493, -1.4366, -0.3252, -0.4893]])\n",
      "Next states:  tensor([[[ 0.0131,  1.3869,  0.4422, -0.3817, -0.0150, -0.0991,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0300,  1.2662, -0.1685, -0.5839,  0.0340,  0.0376,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2269,  0.6411, -0.6375, -1.4184,  0.2566,  0.1423,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0282,  1.4332,  0.7125,  0.2080, -0.0321, -0.1596,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-84.2320, -99.2383, -92.7858, -96.9002]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-84.2320, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-84.2321, -99.2385, -92.7860, -96.9003]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-84.2321]])\n",
      " \n",
      "Q_target =  tensor([[-84.2801]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.0481]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.15582332,  1.0346892 ,  0.4775606 , -0.9344416 , -0.17623591,\n",
      "       -0.10658771,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.1404,  0.5861,  0.2216, -1.4048,  0.0394,  0.0296,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0373,  1.3397, -0.5386, -0.5324,  0.0424,  0.1206,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1479,  1.3441, -0.7483, -0.4024,  0.1676,  0.1671,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0874,  1.4602,  0.4419, -0.1440, -0.0989, -0.0987,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[ 0.2892, -1.2611, -1.2589, -1.0331]])\n",
      "Next states:  tensor([[[ 0.1427,  0.5539,  0.2216, -1.4315,  0.0409,  0.0296,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0426,  1.3271, -0.5386, -0.5591,  0.0484,  0.1206,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1553,  1.3344, -0.7483, -0.4291,  0.1759,  0.1671,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0917,  1.4564,  0.4419, -0.1706, -0.1039, -0.0987,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-84.2365, -99.2403, -92.7876, -96.9021]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-84.2365, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-84.2364, -99.2402, -92.7875, -96.9020]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-84.2364]])\n",
      " \n",
      "Q_target =  tensor([[-84.2450]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.0084]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.16054574,  1.0130787 ,  0.47756004, -0.96111065, -0.18156528,\n",
      "       -0.10658748,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.2301,  1.1941,  0.7273, -0.7160, -0.2607, -0.1623,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0694,  0.0358, -0.1300, -1.8389,  0.0784,  0.0290,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0205,  1.3675,  0.5182, -0.5225, -0.0233, -0.1161,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2116,  1.2647,  0.4884, -0.6924, -0.2281, -0.0968,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[-1.2028,  7.9085, -1.2720, -1.1646]])\n",
      "Next states:  tensor([[[ 0.2373,  1.1774,  0.7273, -0.7427, -0.2688, -0.1623,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0707, -0.0062, -0.1300, -1.8656,  0.0799,  0.0290,  0.0000,\n",
      "           1.0000],\n",
      "         [ 0.0256,  1.3551,  0.5183, -0.5491, -0.0291, -0.1161,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2165,  1.2485,  0.4884, -0.7191, -0.2329, -0.0968,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-84.2332, -99.2357, -92.7835, -96.8976]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-84.2332, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-84.2327, -99.2351, -92.7829, -96.8971]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-84.2327]])\n",
      " \n",
      "Q_target =  tensor([[-84.2016]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.0315]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.16526833,  0.9908685 ,  0.4775594 , -0.98777956, -0.18689466,\n",
      "       -0.10658757,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0385,  1.3404,  0.5559, -0.5276, -0.0437, -0.1245,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1516,  0.7362,  0.4382, -1.3111, -0.1715, -0.0978,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0479,  0.9943,  0.2107, -1.0984, -0.0542, -0.0470,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0398,  1.4096, -0.6708, -0.0765,  0.0453,  0.1502,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 3., 0.]])\n",
      "Rewards:  tensor([[-1.2542, -0.1688, -0.7867, -0.8959]])\n",
      "Next states:  tensor([[[ 0.0440,  1.3280,  0.5559, -0.5542, -0.0499, -0.1244,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1560,  0.7061,  0.4382, -1.3378, -0.1764, -0.0978,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0501,  0.9690,  0.2202, -1.1255, -0.0585, -0.0853,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0464,  1.4073, -0.6708, -0.1032,  0.0528,  0.1502,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-84.2334, -99.2384, -92.7859, -96.9002]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-84.2334, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-84.2334, -99.2383, -92.7859, -96.9002]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-84.2334]])\n",
      " \n",
      "Q_target =  tensor([[-84.1624]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.0711]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.16999093,  0.9680586 ,  0.4775587 , -1.0144485 , -0.19222403,\n",
      "       -0.10658734,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0103,  1.3887, -0.5204, -0.5074,  0.0118,  0.1167,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0768,  0.3644, -0.1273, -1.5629,  0.0868,  0.0284,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1423,  1.3180,  0.6000, -0.4791, -0.1611, -0.1339,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1691,  1.0652, -0.7231, -0.9489,  0.2007,  0.2071,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[-1.2763,  0.6625, -1.2987, -1.1222]])\n",
      "Next states:  tensor([[[-0.0154,  1.3766, -0.5204, -0.5341,  0.0176,  0.1166,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0780,  0.3286, -0.1273, -1.5896,  0.0882,  0.0284,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1483,  1.3066,  0.6000, -0.5058, -0.1678, -0.1339,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1762,  1.0433, -0.7231, -0.9756,  0.2110,  0.2071,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-84.2326, -99.2426, -92.7897, -96.9043]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-84.2326, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-84.2326, -99.2425, -92.7896, -96.9042]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-84.2326]])\n",
      " \n",
      "Q_target =  tensor([[-84.1216]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.1110]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.17471352,  0.94464916,  0.477558  , -1.0411175 , -0.19755337,\n",
      "       -0.10658683,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0033,  1.4132, -0.3299,  0.0997,  0.0038,  0.0747,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0812,  1.1836,  0.4022, -0.7481, -0.0965, -0.1352,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0617,  0.7161, -0.1950, -1.3792,  0.0698,  0.0435,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0194,  1.3776,  0.0700, -0.4137, -0.0219, -0.0156,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[1., 0., 0., 0.]])\n",
      "Rewards:  tensor([[-0.7910, -1.3254,  0.2746, -1.7200]])\n",
      "Next states:  tensor([[[-0.0066,  1.4148, -0.3372,  0.0745,  0.0090,  0.1049,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0851,  1.1662,  0.4022, -0.7748, -0.1033, -0.1352,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0636,  0.6844, -0.1950, -1.4059,  0.0720,  0.0435,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0201,  1.3677,  0.0700, -0.4403, -0.0227, -0.0156,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-84.2216, -99.2374, -92.7851, -96.8993]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-84.2216, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-84.2218, -99.2377, -92.7853, -96.8996]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-84.2218]])\n",
      " \n",
      "Q_target =  tensor([[-84.0712]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.1504]], grad_fn=<SubBackward0>)\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current state:  (array([ 0.1794363 ,  0.92064   ,  0.47755733, -1.0677866 , -0.20288262,\n",
      "       -0.10658487,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0513,  0.9628,  0.1483, -1.0230, -0.0580, -0.0331,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1833,  0.8388, -0.2967, -1.0513,  0.7061, -0.5312,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0029,  1.4208,  0.2897,  0.4400, -0.0033, -0.0656,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0481,  1.0701,  0.2563, -1.0373, -0.0545, -0.0573,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 3., 0., 0.]])\n",
      "Rewards:  tensor([[-0.4552,  3.4006,  0.8931, -0.4980]])\n",
      "Next states:  tensor([[[ 0.0528,  0.9392,  0.1483, -1.0497, -0.0597, -0.0331,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1864,  0.8144, -0.2878, -1.0725,  0.6768, -0.5851,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0057,  1.4301,  0.2893,  0.4143, -0.0066, -0.0649,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0507,  1.0461,  0.2563, -1.0640, -0.0574, -0.0572,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-84.2129, -99.2380, -92.7856, -96.8999]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-84.2129, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-84.2128, -99.2379, -92.7855, -96.8997]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-84.2128]])\n",
      " \n",
      "Q_target =  tensor([[-84.0227]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.1902]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.18415889,  0.8960313 ,  0.47755662, -1.0944556 , -0.20821187,\n",
      "       -0.10658534,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0621,  0.8532, -0.1250, -1.1376,  0.0847,  0.0349,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0786,  1.1128,  0.3785, -0.8978, -0.0890, -0.0845,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0979,  1.2256, -0.5466, -0.2898,  0.2364,  0.2459,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.4101,  0.2484,  0.7724, -1.6662, -0.5574, -0.2188,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 2., 0.]])\n",
      "Rewards:  tensor([[-0.2226, -0.8392, -0.3307, -2.3432]])\n",
      "Next states:  tensor([[[-0.0633,  0.8270, -0.1250, -1.1643,  0.0864,  0.0349,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0823,  1.0920,  0.3785, -0.9245, -0.0932, -0.0845,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1032,  1.2193, -0.5433, -0.2812,  0.2494,  0.2611,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.4177,  0.2104,  0.7724, -1.6929, -0.5683, -0.2188,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-84.2017, -99.2387, -92.7862, -96.9005]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-84.2017, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-84.2015, -99.2384, -92.7859, -96.9002]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-84.2015]])\n",
      " \n",
      "Q_target =  tensor([[-83.9725]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.2292]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.18888168,  0.870823  ,  0.4775559 , -1.1211245 , -0.21354114,\n",
      "       -0.10658511,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0382,  1.1404,  0.2148, -0.8947, -0.0433, -0.0480,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0524,  0.5898, -0.1401, -1.4807,  0.0260, -0.0097,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1290,  1.1796, -0.3034, -0.7998,  0.1458,  0.0677,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0557,  1.3253, -0.2394, -0.4194,  0.0716,  0.0703,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[-0.7711,  0.7579, -1.0229, -1.6930]])\n",
      "Next states:  tensor([[[ 0.0403,  1.1196,  0.2148, -0.9214, -0.0457, -0.0480,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0538,  0.5559, -0.1401, -1.5074,  0.0255, -0.0097,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1320,  1.1610, -0.3034, -0.8265,  0.1492,  0.0677,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0581,  1.3152, -0.2394, -0.4461,  0.0751,  0.0702,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-84.1897, -99.2412, -92.7885, -96.9030]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-84.1897, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-84.1896, -99.2410, -92.7883, -96.9028]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-84.1896]])\n",
      " \n",
      "Q_target =  tensor([[-83.9224]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.2673]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.19360447,  0.845015  ,  0.47755513, -1.1477935 , -0.21887036,\n",
      "       -0.10658487,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.1880,  0.7956, -0.4753, -1.2047,  0.2126,  0.1061,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0305,  1.0382, -0.1185, -0.9715,  0.0345,  0.0265,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1447,  0.7762,  0.4065, -1.2512, -0.1636, -0.0907,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2397,  0.9861,  0.4909, -1.0075,  0.8314,  1.0786,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 3.]])\n",
      "Rewards:  tensor([[-0.4358, -0.5387, -0.2464, -5.3266]])\n",
      "Next states:  tensor([[[-0.1927,  0.7679, -0.4753, -1.2314,  0.2179,  0.1061,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0316,  1.0157, -0.1185, -0.9981,  0.0358,  0.0265,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1487,  0.7475,  0.4065, -1.2779, -0.1682, -0.0907,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2450,  0.9635,  0.4984, -1.0286,  0.8828,  1.0269,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-84.1733, -99.2411, -92.7884, -96.9029]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-84.1733, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-84.1731, -99.2408, -92.7881, -96.9026]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-84.1731]])\n",
      " \n",
      "Q_target =  tensor([[-83.8686]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.3047]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.19832735,  0.81860733,  0.4775544 , -1.1744626 , -0.2241996 ,\n",
      "       -0.10658468,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0740,  0.2645, -0.1071, -1.6408,  0.0118, -0.0107,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2277,  0.9312,  0.5904, -1.0546, -0.2578, -0.1318,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0523,  1.4743,  0.5874,  0.2062, -0.0593, -0.1315,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1586,  0.7188,  0.4455, -1.3222, -0.1793, -0.0994,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[ 0.9505, -0.7778, -0.2531, -0.1679]])\n",
      "Next states:  tensor([[[-0.0751,  0.2270, -0.1071, -1.6674,  0.0113, -0.0107,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2335,  0.9069,  0.5904, -1.0812, -0.2644, -0.1318,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0581,  1.4783,  0.5875,  0.1796, -0.0659, -0.1314,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1630,  0.6884,  0.4455, -1.3488, -0.1843, -0.0994,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-84.1482, -99.2331, -92.7813, -96.8952]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-84.1482, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-84.1479, -99.2328, -92.7810, -96.8949]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-84.1479]])\n",
      " \n",
      "Q_target =  tensor([[-83.8074]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.3407]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.20305023,  0.79160017,  0.4775536 , -1.2011315 , -0.22952881,\n",
      "       -0.10658447,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.2259,  0.6987, -0.4184, -1.2556,  0.2536,  0.0820,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2946,  0.7309, -0.4964, -1.2914,  0.3328,  0.1108,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1821,  1.3877, -0.5581, -0.4591,  0.2060,  0.1246,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1934,  0.6512,  0.4658, -1.3516, -0.2187, -0.1040,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[-0.3383, -0.4979, -1.3375, -0.2174]])\n",
      "Next states:  tensor([[[-0.2301,  0.6699, -0.4184, -1.2823,  0.2577,  0.0820,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2995,  0.7013, -0.4964, -1.3181,  0.3383,  0.1108,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1876,  1.3768, -0.5581, -0.4858,  0.2122,  0.1246,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1981,  0.6202,  0.4658, -1.3782, -0.2239, -0.1040,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-84.1327, -99.2397, -92.7872, -96.9016]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-84.1327, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-84.1324, -99.2394, -92.7869, -96.9012]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-84.1324]])\n",
      " \n",
      "Q_target =  tensor([[-83.7574]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.3753]], grad_fn=<SubBackward0>)\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current state:  (array([ 0.20777321,  0.76399326,  0.4775528 , -1.2278004 , -0.23485804,\n",
      "       -0.10658421,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.5025, -0.0465,  0.5838, -1.8928, -0.5677, -0.1303,  1.0000,\n",
      "           0.0000],\n",
      "         [ 0.0555,  0.8114,  0.1873, -1.2757, -0.0628, -0.0418,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1647,  1.1811, -0.6408, -0.7277,  0.1864,  0.1430,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0117, -0.0391,  0.0160, -1.8445, -0.0132, -0.0036,  1.0000,\n",
      "           1.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[ 7.6101e+00,  6.1701e-02, -1.1441e+00, -1.0000e+02]])\n",
      "Next states:  tensor([[[ 5.0850e-01, -8.7522e-02,  5.7366e-01, -1.8141e+00, -5.5855e-01,\n",
      "           1.6341e-01,  1.0000e+00,  0.0000e+00],\n",
      "         [ 5.7395e-02,  7.8207e-01,  1.8726e-01, -1.3024e+00, -6.4930e-02,\n",
      "          -4.1806e-02,  0.0000e+00,  0.0000e+00],\n",
      "         [-1.7107e-01,  1.1641e+00, -6.4080e-01, -7.5434e-01,  1.9358e-01,\n",
      "           1.4302e-01,  0.0000e+00,  0.0000e+00],\n",
      "         [ 1.1827e-02, -4.2652e-02,  1.5153e-07,  2.2791e-07, -2.2441e-03,\n",
      "          -6.8512e-07,  1.0000e+00,  1.0000e+00]]])\n",
      " \n",
      "All Q_current:  tensor([[-84.1014, -99.2297, -92.7781, -96.8919]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-84.1014, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-84.0986, -99.2260, -92.7748, -96.8883]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-84.0986]])\n",
      " \n",
      "Q_target =  tensor([[-83.6910]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.4105]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.21249628,  0.73578674,  0.47755194, -1.2544693 , -0.24018726,\n",
      "       -0.10658405,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-3.0827e-01,  5.3500e-01, -4.9478e-01, -1.4457e+00,  3.4826e-01,\n",
      "           1.1043e-01,  0.0000e+00,  0.0000e+00],\n",
      "         [ 1.6581e-02,  1.4419e+00,  1.4985e-01, -7.9186e-03, -1.1324e-04,\n",
      "           3.4167e-03,  0.0000e+00,  0.0000e+00],\n",
      "         [-1.1107e-01,  1.3174e+00, -5.5974e-01, -4.6133e-01,  9.9336e-02,\n",
      "           9.5969e-02,  0.0000e+00,  0.0000e+00],\n",
      "         [-1.9830e-01,  1.6903e-01, -4.0927e-01, -1.7673e+00,  2.2415e-01,\n",
      "           9.1360e-02,  0.0000e+00,  0.0000e+00]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[-0.4907, -0.2914, -1.1604, -0.9793]])\n",
      "Next states:  tensor([[[-3.1317e-01,  5.0190e-01, -4.9478e-01, -1.4724e+00,  3.5378e-01,\n",
      "           1.1043e-01,  0.0000e+00,  0.0000e+00],\n",
      "         [ 1.8081e-02,  1.4412e+00,  1.4985e-01, -3.4587e-02,  5.7548e-05,\n",
      "           3.4161e-03,  0.0000e+00,  0.0000e+00],\n",
      "         [-1.1662e-01,  1.3064e+00, -5.5975e-01, -4.8801e-01,  1.0413e-01,\n",
      "           9.5955e-02,  0.0000e+00,  0.0000e+00],\n",
      "         [-2.0235e-01,  1.2868e-01, -4.0927e-01, -1.7940e+00,  2.2872e-01,\n",
      "           9.1360e-02,  0.0000e+00,  0.0000e+00]]])\n",
      " \n",
      "All Q_current:  tensor([[-84.0825, -99.2371, -92.7848, -96.8990]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-84.0825, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-84.0823, -99.2369, -92.7846, -96.8988]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-84.0823]])\n",
      " \n",
      "Q_target =  tensor([[-83.6442]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.4383]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.21721935,  0.7069805 ,  0.47755107, -1.2811381 , -0.24551643,\n",
      "       -0.10658382,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.4226,  0.0019, -0.7185, -1.8340,  0.5234,  0.1996,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2206,  1.0116, -0.1683, -0.8376,  1.3665,  0.8767,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1995,  1.4143,  0.5604, -0.4637, -0.2260, -0.1251,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2744,  1.2609,  0.6453, -0.7165, -0.3105, -0.1440,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[1., 2., 0., 0.]])\n",
      "Rewards:  tensor([[ 4.7855, -6.0648, -1.3425, -1.2268]])\n",
      "Next states:  tensor([[[-0.4298, -0.0399, -0.7265, -1.8640,  0.5353,  0.2399,  0.0000,\n",
      "           1.0000],\n",
      "         [ 0.2184,  0.9930, -0.2290, -0.8568,  1.4102,  0.8753,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2050,  1.4032,  0.5604, -0.4904, -0.2322, -0.1251,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2808,  1.2442,  0.6453, -0.7431, -0.3177, -0.1440,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-84.0537, -99.2348, -92.7829, -96.8969]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-84.0537, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-84.0525, -99.2332, -92.7814, -96.8953]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-84.0525]])\n",
      " \n",
      "Q_target =  tensor([[-83.5869]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.4669]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.22194262,  0.6775748 ,  0.4775502 , -1.3078072 , -0.2508456 ,\n",
      "       -0.10658361,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.2814,  0.7372, -0.5808, -1.2523,  0.3180,  0.1296,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1566,  1.2619, -0.3870, -0.6937,  0.2237,  0.1094,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0848,  0.7230,  0.2451, -1.3277, -0.0959, -0.0547,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.4108,  0.3704, -0.5393, -1.6152,  0.4639,  0.1204,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[-0.6073, -1.3281,  0.0984, -1.1487]])\n",
      "Next states:  tensor([[[-0.2872,  0.7085, -0.5808, -1.2790,  0.3245,  0.1296,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1605,  1.2457, -0.3870, -0.7204,  0.2291,  0.1094,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0872,  0.6925,  0.2451, -1.3543, -0.0987, -0.0547,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.4161,  0.3335, -0.5393, -1.6419,  0.4699,  0.1204,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-84.0290, -99.2396, -92.7872, -96.9015]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-84.0290, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-84.0288, -99.2393, -92.7869, -96.9012]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-84.0288]])\n",
      " \n",
      "Q_target =  tensor([[-83.5389]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.4901]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.22666588,  0.64756936,  0.47754937, -1.334476  , -0.25617477,\n",
      "       -0.10658336,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.2024,  0.9156,  0.4178, -1.0901, -0.2288, -0.0933,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0659,  1.2658, -0.4764, -0.6341,  0.0748,  0.1065,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0201,  1.3479,  0.3381, -0.5332, -0.0228, -0.0757,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0875,  1.1926, -0.3332, -0.6425,  0.0992,  0.0766,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[-0.6024, -1.2231, -1.3929, -1.2841]])\n",
      "Next states:  tensor([[[ 0.2066,  0.8905,  0.4178, -1.1168, -0.2335, -0.0933,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0706,  1.2510, -0.4764, -0.6607,  0.0801,  0.1065,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0234,  1.3353,  0.3381, -0.5599, -0.0266, -0.0757,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0908,  1.1776, -0.3332, -0.6691,  0.1031,  0.0766,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-83.9984, -99.2392, -92.7868, -96.9010]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-83.9984, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-83.9983, -99.2391, -92.7867, -96.9009]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-83.9983]])\n",
      " \n",
      "Q_target =  tensor([[-83.4885]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.5099]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.23138914,  0.6169643 ,  0.47754845, -1.361145  , -0.26150396,\n",
      "       -0.10658318,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0229,  1.3457, -0.4631, -0.6331,  0.0261,  0.1037,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0735,  0.3960, -0.1282, -1.5382,  0.0831,  0.0286,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0150,  1.3741,  0.0362, -0.5861, -0.0170, -0.0081,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1210,  0.9610,  0.3222, -1.0204, -0.1369, -0.0719,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[-1.2119,  0.6296, -1.3239, -0.6106]])\n",
      "Next states:  tensor([[[-0.0275,  1.3308, -0.4631, -0.6598,  0.0312,  0.1037,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0748,  0.3608, -0.1282, -1.5649,  0.0846,  0.0286,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0154,  1.3604,  0.0362, -0.6128, -0.0174, -0.0081,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1242,  0.9374,  0.3222, -1.0471, -0.1405, -0.0719,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-83.9687, -99.2410, -92.7885, -96.9029]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-83.9687, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-83.9686, -99.2409, -92.7884, -96.9027]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-83.9686]])\n",
      " \n",
      "Q_target =  tensor([[-83.4438]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.5249]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.2361125 ,  0.58575964,  0.47754756, -1.387814  , -0.26683313,\n",
      "       -0.10658298,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0888,  1.0342,  0.3905, -1.0215, -0.1005, -0.0872,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1390,  1.0799, -0.3799, -0.8785,  0.1572,  0.0848,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0829,  0.4233,  0.1133, -1.5669, -0.0936, -0.0253,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0123,  1.4511,  0.2068,  0.2311, -0.0139, -0.0463,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[-0.6171, -0.9082,  0.7026,  1.2390]])\n",
      "Next states:  tensor([[[ 0.0926,  1.0106,  0.3905, -1.0482, -0.1049, -0.0872,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1427,  1.0595, -0.3799, -0.9052,  0.1614,  0.0848,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0840,  0.3874,  0.1133, -1.5935, -0.0949, -0.0253,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0143,  1.4557,  0.2068,  0.2044, -0.0163, -0.0463,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-83.9332, -99.2369, -92.7848, -96.8989]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-83.9332, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-83.9331, -99.2368, -92.7847, -96.8988]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-83.9331]])\n",
      " \n",
      "Q_target =  tensor([[-83.3996]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.5336]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.24083586,  0.5539554 ,  0.47754663, -1.4144828 , -0.27216223,\n",
      "       -0.10658282,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-4.5490e-05,  7.4596e-01, -4.4963e-01, -1.1182e+00,  1.9797e+00,\n",
      "           6.7107e-01,  0.0000e+00,  0.0000e+00],\n",
      "         [ 1.1635e-01,  8.6576e-01,  3.2689e-01, -1.1405e+00, -1.3158e-01,\n",
      "          -7.2969e-02,  0.0000e+00,  0.0000e+00],\n",
      "         [-7.8083e-02,  1.2605e+00, -7.1809e-01, -7.4108e-01,  8.8603e-02,\n",
      "           1.6064e-01,  0.0000e+00,  0.0000e+00],\n",
      "         [-1.0316e-01,  1.4646e+00, -4.9702e-01, -1.5327e-01,  1.1684e-01,\n",
      "           1.1098e-01,  0.0000e+00,  0.0000e+00]]])\n",
      "Actions:  tensor([[3., 0., 0., 0.]])\n",
      "Rewards:  tensor([[-2.6068, -0.3743, -1.0601, -1.0363]])\n",
      "Next states:  tensor([[[-0.0047,  0.7208, -0.4539, -1.1381,  2.0106,  0.6174,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1196,  0.8395,  0.3269, -1.1672, -0.1352, -0.0730,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0852,  1.2433, -0.7181, -0.7678,  0.0966,  0.1606,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1081,  1.4606, -0.4970, -0.1799,  0.1224,  0.1110,  0.0000,\n",
      "           0.0000]]])\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Q_current:  tensor([[-83.9004, -99.2366, -92.7845, -96.8986]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-83.9004, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-83.9002, -99.2364, -92.7843, -96.8983]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-83.9002]])\n",
      " \n",
      "Q_target =  tensor([[-83.3657]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.5347]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.2455593 ,  0.52155143,  0.47754565, -1.4411519 , -0.27749136,\n",
      "       -0.10658262,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.2391,  0.3845,  0.4583, -1.5568, -0.2576, -0.0712,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0851,  1.4299,  0.4102, -0.2267, -0.0964, -0.0916,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1627,  0.3545, -0.3226, -1.5880,  0.1839,  0.0720,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0092,  0.4224, -0.0156, -1.5487, -0.0317, -0.0262,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[-0.1919, -1.2619,  0.1482,  0.7454]])\n",
      "Next states:  tensor([[[ 0.2437,  0.3489,  0.4583, -1.5835, -0.2612, -0.0712,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0892,  1.4242,  0.4102, -0.2533, -0.1010, -0.0916,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1659,  0.3182, -0.3226, -1.6147,  0.1875,  0.0720,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0093,  0.3870, -0.0156, -1.5754, -0.0330, -0.0262,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-83.8624, -99.2302, -92.7788, -96.8925]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-83.8624, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-83.8621, -99.2299, -92.7785, -96.8921]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-83.8621]])\n",
      " \n",
      "Q_target =  tensor([[-83.3360]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.5264]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.25028285,  0.48854786,  0.4775447 , -1.4678208 , -0.2828205 ,\n",
      "       -0.10658243,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.3551,  0.1427, -0.7181, -1.7823,  0.4013,  0.1603,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1823,  1.2740, -0.6357, -0.5845,  0.2066,  0.1419,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3253,  1.2299,  0.7522, -0.3314, -0.4078, -0.2453,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2770,  1.2840, -0.7780, -0.6250,  0.3136,  0.1736,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[-2.6380, -1.2689, -1.7582, -1.2970]])\n",
      "Next states:  tensor([[[-0.3622,  0.1021, -0.7181, -1.8089,  0.4093,  0.1603,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1886,  1.2603, -0.6357, -0.6112,  0.2137,  0.1419,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3327,  1.2219,  0.7522, -0.3581, -0.4201, -0.2453,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2847,  1.2694, -0.7780, -0.6517,  0.3222,  0.1736,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-83.8346, -99.2357, -92.7837, -96.8977]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-83.8346, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-83.8343, -99.2353, -92.7834, -96.8974]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-83.8343]])\n",
      " \n",
      "Q_target =  tensor([[-83.3283]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.5063]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.2550065 ,  0.45494464,  0.47754365, -1.4944897 , -0.2881496 ,\n",
      "       -0.10658222,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.1547,  1.0455,  0.6334, -0.9706, -0.1834, -0.1740,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2963,  0.2568,  0.4094, -1.6482, -0.3312, -0.0845,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2645,  0.6921,  0.6859, -1.3273, -0.2994, -0.1531,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1057,  1.3016, -0.7640, -0.5206,  0.1199,  0.1708,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[-1.0491, -0.9819, -0.5601, -1.2220]])\n",
      "Next states:  tensor([[[ 0.1610,  1.0231,  0.6331, -0.9977, -0.1925, -0.1820,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.3004,  0.2191,  0.4094, -1.6749, -0.3355, -0.0845,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2713,  0.6616,  0.6859, -1.3540, -0.3071, -0.1531,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1133,  1.2894, -0.7640, -0.5472,  0.1284,  0.1708,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-83.8005, -99.2321, -92.7806, -96.8943]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-83.8005, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-83.8003, -99.2318, -92.7803, -96.8941]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-83.8003]])\n",
      " \n",
      "Q_target =  tensor([[-83.3289]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.4716]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.25973025,  0.42074177,  0.4775427 , -1.5211586 , -0.2934787 ,\n",
      "       -0.10658202,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0955,  1.0133, -0.4203, -1.0618,  0.1082,  0.0938,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2156,  1.0122,  0.8463, -0.9741, -1.1115, -1.0670,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1645,  1.1115, -0.5093, -0.8313,  0.2327,  0.1556,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0420,  1.3950, -0.6071, -0.1810,  0.0477,  0.1359,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 1., 0., 0.]])\n",
      "Rewards:  tensor([[-0.5571, -4.5033, -1.2310, -1.0472]])\n",
      "Next states:  tensor([[[-0.0997,  0.9888, -0.4203, -1.0885,  0.1129,  0.0938,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2238,  0.9905,  0.8419, -0.9952, -1.1625, -1.0207,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1695,  1.0922, -0.5093, -0.8580,  0.2405,  0.1556,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0480,  1.3903, -0.6071, -0.2077,  0.0545,  0.1359,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-83.7769, -99.2388, -92.7866, -96.9007]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-83.7769, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-83.7768, -99.2386, -92.7865, -96.9006]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-83.7768]])\n",
      " \n",
      "Q_target =  tensor([[-83.3582]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.4187]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.26445398,  0.38593936,  0.47754163, -1.5478275 , -0.2988078 ,\n",
      "       -0.10658183,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.3607, -0.0419, -0.6399, -1.8810,  0.4075,  0.1428,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.3451,  0.4503, -0.4780, -1.5460,  0.3898,  0.1067,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0235,  1.2645,  0.1583, -0.6206, -0.0266, -0.0354,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1953,  1.0965, -0.5808, -0.8523,  0.2208,  0.1296,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[ 5.4022, -0.6236, -1.3115, -1.0224]])\n",
      "Next states:  tensor([[[-0.3671, -0.0848, -0.6399, -1.9076,  0.4147,  0.1428,  0.0000,\n",
      "           1.0000],\n",
      "         [-0.3498,  0.4149, -0.4780, -1.5726,  0.3951,  0.1067,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0250,  1.2499,  0.1583, -0.6472, -0.0284, -0.0354,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2010,  1.0767, -0.5808, -0.8789,  0.2273,  0.1296,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-83.7456, -99.2313, -92.7799, -96.8936]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-83.7456, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-83.7440, -99.2293, -92.7781, -96.8916]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-83.7440]])\n",
      " \n",
      "Q_target =  tensor([[-83.4010]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.3445]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.2691779 ,  0.35053718,  0.47754064, -1.5744965 , -0.30413687,\n",
      "       -0.10658167,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.1353,  1.1291, -0.2776, -0.9048,  0.2466,  0.1072,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0862,  0.4955, -0.1428, -1.4631,  0.0072, -0.0117,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1326,  1.0775, -0.3365, -0.9050, -0.4595, -0.8277,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1933,  1.0498, -0.7821, -0.9626,  0.2187,  0.1745,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[2., 0., 3., 0.]])\n",
      "Rewards:  tensor([[ 3.8739,  0.6765, -4.8826, -0.9645]])\n",
      "Next states:  tensor([[[-0.1383,  1.1097, -0.3087, -0.8652,  0.2517,  0.1015,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0877,  0.4620, -0.1428, -1.4898,  0.0066, -0.0117,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1363,  1.0568, -0.3287, -0.9348, -0.5028, -0.8658,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2010,  1.0276, -0.7818, -0.9898,  0.2279,  0.1825,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-83.7297, -99.2381, -92.7861, -96.9001]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-83.7297, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-83.7291, -99.2374, -92.7854, -96.8994]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-83.7291]])\n",
      " \n",
      "Q_target =  tensor([[-83.4900]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.2397]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.27390185,  0.31453544,  0.4775396 , -1.6011654 , -0.30946594,\n",
      "       -0.10658135,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.4260,  1.0744, -0.7830, -0.9940,  0.4816,  0.1747,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0123,  1.4090,  0.6215, -0.0552, -0.0141, -0.1393,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0776,  1.2270,  0.6042, -0.7889, -0.0881, -0.1351,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2571,  0.1813, -0.3032, -1.7632,  0.1089,  0.0234,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[-1.1467, -0.8133, -1.0165, -0.8746]])\n",
      "Next states:  tensor([[[-0.4337,  1.0515, -0.7830, -1.0206,  0.4903,  0.1747,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0184,  1.4072,  0.6215, -0.0819, -0.0210, -0.1392,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0836,  1.2087,  0.6043, -0.8156, -0.0948, -0.1351,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2601,  0.1411, -0.3032, -1.7899,  0.1100,  0.0234,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-83.7140, -99.2368, -92.7849, -96.8988]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-83.7140, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-83.7140, -99.2367, -92.7848, -96.8987]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-83.7140]])\n",
      " \n",
      "Q_target =  tensor([[-83.6140]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[0.1001]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.27862588,  0.27793407,  0.47753853, -1.6278344 , -0.314795  ,\n",
      "       -0.10658111,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.2373,  0.9201,  0.4799, -1.0906, -0.2681, -0.1071,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1060,  0.6790,  0.2979, -1.3711, -0.1199, -0.0665,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1781,  1.4047,  0.5460, -0.4362, -0.2019, -0.1218,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0543,  0.2432, -0.0699, -1.6105,  0.0295, -0.0171,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[-0.6740,  0.1171, -1.3424,  0.9834]])\n",
      "Next states:  tensor([[[ 0.2420,  0.8950,  0.4799, -1.1173, -0.2735, -0.1071,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1090,  0.6476,  0.2979, -1.3977, -0.1232, -0.0665,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1835,  1.3943,  0.5460, -0.4629, -0.2080, -0.1218,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0550,  0.2064, -0.0699, -1.6371,  0.0286, -0.0171,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-83.7045, -99.2327, -92.7812, -96.8949]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-83.7045, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-83.7042, -99.2324, -92.7809, -96.8946]])\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen maximum Q - value=  tensor([[-83.7042]])\n",
      " \n",
      "Q_target =  tensor([[-83.7866]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.0821]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.28335   ,  0.24073298,  0.47753745, -1.6545032 , -0.32012403,\n",
      "       -0.10658089,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.3221,  0.7083,  0.7574, -1.2880, -0.3644, -0.1690,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0938,  1.4141, -0.2965, -0.4098,  0.1061,  0.0662,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1740,  0.9892, -0.3142, -1.0687,  0.1966,  0.0701,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0245,  1.1510,  0.0954, -0.7786, -0.0278, -0.0213,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[-0.7961, -1.5551, -0.5418, -0.9447]])\n",
      "Next states:  tensor([[[ 0.3296,  0.6787,  0.7574, -1.3146, -0.3729, -0.1690,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0967,  1.4043, -0.2965, -0.4365,  0.1094,  0.0662,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1771,  0.9645, -0.3142, -1.0954,  0.2001,  0.0701,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0255,  1.1328,  0.0954, -0.8053, -0.0288, -0.0213,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-83.7107, -99.2342, -92.7825, -96.8963]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-83.7107, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-83.7105, -99.2340, -92.7823, -96.8961]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-83.7105]])\n",
      " \n",
      "Q_target =  tensor([[-84.0271]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.3164]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.2880741 ,  0.2029323 ,  0.4775363 , -1.6811723 , -0.32545307,\n",
      "       -0.10658069,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.0960,  1.1678,  0.3034, -0.7519, -0.1086, -0.0677,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2076,  0.5437,  0.4285, -1.4275, -0.2347, -0.0957,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0573,  0.9085, -0.1705, -1.0975,  0.0648,  0.0381,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0434,  0.7034, -0.0731, -1.3112,  0.0490,  0.0163,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[-1.0976, -0.1527, -0.3131,  0.2553]])\n",
      "Next states:  tensor([[[ 0.0990,  1.1503,  0.3034, -0.7786, -0.1119, -0.0677,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.2119,  0.5110,  0.4285, -1.4542, -0.2395, -0.0957,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0590,  0.8832, -0.1705, -1.1242,  0.0667,  0.0381,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0441,  0.6733, -0.0731, -1.3379,  0.0498,  0.0163,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-83.7299, -99.2339, -92.7822, -96.8960]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-83.7299, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-83.7297, -99.2337, -92.7820, -96.8958]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-83.7297]])\n",
      " \n",
      "Q_target =  tensor([[-84.3406]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.6108]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.29279834,  0.164532  ,  0.47753516, -1.7078413 , -0.3307821 ,\n",
      "       -0.10658047,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.1151,  1.2171, -0.2647, -0.7698,  0.1302,  0.0591,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0941,  1.1951, -0.1830, -0.8650,  0.1063,  0.0408,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0122,  1.4374, -0.4104,  0.3662,  0.0139,  0.0920,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0150,  1.0548,  0.0303, -0.9703, -0.0169, -0.0068,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[-1.0636, -0.8298,  0.5097, -0.4566]])\n",
      "Next states:  tensor([[[-0.1178,  1.1992, -0.2647, -0.7965,  0.1331,  0.0591,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0959,  1.1751, -0.1830, -0.8916,  0.1084,  0.0408,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0162,  1.4451, -0.4105,  0.3395,  0.0185,  0.0919,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0153,  1.0324,  0.0303, -0.9970, -0.0172, -0.0068,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-83.7720, -99.2396, -92.7873, -96.9015]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-83.7720, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-83.7720, -99.2396, -92.7873, -96.9015]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-83.7720]])\n",
      " \n",
      "Q_target =  tensor([[-84.7431]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-0.9711]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.29752263,  0.12553205,  0.477534  , -1.7345102 , -0.3361111 ,\n",
      "       -0.10658028,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.4545,  0.1818,  0.1335, -0.4966,  0.4548, -2.3120,  0.0000,\n",
      "           1.0000],\n",
      "         [-0.3469,  0.6093, -0.6946, -1.3660,  0.3538,  0.1149,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1885,  0.2977,  0.2803, -1.6215, -0.2130, -0.0626,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0034,  1.4067, -0.0570,  0.0455,  0.0130,  0.0472,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 2.]])\n",
      "Rewards:  tensor([[12.7329, -0.6106, -0.0334, -3.4489]])\n",
      "Next states:  tensor([[[-0.4542,  0.1699,  0.1456, -0.5016,  0.3239, -2.5373,  0.0000,\n",
      "           1.0000],\n",
      "         [-0.3537,  0.5779, -0.6946, -1.3927,  0.3595,  0.1149,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1913,  0.2606,  0.2803, -1.6482, -0.2161, -0.0626,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0041,  1.4083, -0.0721,  0.0709,  0.0146,  0.0331,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-83.8109, -99.2137, -92.7639, -96.8765]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-83.8109, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-83.8105, -99.2132, -92.7635, -96.8761]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-83.8105]])\n",
      " \n",
      "Q_target =  tensor([[-85.2083]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-1.3974]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.30224705,  0.0859324 ,  0.4775328 , -1.761179  , -0.3414401 ,\n",
      "       -0.10658006,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-2.1354e-02,  1.3828e+00, -1.5431e-01, -2.6265e-01,  2.4220e-02,\n",
      "           3.4503e-02,  0.0000e+00,  0.0000e+00],\n",
      "         [-6.6807e-03,  1.4410e+00, -3.3109e-02, -1.3460e-01, -3.0576e-03,\n",
      "          -2.7642e-02,  0.0000e+00,  0.0000e+00],\n",
      "         [-5.3489e-01, -9.1528e-02,  2.8213e-08,  2.9319e-09,  3.7528e-01,\n",
      "          -4.4260e-08,  1.0000e+00,  1.0000e+00],\n",
      "         [-1.8581e-01,  7.5134e-01, -4.5834e-01, -1.2493e+00,  2.1008e-01,\n",
      "           1.0230e-01,  0.0000e+00,  0.0000e+00]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[-1.8516e+00, -2.3774e+00, -9.6005e-07, -3.5023e-01]])\n",
      "Next states:  tensor([[[-2.2880e-02,  1.3763e+00, -1.5432e-01, -2.8932e-01,  2.5945e-02,\n",
      "           3.4499e-02,  0.0000e+00,  0.0000e+00],\n",
      "         [-7.0257e-03,  1.4374e+00, -3.3105e-02, -1.6127e-01, -4.4394e-03,\n",
      "          -2.7638e-02,  0.0000e+00,  0.0000e+00],\n",
      "         [-5.3489e-01, -9.1528e-02,  3.7852e-08, -2.9389e-09,  3.7528e-01,\n",
      "          -4.7465e-08,  1.0000e+00,  1.0000e+00],\n",
      "         [-1.9034e-01,  7.2264e-01, -4.5834e-01, -1.2760e+00,  2.1519e-01,\n",
      "           1.0230e-01,  0.0000e+00,  0.0000e+00]]])\n",
      " \n",
      "All Q_current:  tensor([[-83.9097, -99.2301, -92.7786, -96.8923]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-83.9097, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-83.9099, -99.2303, -92.7787, -96.8925]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-83.9099]])\n",
      " \n",
      "Q_target =  tensor([[-85.7923]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-1.8826]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.30697155,  0.04573318,  0.47753167, -1.7878479 , -0.3467691 ,\n",
      "       -0.10657986,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.1071,  1.1813, -0.3734, -0.7263,  0.1211,  0.0833,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.4576,  1.0061,  0.6670, -0.7523, -0.4316,  0.3223,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0071,  1.4114,  0.7171,  0.0215, -0.0082, -0.1624,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1556,  1.1282,  0.4254, -0.8205, -0.1760, -0.0949,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 1., 0., 0.]])\n",
      "Rewards:  tensor([[-1.1453,  2.1498, -0.6708, -1.0227]])\n",
      "Next states:  tensor([[[-0.1108,  1.1643, -0.3734, -0.7530,  0.1253,  0.0833,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.4644,  0.9886,  0.6561, -0.7750, -0.4129,  0.3745,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0142,  1.4113,  0.7162, -0.0042, -0.0162, -0.1605,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1598,  1.1092,  0.4254, -0.8472, -0.1807, -0.0949,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-84.0312, -99.2381, -92.7858, -96.9000]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-84.0312, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-84.0313, -99.2382, -92.7859, -96.9001]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-84.0313]])\n",
      " \n",
      "Q_target =  tensor([[-86.4392]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-2.4080]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.31169614,  0.00493424,  0.47753042, -1.8145168 , -0.35209808,\n",
      "       -0.1065797 ,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.3639,  0.8475,  0.7046, -1.1695, -0.5464, -0.2397,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2159,  1.3886, -0.7798, -0.3971,  0.2447,  0.1740,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0776,  0.8992,  0.1750, -1.0889, -0.0814, -0.0078,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2441,  0.4219, -0.4690, -1.5242,  0.3850,  0.1506,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[-1.3154, -1.2946, -0.1879, -0.5687]])\n",
      "Next states:  tensor([[[ 0.3709,  0.8207,  0.7046, -1.1962, -0.5584, -0.2397,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2236,  1.3791, -0.7798, -0.4237,  0.2534,  0.1740,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0794,  0.8741,  0.1750, -1.1155, -0.0818, -0.0078,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.2487,  0.3871, -0.4690, -1.5508,  0.3926,  0.1506,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-84.1790, -99.2382, -92.7858, -96.9001]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-84.1790, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-84.1788, -99.2380, -92.7856, -96.8999]])\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [40:11<00:00,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen maximum Q - value=  tensor([[-84.1788]])\n",
      " \n",
      "Q_target =  tensor([[-87.1282]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-2.9492]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.31642085, -0.03646437,  0.47752923, -1.8411858 , -0.35742706,\n",
      "       -0.10657953,  0.        ,  0.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[-0.0027,  1.3550, -0.0304, -0.3828,  0.0031,  0.0068,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.4131,  0.2100,  0.6233, -1.6782, -0.4669, -0.1391,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0976,  0.5452, -0.4230, -1.2881,  1.7129, -0.3081,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0609,  0.7150, -0.1282, -1.2715,  0.0688,  0.0286,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 3., 0.]])\n",
      "Rewards:  tensor([[-1.7719, -2.1626,  2.6904,  0.1018]])\n",
      "Next states:  tensor([[[-0.0030,  1.3458, -0.0304, -0.4094,  0.0034,  0.0068,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.4193,  0.1717,  0.6233, -1.7048, -0.4739, -0.1391,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.1018,  0.5155, -0.4238, -1.3080,  1.6950, -0.3590,  0.0000,\n",
      "           0.0000],\n",
      "         [-0.0621,  0.6858, -0.1282, -1.2982,  0.0702,  0.0286,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-84.3586, -99.2366, -92.7841, -96.8985]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-84.3586, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-84.3584, -99.2364, -92.7840, -96.8983]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-84.3584]])\n",
      " \n",
      "Q_target =  tensor([[-67.8380]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[16.5205]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Current state:  (array([ 0.32114562, -0.07846262,  0.47752795, -1.8678546 , -0.362756  ,\n",
      "       -0.10657933,  1.        ,  1.        ], dtype=float32), {})\n",
      " \n",
      "Experiences: \n",
      "States:  tensor([[[ 0.5392,  0.0983,  0.6410, -1.8082, -0.6090, -0.1431,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0692,  0.2228,  0.1195, -1.6661, -0.1364, -0.0595,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0239,  1.2443,  0.1210, -0.6237, -0.0271, -0.0270,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1273,  0.8698,  0.3519, -1.1353, -0.0955, -0.0350,  0.0000,\n",
      "           0.0000]]])\n",
      "Actions:  tensor([[0., 0., 0., 0.]])\n",
      "Rewards:  tensor([[-3.2748,  0.6051, -1.2947, -0.1917]])\n",
      "Next states:  tensor([[[ 0.5455,  0.0571,  0.6410, -1.8349, -0.6162, -0.1431,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0704,  0.1847,  0.1195, -1.6927, -0.1394, -0.0595,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.0251,  1.2296,  0.1210, -0.6503, -0.0284, -0.0270,  0.0000,\n",
      "           0.0000],\n",
      "         [ 0.1308,  0.8437,  0.3519, -1.1620, -0.0972, -0.0350,  0.0000,\n",
      "           0.0000]]])\n",
      " \n",
      "All Q_current:  tensor([[-83.3350, -99.2231, -92.7728, -96.8857]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "Q_current:  tensor(-83.3350, grad_fn=<SelectBackward0>)\n",
      " \n",
      "All Q values for next_states = \n",
      "tensor([[-83.3345, -99.2225, -92.7722, -96.8851]])\n",
      " \n",
      "Chosen maximum Q - value=  tensor([[-83.3345]])\n",
      " \n",
      "Q_target =  tensor([[-100.]])\n",
      " \n",
      "Error(Q_target - Q_current):  tensor([[-16.6650]], grad_fn=<SubBackward0>)\n",
      " \n",
      "Score:  -152.18512946436766\n",
      "----------------End Algorithm--------------------\n",
      "------------------------End Environment-------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Seed = 1\")\n",
    "for i in envs:\n",
    "    print(\"ENVIRONMENT:-----------\", i)\n",
    "    env = gym.make(i)\n",
    "    res=[]\n",
    "    for j in algos:\n",
    "        print(\"Algorithm:\", j[2])\n",
    "        rewards = []\n",
    "        aver_reward = []\n",
    "        aver = deque(maxlen=100)\n",
    "        state_size = env.observation_space.shape[0]\n",
    "        action_size=env.action_space.n\n",
    "        batch_size = 4\n",
    "        seed = 1\n",
    "        agent = Agent(state_size, action_size, hidden_size, num_layers, batch_size, seed, ddqn=j[0], priority=j[1])\n",
    "        epsilon = epsilon_start                    # initialize epsilon\n",
    "       \n",
    "        for i_episode in tqdm(range(1, n_episodes+1)):\n",
    "            if i_episode < 3 or i_episode > 1998:\n",
    "                print(\"Iteration number: \", i_episode - 1)\n",
    "            state = env.reset()\n",
    "            score = 0\n",
    "#             max_t = 200\n",
    "            for t in range(max_t):\n",
    "                action = agent.act(state, epsilon)\n",
    "                step_result = env.step(action)\n",
    "                next_state, reward, done, _ = step_result[:4]\n",
    "                next_state = (next_state, {})\n",
    "                agent.step(state, action, reward, next_state, done, state_size)\n",
    "                \n",
    "                state = next_state\n",
    "                score = score + reward\n",
    "                if done:\n",
    "                    break \n",
    "            if i_episode < 3 or i_episode > 1998:\n",
    "                print(\"Score: \", score)\n",
    "\n",
    "            aver.append(score)     \n",
    "            aver_reward.append(np.mean(aver))\n",
    "            rewards.append(score)\n",
    "            epsilon = max(epsilon_end, epsilon_decay*epsilon) # decrease epsilon\n",
    "            \n",
    "        reward=\"model/Seed41_\"+i+\"_LSTMDQN_\"+str(n_episodes)+\"_\"+str(datetime.now().strftime(\"%Y%m%d%H%M%S\"))\n",
    "        torch.save(agent.qnetwork_local.state_dict(),reward+'.pt')\n",
    "        res.append(aver_reward)\n",
    "        print(\"----------------End Algorithm--------------------\")\n",
    "    \n",
    "    fig=plt.figure()   \n",
    "    \n",
    "    reward='plots/Seed42_'+i+'_LSTMDQN_result'+str(datetime.now().strftime(\"%Y%m%d%H%M%S\"))\n",
    "    df=pd.DataFrame({'DQN':res[0]})\n",
    "    df.to_csv(reward+'.csv')\n",
    "    print(\"------------------------End Environment-------------------\")\n",
    "    \n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.plot(df['DQN'], 'r', label='DQN')\n",
    "    \n",
    "    plt.title('Learning Curve '+i)\n",
    "\n",
    "    #Insert the legends in the plot\n",
    "    fig.legend(loc='lower right')\n",
    "    fig.savefig(reward+'.png', dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f810bdb",
   "metadata": {},
   "source": [
    "## 8. Demonstration with random policy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2caa44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the environment\n",
    "env_name = 'MountainCar-v0'  # Change this to the environment you want to simulate\n",
    "env = gym.make(env_name, render_mode = 'human')\n",
    "\n",
    "# Get environment parameters\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "# Initialize the agent\n",
    "agent = Agent(state_size, action_size, seed=65)  \n",
    "\n",
    "# Simulate the model in the environment with random actions\n",
    "scores = []\n",
    "n_episodes = 100  # Number of episodes for simulation\n",
    "max_t = 1000  # Maximum number of timesteps per episode\n",
    "\n",
    "for i_episode in tqdm(range(1, n_episodes+1)):\n",
    "    state = env.reset()\n",
    "    env.render()\n",
    "    score = 0\n",
    "    \n",
    "    for t in range(max_t):\n",
    "        action = agent.act(state, epsilon=0.5) \n",
    "        step_result = env.step(action)\n",
    "        next_state, reward, done, _ = step_result[:4]\n",
    "        state = next_state\n",
    "        score = score + reward\n",
    "        if done:\n",
    "            break\n",
    "    print(score)        \n",
    "    scores.append(score)\n",
    "\n",
    "# Close the environment\n",
    "env.close()\n",
    "\n",
    "# Print average score\n",
    "print(\"Average score with random actions:\", np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c569436d",
   "metadata": {},
   "source": [
    "## 9. Demonstration with learned policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1980aa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the environment\n",
    "env_name = 'MountainCar-v0'  # Change this to the environment you want to simulate\n",
    "env = gym.make(env_name, render_mode = 'human')\n",
    "\n",
    "# Get environment parameters\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "# Initialize the agent\n",
    "agent = Agent(state_size, action_size, seed=1)  \n",
    "# Load the model weights\n",
    "model_path = 'model/Seed1_MountainCar-v0_PriorityDDQN_4000_20240405161249.pt'  # Path to your trained model\n",
    "agent.qnetwork_local.load_state_dict(torch.load(model_path))\n",
    "agent.qnetwork_local.eval()\n",
    "\n",
    "# Simulate the model in the environment\n",
    "scores = []\n",
    "n_episodes = 100  # Number of episodes for simulation\n",
    "max_t = 1000  # Maximum number of timesteps per episode\n",
    "\n",
    "for i_episode in tqdm(range(1, n_episodes+1)):\n",
    "    state = env.reset()\n",
    "    env.render()\n",
    "    score = 0\n",
    "    \n",
    "    for t in range(max_t):\n",
    "        action = agent.act(state, epsilon=0.)  # Greedy action selection, no exploration\n",
    "        step_result = env.step(action)\n",
    "        next_state, reward, done, _ = step_result[:4]\n",
    "        state = next_state\n",
    "        score = score + reward\n",
    "        if done:\n",
    "            break\n",
    "    print(score)        \n",
    "    scores.append(score)\n",
    "\n",
    "# Close the environment\n",
    "env.close()\n",
    "\n",
    "# Print average score\n",
    "print(\"Average score:\", np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eaa2674",
   "metadata": {},
   "source": [
    "## 10. Draw the shaded plot for each case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "294a26e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard deviation:  0       163.474949\n",
      "1       100.444962\n",
      "2        75.915617\n",
      "3        62.241312\n",
      "4        65.174756\n",
      "           ...    \n",
      "3995     69.422866\n",
      "3996     70.535840\n",
      "3997     70.094424\n",
      "3998     73.780648\n",
      "3999     74.949975\n",
      "Length: 4000, dtype: float64\n",
      "Mean:  0      -250.903886\n",
      "1      -231.173764\n",
      "2      -220.472175\n",
      "3      -204.139179\n",
      "4      -195.567122\n",
      "           ...    \n",
      "3995   -179.953481\n",
      "3996   -180.219427\n",
      "3997   -180.415647\n",
      "3998   -182.397746\n",
      "3999   -182.755575\n",
      "Length: 4000, dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 1: Read Data from CSV\n",
    "df = pd.read_csv('plots/LSTMDQN_LunarLander-v2.csv')\n",
    "\n",
    "# Step 2: Remove the first row (Episodes, Run 1, Run 2, Run 3) to keep only the rewards data\n",
    "# df = df.drop(0)\n",
    "\n",
    "# Step 3: Convert the remaining DataFrame to numeric values\n",
    "df = df.apply(pd.to_numeric)\n",
    "\n",
    "# Step 4: Select only the three late columns (Run 1, Run 2, Run 3)\n",
    "later_columns = df.iloc[:, 1:4]\n",
    "\n",
    "# Step 5: Calculate Mean and Standard Deviation for the three later columns\n",
    "mean_values = later_columns.mean(axis=1)\n",
    "std_values = later_columns.std(axis=1)\n",
    "print(\"Standard deviation: \", std_values)\n",
    "print(\"Mean: \", mean_values)\n",
    "\n",
    "# Step 6: Plot the Data\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(mean_values, label='Mean Sum of Rewards', color = \"red\")\n",
    "plt.fill_between(mean_values.index, mean_values - std_values, mean_values + std_values, alpha=0.5, label='Standard Deviation', color = \"orange\")\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Sum of Rewards')\n",
    "plt.title('Shaded Plot of Sum of Rewards with Mean and Standard Deviation for LSTM Deep Q-Learning Algorithm, Lunar Lander')\n",
    "plt.savefig('plots/shaded_plot_LSTMDQN_LunarLander.png')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df187e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c548e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
